{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70202679",
   "metadata": {},
   "source": [
    "# Imports and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afc5481",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Softmax\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import functional as F\n",
    "import time\n",
    "\n",
    "\n",
    "import os\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c33059f",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13be92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DP_DIR = \"Data_preprocessing\"\n",
    "export_dir = Path(os.getcwd())\n",
    "files_path = Path(export_dir.parent, DP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d88e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_mixed = pd.read_csv(Path(files_path,'train_data.csv'))\n",
    "test_data = pd.read_csv(Path(files_path,'test_data.csv'))\n",
    "train_array = train_data_mixed.to_numpy()\n",
    "test_array = test_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b81912",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(files_path,'items_values_dict.pkl'), 'rb') as f:\n",
    "    items_values_dict = pickle.load(f) # deserialize using load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386f4dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array = train_data_mixed.iloc[:,:-3].to_numpy()\n",
    "test_array = test_data.iloc[:,:-2].to_numpy()\n",
    "test_user_ids = test_data.iloc[:,-2].to_numpy()\n",
    "test_y_pos = test_data['y_positive'].to_numpy()\n",
    "\n",
    "test_array_without_item = test_array.copy()\n",
    "for i in range(len(test_array)):\n",
    "    test_array_without_item[i][test_y_pos[i]] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47894bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = 6040\n",
    "num_items = 3706"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882cc076",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49baa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(files_path,'prob_dict.pkl'), 'rb') as f:\n",
    "    prob_dict = pickle.load(f)\n",
    "    \n",
    "items_values= pd.read_csv(Path(files_path,'items_values.csv'))\n",
    "\n",
    "items_array = items_values.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0314d1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_prob_dict  = list(sorted(prob_dict.items(), key=lambda item: item[1],reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae2fac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_index_dict = {} # dictionary where the key is the item id and the value is the popularity of the item\n",
    "for i in range(len(sorted_prob_dict)):\n",
    "    pop_index_dict[sorted_prob_dict[i][0]] = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afd7086",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(files_path,'user_similarities_Jaccard.pkl'), 'rb') as f:\n",
    "    user_based_Jaccard_sim = pickle.load(f) # deserialize using load()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f849a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(files_path,'cosine_items.pkl'), 'rb') as f:\n",
    "    cosine_items = pickle.load(f) # deserialize using load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6685cefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'tf_idf_items.pkl'\n",
    "with open(Path(files_path,file_path), 'rb') as f:\n",
    "    tf_idf_items = pickle.load(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde80374",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'shap_values.pkl'\n",
    "with open(Path(files_path,file_path), 'rb') as f:\n",
    "    shap_values = pickle.load(f)\n",
    "\n",
    "file_path = 'item_to_cluster.pkl'\n",
    "with open(Path(files_path,file_path), 'rb') as f:\n",
    "    item_to_cluster = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9e6130",
   "metadata": {},
   "source": [
    "# VAE Model definition and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21198ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb\n",
    "import importlib\n",
    "from ipynb.fs.defs.VAE import MultVAE\n",
    "importlib.reload(ipynb.fs.defs.VAE)\n",
    "from ipynb.fs.defs.VAE import MultVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86a578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config= {\n",
    "    \"data_name\": \"ml-1m\",\n",
    "    \"train_ratio\":0.8,\n",
    "  \n",
    "    \"enc_dims\": [512,128],\n",
    "    \"dropout\": 0.5,\n",
    "    \"anneal_cap\": 0.2,\n",
    "    \"total_anneal_steps\": 200000,\n",
    "  \n",
    "    \"num_epochs\": 500,\n",
    "    \"batch_size\": 512,\n",
    "    \"test_batch_size\": 512,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"early_stop\": True,\n",
    "    \"patience\": 50,\n",
    "  \n",
    "    \"top_k\": [100]\n",
    "  }\n",
    "\n",
    "\n",
    "# Train the model on GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"num_users is \", num_users)\n",
    "print(\"num_items is \", num_items)\n",
    "VAE_recommender = MultVAE(config, num_users,num_items, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eda257",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"VAE_epoch_9.pt\")\n",
    "VAE_recommender.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971332b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in VAE_recommender.parameters():\n",
    "    param.requires_grad= False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78b5a66",
   "metadata": {},
   "source": [
    "# Import LXR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0287a992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb\n",
    "import importlib\n",
    "from ipynb.fs.defs.LXR_VAE_model import MLP_G, Recommender_G, Explainer_G, LossModelCombined\n",
    "importlib.reload(ipynb.fs.defs.LXR_VAE_model)\n",
    "from ipynb.fs.defs.LXR_VAE_model import MLP_G, Recommender_G, Explainer_G, LossModelCombined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a91800",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim=20\n",
    "backbone = Recommender_G(num_items, hidden_dim, device)\n",
    "backbone.load_state_dict(torch.load(\"MLP_backbone.pt\"))\n",
    "backbone.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a01fde",
   "metadata": {},
   "source": [
    "### Recommender Freezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629fb35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in backbone.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ae98f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_parameter = 0.008\n",
    "explainer_model_g = Explainer_G(backbone, num_items, hidden_dim, device).to(device)\n",
    "explainer_path = \"LXR_VAE_explainer_0.00125_11.pt\"\n",
    "explainer_checkpoint = torch.load(explainer_path)\n",
    "explainer_model_g.load_state_dict(explainer_checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec59c966",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelCombined_g = LossModelCombined(alpha_parameter, backbone, VAE_recommender, explainer_model_g, hidden_dim, device).to(device)\n",
    "lxr_path = \"LXR_VAE_0.00125_11.pt\"\n",
    "lxr_checkpoint = torch.load(lxr_path)\n",
    "modelCombined_g.load_state_dict(lxr_checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738eb37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in modelCombined_g.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56512b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in explainer_model_g.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e969ab1",
   "metadata": {},
   "source": [
    "# Init LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383a8fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb\n",
    "from ipynb.fs.defs.lime import distance_to_proximity, LimeBase, get_lime_args, gaussian_kernel\n",
    "importlib.reload(ipynb.fs.defs.lime)\n",
    "from ipynb.fs.defs.lime import distance_to_proximity, LimeBase, get_lime_args, gaussian_kernel\n",
    "'''\n",
    "distance_to_proximity(distances_list) - takes distances from origin user and returns proximity\n",
    "LimeBase() - class that gets kernel function\n",
    "get_lime_args(user_vetor, item_id, model, min_pert = 10, max_pert = 20, num_of_perturbations = 5, seed = 0) - \n",
    "    returns neighborhood_data, neighborhood_labels, distances, item_id\n",
    "'''\n",
    "\n",
    "lime = LimeBase(distance_to_proximity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b55b34e",
   "metadata": {},
   "source": [
    "# Mask functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9af95e",
   "metadata": {},
   "source": [
    "#### LXR based similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988248e6-ac2d-405a-94ec-bd7ca2761841",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_LXR_mask(x, item_id, item_tensor, modelCombined_g):\n",
    "    user_hist = x\n",
    "    user_hist[item_id] = 0\n",
    "    x_masked_g, loss,_,_ = modelCombined_g(user_hist, item_tensor, item_id, device)\n",
    "    item_sim_dict = {i: x_masked_g[i].item() for i in range(x_masked_g.numel())}    \n",
    "\n",
    "    return item_sim_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91e0175",
   "metadata": {},
   "source": [
    "#### User based similarities using Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf28d4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_jaccard_u_mask(x, item_id, num_items, user_based_Jaccard_sim):\n",
    "    user_hist = x # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    item_jaccard_dict = {}\n",
    "    for i in range(num_items): \n",
    "        if(user_hist[i] == 1): # iterate all positive items of the user\n",
    "            if((i,item_id) in user_based_Jaccard_sim.keys()):\n",
    "                item_jaccard_dict[i]=user_based_Jaccard_sim[(i,item_id)] # add Jaccard similarity between items\n",
    "            else:\n",
    "                item_jaccard_dict[i] = 0\n",
    "            \n",
    "    return item_jaccard_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9d3949",
   "metadata": {},
   "source": [
    "#### Cosine based similarities between users and items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad6767b-0c7b-49de-8a9e-625ddb771891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_cosine_mask(x, item_id, num_items, item_cosine):\n",
    "    user_hist = x # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    item_cosine_dict = {}\n",
    "        \n",
    "    for i in range(num_items): \n",
    "        if(user_hist[i] == 1): # iterate all positive items of the user\n",
    "            if((i,item_id) in item_cosine.keys()):\n",
    "                item_cosine_dict[i]=item_cosine[(i,item_id)] # add cosine similarity between items\n",
    "            else:\n",
    "                item_cosine_dict[i] = 0\n",
    "    \n",
    "    return item_cosine_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad1eb33",
   "metadata": {},
   "source": [
    "#### Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960f4701-a38b-485d-bc34-942d4fe5e6f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def find_pop_mask(x, item_id, num_items):\n",
    "    user_hist = torch.tensor(x) # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    item_pop_dict = {}\n",
    "    for i in range(num_items): \n",
    "        if(user_hist[i] == 1): # iterate over all positive items of the user\n",
    "            item_pop_dict[i]=prob_dict[i] # add the pop of the item to the dictionary\n",
    "            \n",
    "            \n",
    "    return item_pop_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90b4e69",
   "metadata": {},
   "source": [
    "#### Lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9fb4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lime_mask(x, item_id, min_pert, max_pert, num_of_perturbations, kernel_func, feature_selection, recommender, num_samples=10, method = 'POS'):\n",
    "    user_hist = x # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    lime.kernel_fn = kernel_func\n",
    "    neighborhood_data, neighborhood_labels, distances, item_id = get_lime_args(user_hist, item_id, recommender, min_pert = min_pert, max_pert = max_pert, num_of_perturbations = num_of_perturbations, seed = item_id)\n",
    "    if method=='POS':\n",
    "        most_pop_items  = lime.explain_instance_with_data(neighborhood_data, neighborhood_labels, distances, item_id, num_samples, feature_selection, pos_neg='POS')\n",
    "    if method=='NEG':\n",
    "        most_pop_items  = lime.explain_instance_with_data(neighborhood_data, neighborhood_labels, distances, item_id, num_samples, feature_selection ,pos_neg='NEG')\n",
    "        \n",
    "    return most_pop_items "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cf7843",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dda1f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tf_idf_mask(x, item_id, num_items, tf_idf_sim):\n",
    "    x = x.cpu().detach().numpy()\n",
    "    x[item_id] = 0\n",
    "   \n",
    "    positive_items = np.where(x == 1)[0]\n",
    "    tf_idf_dict = {i: tf_idf_sim.get((i, item_id), 0) for i in positive_items}\n",
    "   \n",
    "    return tf_idf_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83464ce4",
   "metadata": {},
   "source": [
    "#### SHAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00546663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_shapley_mask(user_vector, user_id, model, shap_values, item_to_cluster):\n",
    "        item_shap = {}\n",
    "        shapley_values = shap_values[shap_values[:, 0].astype(int) == user_id][:,1:].flatten()\n",
    "        user_vector = user_vector.cpu().detach().numpy().astype(int)\n",
    "        for i in np.where(user_vector.astype(int) == 1)[0]:\n",
    "            items_cluster = item_to_cluster[i]\n",
    "            item_shap[i] = shapley_values[items_cluster]\n",
    "        return item_shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44abf508",
   "metadata": {},
   "source": [
    "# Utils functions for evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188f5428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VAE_get_top_k(user_tensor, original_user_vector, num_items, model, top_k):\n",
    "    item_prob_dict = {}\n",
    "    output_model = [float(i) for i in softmax(model(user_tensor)[0]).cpu().detach().numpy()]\n",
    "    original_user_vector = np.array(original_user_vector.cpu())\n",
    "    neg = np.ones_like(original_user_vector)- original_user_vector\n",
    "    output = neg*output_model\n",
    "    for i in range(len(output)):\n",
    "        if output[i] > 0:\n",
    "            item_prob_dict[i]=output[i]\n",
    "    sorted_items_by_prob  = sorted(item_prob_dict.items(), key=lambda item: item[1],reverse=True)\n",
    "    top_k = min(top_k, len(sorted_items_by_prob))\n",
    "    return dict(sorted_items_by_prob[0:top_k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e61aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VAE_get_index_in_the_list(user_tensor, item_id, num_items, original_user_tensor, recommender):\n",
    "    return list(VAE_get_top_k(user_tensor, original_user_tensor, num_items, recommender, num_items).keys()).index(item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c1c3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_recommended_item(user_tensor, recommender):\n",
    "    user_res = softmax(recommender(user_tensor)[0])\n",
    "    user_catalog = torch.ones_like(user_tensor)-user_tensor\n",
    "    user_recommenations = torch.mul(user_res, user_catalog)\n",
    "    return(torch.argmax(user_recommenations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5addf221-c6e9-49d1-9c2f-c0776c2aff0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_ndcg(ranked_list, target_item):\n",
    "    if target_item not in ranked_list:\n",
    "        return 0.0\n",
    "\n",
    "    target_idx = torch.tensor(ranked_list.index(target_item), device=device)\n",
    "    dcg = torch.reciprocal(torch.log2(target_idx + 2))\n",
    "\n",
    "    return dcg.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9891668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_array = train_data_mixed.to_numpy()\n",
    "full_test_array = test_data.to_numpy()\n",
    "\n",
    "topk_train = {}\n",
    "for i in range(len(full_train_array)):\n",
    "    vec = full_train_array[i][:-3]\n",
    "    user_id = full_train_array[i][-3]\n",
    "    tens = torch.Tensor(vec).to(device)\n",
    "    topk_train[user_id] = int(get_user_recommended_item(tens, VAE_recommender).cpu().detach().numpy())\n",
    "    \n",
    "topk_test = {}\n",
    "for i in range(len(full_test_array)):\n",
    "    vec = full_test_array[i][:-2]\n",
    "    user_id = full_test_array[i][-2]\n",
    "    tens = torch.Tensor(vec).to(device)\n",
    "    topk_test[user_id] = int(get_user_recommended_item(tens, VAE_recommender).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dce6e6a-87b9-41de-86c3-039b231683ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_user_metrics(user_vector, item_id, num_of_bins, num_items, recommender_model, model_combined, user_id=None,\n",
    "                        mask_type=None):\n",
    "    \"\"\"\n",
    "    calculates all the metrics for a single user and a single masking method\n",
    "    \"\"\"\n",
    "    user_tensor = torch.Tensor(user_vector).to(device)\n",
    "    POS_masked = user_tensor\n",
    "    NEG_masked = user_tensor\n",
    "    POS_masked[item_id] = 0\n",
    "    NEG_masked[item_id] = 0\n",
    "    item_vector = items_array[item_id]\n",
    "    item_tensor = torch.FloatTensor(item_vector).to(device)\n",
    "    user_hist_size = np.sum(user_vector)\n",
    "\n",
    "    bins = [0] + [len(x) for x in np.array_split(np.arange(user_hist_size), num_of_bins, axis=0)]\n",
    "\n",
    "    POS_at_1 = [0] * (len(bins))\n",
    "    POS_at_5 = [0] * (len(bins))\n",
    "    POS_at_10 = [0] * (len(bins))\n",
    "    POS_at_20 = [0] * (len(bins))\n",
    "    POS_at_50 = [0] * (len(bins))\n",
    "    POS_at_100 = [0] * (len(bins))\n",
    "\n",
    "    NEG_at_1 = [0] * (len(bins))\n",
    "    NEG_at_5 = [0] * (len(bins))\n",
    "    NEG_at_10 = [0] * (len(bins))\n",
    "    NEG_at_20 = [0] * (len(bins))\n",
    "    NEG_at_50 = [0] * (len(bins))\n",
    "    NEG_at_100 = [0] * (len(bins))\n",
    "\n",
    "    DEL = [0] * (len(bins))\n",
    "    INS = [0] * (len(bins))\n",
    "\n",
    "    rankA_at_1 = [0] * (len(bins))\n",
    "    rankA_at_5 = [0] * (len(bins))\n",
    "    rankA_at_10 = [0] * (len(bins))\n",
    "    rankA_at_20 = [0] * (len(bins))\n",
    "    rankA_at_50 = [0] * (len(bins))\n",
    "    rankA_at_100 = [0] * (len(bins))\n",
    "\n",
    "    rankB = [0] * (len(bins))\n",
    "\n",
    "    NDCG_at_1 = [0] * (len(bins))\n",
    "    NDCG_at_5 = [0] * (len(bins))\n",
    "    NDCG_at_10 = [0] * (len(bins))\n",
    "    NDCG_at_20 = [0] * (len(bins))\n",
    "    NDCG_at_50 = [0] * (len(bins))\n",
    "    NDCG_at_100 = [0] * (len(bins))\n",
    "\n",
    "    if mask_type == 'lime':\n",
    "        POS_sim_items = find_lime_mask(user_vector, item_id, 50, 100, 400, distance_to_proximity, 'highest_weights',\n",
    "                                       recommender_model, num_samples=user_hist_size)\n",
    "        NEG_sim_items = find_lime_mask(user_vector, item_id, 50, 100, 400, distance_to_proximity, 'highest_weights',\n",
    "                                       recommender_model, num_samples=user_hist_size, method='NEG')\n",
    "\n",
    "    else:\n",
    "        if mask_type == 'jaccard_u':\n",
    "            sim_items = find_jaccard_u_mask(user_tensor, item_id, num_items, user_based_Jaccard_sim)\n",
    "        elif mask_type == 'cosine':\n",
    "            sim_items = find_cosine_mask(user_tensor, item_id, num_items, cosine_items)\n",
    "        elif mask_type == 'pop':\n",
    "            sim_items = find_pop_mask(user_tensor, item_id, num_items)\n",
    "        elif mask_type == 'lxr':\n",
    "            sim_items = find_LXR_mask(user_tensor, item_id, item_tensor, model_combined)\n",
    "        elif mask_type == 'tf_idf':\n",
    "            sim_items = find_tf_idf_mask(user_tensor, item_id, num_items, tf_idf_items)\n",
    "        elif mask_type == 'shap':\n",
    "            sim_items = find_shapley_mask(user_tensor, user_id, recommender_model, shap_values, item_to_cluster)\n",
    "\n",
    "        POS_sim_items = list(sorted(sim_items.items(), key=lambda item: item[1], reverse=True))[0:user_hist_size]\n",
    "        NEG_sim_items = list(sorted(dict(POS_sim_items).items(), key=lambda item: item[1], reverse=False))\n",
    "\n",
    "    total_items = 0\n",
    "    for i in range(len(bins)):\n",
    "        total_items += bins[i]\n",
    "\n",
    "        POS_masked = torch.zeros_like(user_tensor, dtype=torch.float32, device=device)\n",
    "        for j in POS_sim_items[:total_items]:\n",
    "            POS_masked[j[0]] = 1\n",
    "        POS_masked = user_tensor - POS_masked  # remove the masked items from the user history\n",
    "\n",
    "        NEG_masked = torch.zeros_like(user_tensor, dtype=torch.float32, device=device)\n",
    "        for j in NEG_sim_items[:total_items]:\n",
    "            NEG_masked[j[0]] = 1\n",
    "        NEG_masked = user_tensor - NEG_masked  # remove the masked items from the user history \n",
    "\n",
    "        POS_ranked_list = VAE_get_top_k(POS_masked, user_tensor, num_items, recommender_model, num_items)\n",
    "        POS_index = list(POS_ranked_list.keys()).index(item_id) + 1\n",
    "        NEG_index = VAE_get_index_in_the_list(NEG_masked, item_id, num_items, user_tensor, recommender_model) + 1\n",
    "\n",
    "        # for pos:\n",
    "        POS_at_1[i] = 1 if POS_index <= 1 else 0\n",
    "        POS_at_5[i] = 1 if POS_index <= 5 else 0\n",
    "        POS_at_10[i] = 1 if POS_index <= 10 else 0\n",
    "        POS_at_20[i] = 1 if POS_index <= 20 else 0\n",
    "        POS_at_50[i] = 1 if POS_index <= 50 else 0\n",
    "        POS_at_100[i] = 1 if POS_index <= 100 else 0\n",
    "\n",
    "        # for neg:\n",
    "        NEG_at_1[i] = 1 if NEG_index <= 1 else 0\n",
    "        NEG_at_5[i] = 1 if NEG_index <= 5 else 0\n",
    "        NEG_at_10[i] = 1 if NEG_index <= 10 else 0\n",
    "        NEG_at_20[i] = 1 if NEG_index <= 20 else 0\n",
    "        NEG_at_50[i] = 1 if NEG_index <= 50 else 0\n",
    "        NEG_at_100[i] = 1 if NEG_index <= 100 else 0\n",
    "\n",
    "        # for del:\n",
    "        DEL[i] = float(softmax(recommender_model(POS_masked)[0])[item_id].detach().cpu().numpy())\n",
    "\n",
    "        # for ins:\n",
    "        INS[i] = float(softmax(recommender_model(user_tensor - POS_masked)[0])[item_id].detach().cpu().numpy())\n",
    "\n",
    "        # for rankA:\n",
    "        rankA_at_1[i] = max(0, (1 + 1 - POS_index) / 10)\n",
    "        rankA_at_5[i] = max(0, (5 + 1 - POS_index) / 20)\n",
    "        rankA_at_10[i] = max(0, (10 + 1 - POS_index) / 10)\n",
    "        rankA_at_20[i] = max(0, (20 + 1 - POS_index) / 20)\n",
    "        rankA_at_50[i] = max(0, (50 + 1 - POS_index) / 50)\n",
    "        rankA_at_100[i] = max(0, (100 + 1 - POS_index) / 100)\n",
    "\n",
    "        # for rankB:\n",
    "        rankB[i] = 1 / POS_index\n",
    "\n",
    "        # for NDCG:\n",
    "        NDCG_at_1[i] = get_ndcg(list(POS_ranked_list.keys())[:1], item_id)\n",
    "        NDCG_at_5[i] = get_ndcg(list(POS_ranked_list.keys())[:5], item_id)\n",
    "        NDCG_at_10[i] = get_ndcg(list(POS_ranked_list.keys())[:10], item_id)\n",
    "        NDCG_at_20[i] = get_ndcg(list(POS_ranked_list.keys())[:20], item_id)\n",
    "        NDCG_at_50[i] = get_ndcg(list(POS_ranked_list.keys())[:50], item_id)\n",
    "        NDCG_at_100[i] = get_ndcg(list(POS_ranked_list.keys())[:100], item_id)\n",
    "\n",
    "    res = [POS_at_1, POS_at_5, POS_at_10, POS_at_20, POS_at_50, POS_at_100, NEG_at_1, NEG_at_5, NEG_at_10, NEG_at_20,\n",
    "           NEG_at_50, NEG_at_100, DEL, INS, rankA_at_1, rankA_at_5, rankA_at_10, rankA_at_20, rankA_at_50, rankA_at_100,\n",
    "           rankB, NDCG_at_1, NDCG_at_5, NDCG_at_10, NDCG_at_20, NDCG_at_50, NDCG_at_100]\n",
    "    for i in range(len(res)):\n",
    "        res[i] = np.array(res[i])\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1f3a94",
   "metadata": {},
   "source": [
    "# Evaluate on all baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2961309",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "VAE_recommender.eval()\n",
    "modelCombined_g.eval()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss_fn = nn.BCELoss()\n",
    "test_losses = []\n",
    "y_true = []\n",
    "y_pred = []\n",
    "correct = 0\n",
    "threshold = 0.5\n",
    "total = len(test_data)\n",
    "num_of_bins = 11\n",
    "k = 10\n",
    "\n",
    "POS_at_1_j_u = np.zeros(num_of_bins)\n",
    "pos_at_5_j_u = np.zeros(num_of_bins)\n",
    "POS_at_10_j_u = np.zeros(num_of_bins)\n",
    "POS_at_20_j_u = np.zeros(num_of_bins)\n",
    "POS_at_50_j_u = np.zeros(num_of_bins)\n",
    "POS_at_100_j_u = np.zeros(num_of_bins)\n",
    "NEG_at_1_j_u = np.zeros(num_of_bins)\n",
    "NEG_at_5_j_u = np.zeros(num_of_bins)\n",
    "NEG_at_10_j_u = np.zeros(num_of_bins)\n",
    "NEG_at_20_j_u = np.zeros(num_of_bins)\n",
    "NEG_at_50_j_u = np.zeros(num_of_bins)\n",
    "NEG_at_100_j_u = np.zeros(num_of_bins)\n",
    "users_DEL_j_u = np.zeros(num_of_bins)\n",
    "users_INS_j_u = np.zeros(num_of_bins)\n",
    "rank_at_1_j_u = np.zeros(num_of_bins)\n",
    "rank_at_5_j_u = np.zeros(num_of_bins)\n",
    "rank_at_10_A_j_u = np.zeros(num_of_bins)\n",
    "rank_at_20_A_j_u = np.zeros(num_of_bins)\n",
    "rank_at_50_A_j_u = np.zeros(num_of_bins)\n",
    "rank_at_100_A_j_u = np.zeros(num_of_bins)\n",
    "rank_at_k_B_j_u = np.zeros(num_of_bins)\n",
    "NDCG_at_1_j_u = np.zeros(num_of_bins)\n",
    "NDCG_at_5_j_u = np.zeros(num_of_bins)\n",
    "NDCG_at_10_j_u = np.zeros(num_of_bins)\n",
    "NDCG_at_20_j_u = np.zeros(num_of_bins)\n",
    "NDCG_at_50_j_u = np.zeros(num_of_bins)\n",
    "NDCG_at_100_j_u = np.zeros(num_of_bins)\n",
    "\n",
    "\n",
    "POS_at_1_c_s = np.zeros(num_of_bins)\n",
    "pos_at_5_c_s = np.zeros(num_of_bins)\n",
    "POS_at_10_c_s = np.zeros(num_of_bins)\n",
    "POS_at_20_c_s = np.zeros(num_of_bins)\n",
    "POS_at_50_c_s = np.zeros(num_of_bins)\n",
    "POS_at_100_c_s = np.zeros(num_of_bins)\n",
    "NEG_at_1_c_s = np.zeros(num_of_bins)\n",
    "NEG_at_5_c_s = np.zeros(num_of_bins)\n",
    "NEG_at_10_c_s = np.zeros(num_of_bins)\n",
    "NEG_at_20_c_s= np.zeros(num_of_bins)\n",
    "NEG_at_50_c_s = np.zeros(num_of_bins)\n",
    "NEG_at_100_c_s = np.zeros(num_of_bins)\n",
    "users_DEL_c_s = np.zeros(num_of_bins)\n",
    "users_INS_c_s = np.zeros(num_of_bins)\n",
    "rank_at_1_c_s = np.zeros(num_of_bins)\n",
    "rank_at_5_c_s = np.zeros(num_of_bins)\n",
    "rank_at_10_A_c_s = np.zeros(num_of_bins)\n",
    "rank_at_20_A_c_s = np.zeros(num_of_bins)\n",
    "rank_at_50_A_c_s = np.zeros(num_of_bins)\n",
    "rank_at_100_A_c_s = np.zeros(num_of_bins)\n",
    "rank_at_k_B_c_s = np.zeros(num_of_bins)\n",
    "NDCG_at_1_c_s = np.zeros(num_of_bins)\n",
    "NDCG_at_5_c_s = np.zeros(num_of_bins)\n",
    "NDCG_at_10_c_s = np.zeros(num_of_bins)\n",
    "NDCG_at_20_c_s = np.zeros(num_of_bins)\n",
    "NDCG_at_50_c_s = np.zeros(num_of_bins)\n",
    "NDCG_at_100_c_s = np.zeros(num_of_bins)\n",
    "\n",
    "\n",
    "POS_at_1_pop = np.zeros(num_of_bins)\n",
    "pos_at_5_pop = np.zeros(num_of_bins)\n",
    "POS_at_10_pop = np.zeros(num_of_bins)\n",
    "POS_at_20_pop = np.zeros(num_of_bins)\n",
    "POS_at_50_pop = np.zeros(num_of_bins)\n",
    "POS_at_100_pop = np.zeros(num_of_bins)\n",
    "NEG_at_1_pop = np.zeros(num_of_bins)\n",
    "NEG_at_5_pop = np.zeros(num_of_bins)\n",
    "NEG_at_10_pop = np.zeros(num_of_bins)\n",
    "NEG_at_20_pop = np.zeros(num_of_bins)\n",
    "NEG_at_50_pop = np.zeros(num_of_bins)\n",
    "NEG_at_100_pop = np.zeros(num_of_bins)\n",
    "users_DEL_pop = np.zeros(num_of_bins)\n",
    "users_INS_pop = np.zeros(num_of_bins)\n",
    "rank_at_1_pop = np.zeros(num_of_bins)\n",
    "rank_at_5_pop = np.zeros(num_of_bins)\n",
    "rank_at_10_A_pop = np.zeros(num_of_bins)\n",
    "rank_at_20_A_pop = np.zeros(num_of_bins)\n",
    "rank_at_50_A_pop = np.zeros(num_of_bins)\n",
    "rank_at_100_A_pop = np.zeros(num_of_bins)\n",
    "rank_at_k_B_pop = np.zeros(num_of_bins)\n",
    "NDCG_at_1_pop = np.zeros(num_of_bins)\n",
    "NDCG_at_5_pop = np.zeros(num_of_bins)\n",
    "NDCG_at_10_pop = np.zeros(num_of_bins)\n",
    "NDCG_at_20_pop = np.zeros(num_of_bins)\n",
    "NDCG_at_50_pop = np.zeros(num_of_bins)\n",
    "NDCG_at_100_pop = np.zeros(num_of_bins)\n",
    "\n",
    "POS_at_1_lime = np.zeros(num_of_bins)\n",
    "pos_at_5_lime = np.zeros(num_of_bins)\n",
    "POS_at_10_lime = np.zeros(num_of_bins)\n",
    "POS_at_20_lime = np.zeros(num_of_bins)\n",
    "POS_at_50_lime = np.zeros(num_of_bins)\n",
    "POS_at_100_lime = np.zeros(num_of_bins)\n",
    "NEG_at_1_lime = np.zeros(num_of_bins)\n",
    "NEG_at_5_lime = np.zeros(num_of_bins)\n",
    "NEG_at_10_lime = np.zeros(num_of_bins)\n",
    "NEG_at_20_lime = np.zeros(num_of_bins)\n",
    "NEG_at_50_lime = np.zeros(num_of_bins)\n",
    "NEG_at_100_lime = np.zeros(num_of_bins)\n",
    "users_DEL_lime = np.zeros(num_of_bins)\n",
    "users_INS_lime = np.zeros(num_of_bins)\n",
    "rank_at_1_lime = np.zeros(num_of_bins)\n",
    "rank_at_5_lime = np.zeros(num_of_bins)\n",
    "rank_at_10_A_lime = np.zeros(num_of_bins)\n",
    "rank_at_20_A_lime = np.zeros(num_of_bins)\n",
    "rank_at_50_A_lime = np.zeros(num_of_bins)\n",
    "rank_at_100_A_lime = np.zeros(num_of_bins)\n",
    "rank_at_k_B_lime = np.zeros(num_of_bins)\n",
    "NDCG_at_1_lime = np.zeros(num_of_bins)\n",
    "NDCG_at_5_lime = np.zeros(num_of_bins)\n",
    "NDCG_at_10_lime = np.zeros(num_of_bins)\n",
    "NDCG_at_20_lime = np.zeros(num_of_bins)\n",
    "NDCG_at_50_lime = np.zeros(num_of_bins)\n",
    "NDCG_at_100_lime = np.zeros(num_of_bins)\n",
    "\n",
    "POS_at_1_tf_idf = np.zeros(num_of_bins)\n",
    "pos_at_5_tf_idf = np.zeros(num_of_bins)\n",
    "POS_at_10_tf_idf = np.zeros(num_of_bins)\n",
    "POS_at_20_tf_idf = np.zeros(num_of_bins)\n",
    "POS_at_50_tf_idf = np.zeros(num_of_bins)\n",
    "POS_at_100_tf_idf = np.zeros(num_of_bins)\n",
    "NEG_at_1_tf_idf = np.zeros(num_of_bins)\n",
    "NEG_at_5_tf_idf = np.zeros(num_of_bins)\n",
    "NEG_at_10_tf_idf = np.zeros(num_of_bins)\n",
    "NEG_at_20_tf_idf = np.zeros(num_of_bins)\n",
    "NEG_at_50_tf_idf = np.zeros(num_of_bins)\n",
    "NEG_at_100_tf_idf = np.zeros(num_of_bins)\n",
    "users_DEL_tf_idf = np.zeros(num_of_bins)\n",
    "users_INS_tf_idf = np.zeros(num_of_bins)\n",
    "rank_at_1_tf_idf = np.zeros(num_of_bins)\n",
    "rank_at_5_tf_idf = np.zeros(num_of_bins)\n",
    "rank_at_10_A_tf_idf = np.zeros(num_of_bins)\n",
    "rank_at_20_A_tf_idf = np.zeros(num_of_bins)\n",
    "rank_at_50_A_tf_idf = np.zeros(num_of_bins)\n",
    "rank_at_100_A_tf_idf = np.zeros(num_of_bins)\n",
    "rank_at_k_B_tf_idf = np.zeros(num_of_bins)\n",
    "NDCG_at_1_tf_idf = np.zeros(num_of_bins)\n",
    "NDCG_at_5_tf_idf = np.zeros(num_of_bins)\n",
    "NDCG_at_10_tf_idf = np.zeros(num_of_bins)\n",
    "NDCG_at_20_tf_idf = np.zeros(num_of_bins)\n",
    "NDCG_at_50_tf_idf = np.zeros(num_of_bins)\n",
    "NDCG_at_100_tf_idf = np.zeros(num_of_bins)\n",
    "\n",
    "POS_at_1_shap = np.zeros(num_of_bins)\n",
    "pos_at_5_shap = np.zeros(num_of_bins)\n",
    "POS_at_10_shap = np.zeros(num_of_bins)\n",
    "POS_at_20_shap = np.zeros(num_of_bins)\n",
    "POS_at_50_shap = np.zeros(num_of_bins)\n",
    "POS_at_100_shap = np.zeros(num_of_bins)\n",
    "NEG_at_1_shap = np.zeros(num_of_bins)\n",
    "NEG_at_5_shap = np.zeros(num_of_bins)\n",
    "NEG_at_10_shap = np.zeros(num_of_bins)\n",
    "NEG_at_20_shap = np.zeros(num_of_bins)\n",
    "NEG_at_50_shap = np.zeros(num_of_bins)\n",
    "NEG_at_100_shap = np.zeros(num_of_bins)\n",
    "users_DEL_shap = np.zeros(num_of_bins)\n",
    "users_INS_shap = np.zeros(num_of_bins)\n",
    "rank_at_1_shap = np.zeros(num_of_bins)\n",
    "rank_at_5_shap = np.zeros(num_of_bins)\n",
    "rank_at_10_A_shap = np.zeros(num_of_bins)\n",
    "rank_at_20_A_shap = np.zeros(num_of_bins)\n",
    "rank_at_50_A_shap = np.zeros(num_of_bins)\n",
    "rank_at_100_A_shap = np.zeros(num_of_bins)\n",
    "rank_at_k_B_shap = np.zeros(num_of_bins)\n",
    "NDCG_at_1_shap = np.zeros(num_of_bins)\n",
    "NDCG_at_5_shap = np.zeros(num_of_bins)\n",
    "NDCG_at_10_shap = np.zeros(num_of_bins)\n",
    "NDCG_at_20_shap = np.zeros(num_of_bins)\n",
    "NDCG_at_50_shap = np.zeros(num_of_bins)\n",
    "NDCG_at_100_shap = np.zeros(num_of_bins)\n",
    "\n",
    "POS_at_1_lxr = np.zeros(num_of_bins)\n",
    "pos_at_5_lxr = np.zeros(num_of_bins)\n",
    "POS_at_10_lxr = np.zeros(num_of_bins)\n",
    "POS_at_20_lxr = np.zeros(num_of_bins)\n",
    "POS_at_50_lxr = np.zeros(num_of_bins)\n",
    "POS_at_100_lxr = np.zeros(num_of_bins)\n",
    "NEG_at_1_lxr = np.zeros(num_of_bins)\n",
    "NEG_at_5_lxr = np.zeros(num_of_bins)\n",
    "NEG_at_10_lxr = np.zeros(num_of_bins)\n",
    "NEG_at_20_lxr = np.zeros(num_of_bins)\n",
    "NEG_at_50_lxr = np.zeros(num_of_bins)\n",
    "NEG_at_100_lxr = np.zeros(num_of_bins)\n",
    "users_DEL_lxr = np.zeros(num_of_bins)\n",
    "users_INS_lxr = np.zeros(num_of_bins)\n",
    "rank_at_1_lxr = np.zeros(num_of_bins)\n",
    "rank_at_5_lxr = np.zeros(num_of_bins)\n",
    "rank_at_10_A_lxr = np.zeros(num_of_bins)\n",
    "rank_at_20_A_lxr = np.zeros(num_of_bins)\n",
    "rank_at_50_A_lxr = np.zeros(num_of_bins)\n",
    "rank_at_100_A_lxr = np.zeros(num_of_bins)\n",
    "rank_at_k_B_lxr = np.zeros(num_of_bins)\n",
    "NDCG_at_1_lxr = np.zeros(num_of_bins)\n",
    "NDCG_at_5_lxr = np.zeros(num_of_bins)\n",
    "NDCG_at_10_lxr = np.zeros(num_of_bins)\n",
    "NDCG_at_20_lxr = np.zeros(num_of_bins)\n",
    "NDCG_at_50_lxr = np.zeros(num_of_bins)\n",
    "NDCG_at_100_lxr = np.zeros(num_of_bins)\n",
    "\n",
    "\n",
    "num_of_bins = 10\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(test_array.shape[0]):\n",
    "        if i % 500 == 0:\n",
    "            print(i)\n",
    "        start_time = time.time()\n",
    "        user_vector = test_array[i]\n",
    "        user_tensor = torch.FloatTensor(user_vector).to(device)\n",
    "        user_id = test_user_ids[i]\n",
    "        item_id = int(get_user_recommended_item(user_tensor, VAE_recommender).detach().cpu().numpy())\n",
    "        item_vector =  items_values_dict[item_id]\n",
    "        item_tensor = torch.FloatTensor(item_vector).to(device)\n",
    "\n",
    "        user_vector[item_id] = 0\n",
    "        user_tensor [item_id] = 0\n",
    "        \n",
    "        res = single_user_metrics(user_vector, item_id, num_of_bins, num_items, VAE_recommender, modelCombined_g, mask_type= 'jaccard_u')\n",
    "        POS_at_1_j_u += res[0]\n",
    "        pos_at_5_j_u += res[1]\n",
    "        POS_at_10_j_u += res[2]\n",
    "        POS_at_20_j_u += res[3]\n",
    "        POS_at_50_j_u += res[4]\n",
    "        POS_at_100_j_u += res[5]\n",
    "        NEG_at_1_j_u += res[6]\n",
    "        NEG_at_5_j_u += res[7]\n",
    "        NEG_at_10_j_u += res[8]\n",
    "        NEG_at_20_j_u += res[9]\n",
    "        NEG_at_50_j_u += res[10]\n",
    "        NEG_at_100_j_u += res[11]\n",
    "        users_DEL_j_u += res[12]\n",
    "        users_INS_j_u += res[13]\n",
    "        rank_at_1_j_u += res[14]\n",
    "        rank_at_5_j_u += res[15]\n",
    "        rank_at_10_A_j_u += res[16]\n",
    "        rank_at_20_A_j_u += res[17]\n",
    "        rank_at_50_A_j_u += res[18]\n",
    "        rank_at_100_A_j_u += res[19]\n",
    "        rank_at_k_B_j_u += res[20]\n",
    "        NDCG_at_1_j_u += res[21]\n",
    "        NDCG_at_5_j_u += res[22]\n",
    "        NDCG_at_10_j_u += res[23]\n",
    "        NDCG_at_20_j_u += res[24]\n",
    "        NDCG_at_50_j_u += res[25]\n",
    "        NDCG_at_100_j_u += res[26]\n",
    "\n",
    "\n",
    "        res = single_user_metrics(user_vector, item_id, num_of_bins, num_items, VAE_recommender, modelCombined_g, mask_type= 'cosine')\n",
    "        POS_at_1_c_s += res[0]\n",
    "        pos_at_5_c_s += res[1]\n",
    "        POS_at_10_c_s += res[2]\n",
    "        POS_at_20_c_s += res[3]\n",
    "        POS_at_50_c_s += res[4]\n",
    "        POS_at_100_c_s += res[5]\n",
    "        NEG_at_1_c_s += res[6]\n",
    "        NEG_at_5_c_s += res[7]\n",
    "        NEG_at_10_c_s += res[8]\n",
    "        NEG_at_20_c_s += res[9]\n",
    "        NEG_at_50_c_s += res[10]\n",
    "        NEG_at_100_c_s += res[11]\n",
    "        users_DEL_c_s += res[12]\n",
    "        users_INS_c_s += res[13]\n",
    "        rank_at_1_c_s += res[14]\n",
    "        rank_at_5_c_s += res[15]\n",
    "        rank_at_10_A_c_s += res[16]\n",
    "        rank_at_20_A_c_s += res[17]\n",
    "        rank_at_50_A_c_s += res[18]\n",
    "        rank_at_100_A_c_s += res[19]\n",
    "        rank_at_k_B_c_s += res[20]\n",
    "        NDCG_at_1_c_s += res[21]\n",
    "        NDCG_at_5_c_s += res[22]\n",
    "        NDCG_at_10_c_s += res[23]\n",
    "        NDCG_at_20_c_s += res[24]\n",
    "        NDCG_at_50_c_s += res[25]\n",
    "        NDCG_at_100_c_s += res[26]\n",
    "\n",
    "        res = single_user_metrics(user_vector, item_id, num_of_bins, num_items, VAE_recommender, modelCombined_g, mask_type= 'pop')\n",
    "        POS_at_1_pop += res[0]\n",
    "        pos_at_5_pop += res[1]\n",
    "        POS_at_10_pop += res[2]\n",
    "        POS_at_20_pop += res[3]\n",
    "        POS_at_50_pop += res[4]\n",
    "        POS_at_100_pop += res[5]\n",
    "        NEG_at_1_pop += res[6]\n",
    "        NEG_at_5_pop += res[7]\n",
    "        NEG_at_10_pop += res[8]\n",
    "        NEG_at_20_pop += res[9]\n",
    "        NEG_at_50_pop += res[10]\n",
    "        NEG_at_100_pop += res[11]\n",
    "        users_DEL_pop += res[12]\n",
    "        users_INS_pop += res[13]\n",
    "        rank_at_1_pop += res[14]\n",
    "        rank_at_5_pop += res[15]\n",
    "        rank_at_10_A_pop += res[16]\n",
    "        rank_at_20_A_pop += res[17]\n",
    "        rank_at_50_A_pop += res[18]\n",
    "        rank_at_100_A_pop += res[19]\n",
    "        rank_at_k_B_pop += res[20]\n",
    "        NDCG_at_1_pop += res[21]\n",
    "        NDCG_at_5_pop += res[22]\n",
    "        NDCG_at_10_pop += res[23]\n",
    "        NDCG_at_20_pop += res[24]\n",
    "        NDCG_at_50_pop += res[25]\n",
    "        NDCG_at_100_pop += res[26]\n",
    "\n",
    "        ### lime:\n",
    "        res = single_user_metrics(user_vector, item_id, num_of_bins, num_items, VAE_recommender, modelCombined_g, mask_type= 'lime')    \n",
    "        POS_at_1_lime += res[0]\n",
    "        pos_at_5_lime += res[1]\n",
    "        POS_at_10_lime += res[2]\n",
    "        POS_at_20_lime += res[3]\n",
    "        POS_at_50_lime += res[4]\n",
    "        POS_at_100_lime += res[5]\n",
    "        NEG_at_1_lime += res[6]\n",
    "        NEG_at_5_lime += res[7]\n",
    "        NEG_at_10_lime += res[8]\n",
    "        NEG_at_20_lime += res[9]\n",
    "        NEG_at_50_lime += res[10]\n",
    "        NEG_at_100_lime += res[11]\n",
    "        users_DEL_lime += res[12]\n",
    "        users_INS_lime += res[13]\n",
    "        rank_at_1_lime += res[14]\n",
    "        rank_at_5_lime += res[15]\n",
    "        rank_at_10_A_lime += res[16]\n",
    "        rank_at_20_A_lime += res[17]\n",
    "        rank_at_50_A_lime += res[18]\n",
    "        rank_at_100_A_lime += res[19]\n",
    "        rank_at_k_B_lime += res[20]\n",
    "        NDCG_at_1_lime += res[21]\n",
    "        NDCG_at_5_lime += res[22]\n",
    "        NDCG_at_10_lime += res[23]\n",
    "        NDCG_at_20_lime += res[24]\n",
    "        NDCG_at_50_lime += res[25]\n",
    "        NDCG_at_100_lime += res[26]\n",
    "\n",
    "        \n",
    "        res = single_user_metrics(user_vector, item_id, num_of_bins, num_items, VAE_recommender, modelCombined_g, mask_type= 'tf_idf')    \n",
    "        POS_at_1_tf_idf += res[0]\n",
    "        pos_at_5_tf_idf += res[1]\n",
    "        POS_at_10_tf_idf += res[2]\n",
    "        POS_at_20_tf_idf += res[3]\n",
    "        POS_at_50_tf_idf += res[4]\n",
    "        POS_at_100_tf_idf += res[5]\n",
    "        NEG_at_1_tf_idf += res[6]\n",
    "        NEG_at_5_tf_idf += res[7]\n",
    "        NEG_at_10_tf_idf += res[8]\n",
    "        NEG_at_20_tf_idf += res[9]\n",
    "        NEG_at_50_tf_idf += res[10]\n",
    "        NEG_at_100_tf_idf += res[11]\n",
    "        users_DEL_tf_idf += res[12]\n",
    "        users_INS_tf_idf += res[13]\n",
    "        rank_at_1_tf_idf += res[14]\n",
    "        rank_at_5_tf_idf += res[15]\n",
    "        rank_at_10_A_tf_idf += res[16]\n",
    "        rank_at_20_A_tf_idf += res[17]\n",
    "        rank_at_50_A_tf_idf += res[18]\n",
    "        rank_at_100_A_tf_idf += res[19]\n",
    "        rank_at_k_B_tf_idf += res[20]\n",
    "        NDCG_at_1_tf_idf += res[21]\n",
    "        NDCG_at_5_tf_idf += res[22]\n",
    "        NDCG_at_10_tf_idf += res[23]\n",
    "        NDCG_at_20_tf_idf += res[24]\n",
    "        NDCG_at_50_tf_idf += res[25]\n",
    "        NDCG_at_100_tf_idf += res[26]\n",
    "        \n",
    "        ## SHAP\n",
    "        res = single_user_metrics(user_vector,item_id, num_of_bins, num_items, VAE_recommender, modelCombined_g,  user_id =user_id, mask_type= 'shap')    \n",
    "    \n",
    "        POS_at_1_shap += res[0]\n",
    "        pos_at_5_shap += res[1]\n",
    "        POS_at_10_shap += res[2]\n",
    "        POS_at_20_shap += res[3]\n",
    "        POS_at_50_shap += res[4]\n",
    "        POS_at_100_shap += res[5]\n",
    "        NEG_at_1_shap += res[6]\n",
    "        NEG_at_5_shap += res[7]\n",
    "        NEG_at_10_shap += res[8]\n",
    "        NEG_at_20_shap += res[9]\n",
    "        NEG_at_50_shap += res[10]\n",
    "        NEG_at_100_shap += res[11]\n",
    "        users_DEL_shap += res[12]\n",
    "        users_INS_shap += res[13]\n",
    "        rank_at_1_shap += res[14]\n",
    "        rank_at_5_shap += res[15]\n",
    "        rank_at_10_A_shap += res[16]\n",
    "        rank_at_20_A_shap += res[17]\n",
    "        rank_at_50_A_shap += res[18]\n",
    "        rank_at_100_A_shap += res[19]\n",
    "        rank_at_k_B_shap += res[20]\n",
    "        NDCG_at_1_shap += res[21]\n",
    "        NDCG_at_5_shap += res[22]\n",
    "        NDCG_at_10_shap += res[23]\n",
    "        NDCG_at_20_shap += res[24]\n",
    "        NDCG_at_50_shap += res[25]\n",
    "        NDCG_at_100_shap += res[26]\n",
    "\n",
    "        ### lxr:\n",
    "        res = single_user_metrics(user_vector, item_id, num_of_bins, num_items, VAE_recommender, modelCombined_g, mask_type= 'lxr')    \n",
    "        POS_at_1_lxr += res[0]\n",
    "        pos_at_5_lxr += res[1]\n",
    "        POS_at_10_lxr += res[2]\n",
    "        POS_at_20_lxr += res[3]\n",
    "        POS_at_50_lxr += res[4]\n",
    "        POS_at_100_lxr += res[5]\n",
    "        NEG_at_1_lxr += res[6]\n",
    "        NEG_at_5_lxr += res[7]\n",
    "        NEG_at_10_lxr += res[8]\n",
    "        NEG_at_20_lxr += res[9]\n",
    "        NEG_at_50_lxr += res[10]\n",
    "        NEG_at_100_lxr += res[11]\n",
    "        users_DEL_lxr += res[12]\n",
    "        users_INS_lxr += res[13]\n",
    "        rank_at_1_lxr += res[14]\n",
    "        rank_at_5_lxr += res[15]\n",
    "        rank_at_10_A_lxr += res[16]\n",
    "        rank_at_20_A_lxr += res[17]\n",
    "        rank_at_50_A_lxr += res[18]\n",
    "        rank_at_100_A_lxr += res[19]\n",
    "        rank_at_k_B_lxr += res[20]\n",
    "        NDCG_at_1_lxr += res[21]\n",
    "        NDCG_at_5_lxr += res[22]\n",
    "        NDCG_at_10_lxr += res[23]\n",
    "        NDCG_at_20_lxr += res[24]\n",
    "        NDCG_at_50_lxr += res[25]\n",
    "        NDCG_at_100_lxr += res[26]\n",
    "\n",
    "        prev_time = time.time()\n",
    "        print(\"User {}, total time: {:.2f}\".format(i,prev_time - start_time))\n",
    "    a = i+1\n",
    "\n",
    "print('POS_at_1_j_u: ', np.mean(POS_at_1_j_u[1:])/a)\n",
    "print('pos_at_5_j_u: ', np.mean(pos_at_5_j_u[1:])/a)\n",
    "print('POS_at_10_j_u: ', np.mean(POS_at_10_j_u[1:])/a)\n",
    "print('POS_at_20_j_u: ', np.mean(POS_at_20_j_u[1:])/a)\n",
    "print('POS_at_50_j_u: ', np.mean(POS_at_50_j_u[1:])/a)\n",
    "print('POS_at_100_j_u: ', np.mean(POS_at_100_j_u[1:])/a)\n",
    "print('NEG_at_1_j_u: ', np.mean(NEG_at_1_j_u[1:])/a)\n",
    "print('NEG_at_5_j_u: ', np.mean(NEG_at_5_j_u[1:])/a)\n",
    "print('NEG_at_10_j_u: ', np.mean(NEG_at_10_j_u[1:])/a)\n",
    "print('NEG_at_20_j_u: ', np.mean(NEG_at_20_j_u[1:])/a)\n",
    "print('NEG_at_50_j_u: ', np.mean(NEG_at_50_j_u[1:])/a)\n",
    "print('NEG_at_100_j_u: ', np.mean(NEG_at_100_j_u[1:])/a)\n",
    "print('users_DEL_j_u: ', np.mean(users_DEL_j_u[1:])/a)\n",
    "print('users_INS_j_u: ', np.mean(users_INS_j_u[1:])/a)\n",
    "print('rank_at_1_j_u: ', np.mean(rank_at_1_j_u[1:])/a)\n",
    "print('rank_at_5_j_u: ', np.mean(rank_at_5_j_u[1:])/a)\n",
    "print('rank_at_10_A_j_u: ', np.mean(rank_at_10_A_j_u[1:])/a)\n",
    "print('rank_at_20_A_j_u: ', np.mean(rank_at_20_A_j_u[1:])/a)\n",
    "print('rank_at_50_A_j_u: ', np.mean(rank_at_50_A_j_u[1:])/a)\n",
    "print('rank_at_100_A_j_u: ', np.mean(rank_at_100_A_j_u[1:])/a)\n",
    "print('rank_at_k_B_j_u: ', np.mean(rank_at_k_B_j_u[1:])/a)\n",
    "print('NDCG_at_1_j_u: ', np.mean(NDCG_at_1_j_u[1:])/a)\n",
    "print('NDCG_at_5_j_u: ', np.mean(NDCG_at_5_j_u[1:])/a)\n",
    "print('NDCG_at_10_j_u: ', np.mean(NDCG_at_10_j_u[1:])/a)\n",
    "print('NDCG_at_20_j_u: ', np.mean(NDCG_at_20_j_u[1:])/a)\n",
    "print('NDCG_at_50_j_u: ', np.mean(NDCG_at_50_j_u[1:])/a)\n",
    "print('NDCG_at_100_j_u: ', np.mean(NDCG_at_100_j_u[1:])/a)\n",
    "\n",
    "print('POS_at_1_j_g: ', np.mean(POS_at_1_j_g[1:])/a)\n",
    "print('pos_at_5_j_g: ', np.mean(pos_at_5_j_g[1:])/a)\n",
    "print('POS_at_10_j_g: ', np.mean(POS_at_10_j_g[1:])/a)\n",
    "print('POS_at_20_j_g: ', np.mean(POS_at_20_j_g[1:])/a)\n",
    "print('POS_at_50_j_g: ', np.mean(POS_at_50_j_g[1:])/a)\n",
    "print('POS_at_100_j_g: ', np.mean(POS_at_100_j_g[1:])/a)\n",
    "print('NEG_at_1_j_g: ', np.mean(NEG_at_1_j_g[1:])/a)\n",
    "print('NEG_at_5_j_g: ', np.mean(NEG_at_5_j_g[1:])/a)\n",
    "print('NEG_at_10_j_g: ', np.mean(NEG_at_10_j_g[1:])/a)\n",
    "print('NEG_at_20_j_g: ', np.mean(NEG_at_20_j_g[1:])/a)\n",
    "print('NEG_at_50_j_g: ', np.mean(NEG_at_50_j_g[1:])/a)\n",
    "print('NEG_at_100_j_g: ', np.mean(NEG_at_100_j_g[1:])/a)\n",
    "print('users_DEL_j_g: ', np.mean(users_DEL_j_g[1:])/a)\n",
    "print('users_INS_j_g: ', np.mean(users_INS_j_g[1:])/a)\n",
    "print('rank_at_1_j_g: ', np.mean(rank_at_1_j_g[1:])/a)\n",
    "print('rank_at_5_j_g: ', np.mean(rank_at_5_j_g[1:])/a)\n",
    "print('rank_at_10_A_j_g: ', np.mean(rank_at_10_A_j_g[1:])/a)\n",
    "print('rank_at_20_A_j_g: ', np.mean(rank_at_20_A_j_g[1:])/a)\n",
    "print('rank_at_50_A_j_g: ', np.mean(rank_at_50_A_j_g[1:])/a)\n",
    "print('rank_at_100_A_j_g: ', np.mean(rank_at_100_A_j_g[1:])/a)\n",
    "print('rank_at_k_B_j_g: ', np.mean(rank_at_k_B_j_g[1:])/a)\n",
    "print('NDCG_at_1_j_g: ', np.mean(NDCG_at_1_j_g[1:])/a)\n",
    "print('NDCG_at_5_j_g: ', np.mean(NDCG_at_5_j_g[1:])/a)\n",
    "print('NDCG_at_10_j_g: ', np.mean(NDCG_at_10_j_g[1:])/a)\n",
    "print('NDCG_at_20_j_g: ', np.mean(NDCG_at_20_j_g[1:])/a)\n",
    "print('NDCG_at_50_j_g: ', np.mean(NDCG_at_50_j_g[1:])/a)\n",
    "print('NDCG_at_100_j_g: ', np.mean(NDCG_at_100_j_g[1:])/a)\n",
    "\n",
    "print('POS_at_1_c_s: ', np.mean(POS_at_1_c_s[1:])/a)\n",
    "print('pos_at_5_c_s: ', np.mean(pos_at_5_c_s[1:])/a)\n",
    "print('POS_at_10_c_s: ', np.mean(POS_at_10_c_s[1:])/a)\n",
    "print('POS_at_20_c_s: ', np.mean(POS_at_20_c_s[1:])/a)\n",
    "print('POS_at_50_c_s: ', np.mean(POS_at_50_c_s[1:])/a)\n",
    "print('POS_at_100_c_s: ', np.mean(POS_at_100_c_s[1:])/a)\n",
    "print('NEG_at_1_c_s: ', np.mean(NEG_at_1_c_s[1:])/a)\n",
    "print('NEG_at_5_c_s: ', np.mean(NEG_at_5_c_s[1:])/a)\n",
    "print('NEG_at_10_c_s: ', np.mean(NEG_at_10_c_s[1:])/a)\n",
    "print('NEG_at_20_c_s: ', np.mean(NEG_at_20_c_s[1:])/a)\n",
    "print('NEG_at_50_c_s: ', np.mean(NEG_at_50_c_s[1:])/a)\n",
    "print('NEG_at_100_c_s: ', np.mean(NEG_at_100_c_s[1:])/a)\n",
    "print('users_DEL_c_s: ', np.mean(users_DEL_c_s[1:])/a)\n",
    "print('users_INS_c_s: ', np.mean(users_INS_c_s[1:])/a)\n",
    "print('rank_at_1_c_s: ', np.mean(rank_at_1_c_s[1:])/a)\n",
    "print('rank_at_5_c_s: ', np.mean(rank_at_5_c_s[1:])/a)\n",
    "print('rank_at_10_A_c_s: ', np.mean(rank_at_10_A_c_s[1:])/a)\n",
    "print('rank_at_20_A_c_s: ', np.mean(rank_at_20_A_c_s[1:])/a)\n",
    "print('rank_at_50_A_c_s: ', np.mean(rank_at_50_A_c_s[1:])/a)\n",
    "print('rank_at_100_A_c_s: ', np.mean(rank_at_100_A_c_s[1:])/a)\n",
    "print('rank_at_k_B_c_s: ', np.mean(rank_at_k_B_c_s[1:])/a)\n",
    "print('NDCG_at_1_c_s: ', np.mean(NDCG_at_1_c_s[1:])/a)\n",
    "print('NDCG_at_5_c_s: ', np.mean(NDCG_at_5_c_s[1:])/a)\n",
    "print('NDCG_at_10_c_s: ', np.mean(NDCG_at_10_c_s[1:])/a)\n",
    "print('NDCG_at_20_c_s: ', np.mean(NDCG_at_20_c_s[1:])/a)\n",
    "print('NDCG_at_50_c_s: ', np.mean(NDCG_at_50_c_s[1:])/a)\n",
    "print('NDCG_at_100_c_s: ', np.mean(NDCG_at_100_c_s[1:])/a)\n",
    "\n",
    "print('POS_at_1_pop: ', np.mean(POS_at_1_pop[1:])/a)\n",
    "print('pos_at_5_pop: ', np.mean(pos_at_5_pop[1:])/a)\n",
    "print('POS_at_10_pop: ', np.mean(POS_at_10_pop[1:])/a)\n",
    "print('POS_at_20_pop: ', np.mean(POS_at_20_pop[1:])/a)\n",
    "print('POS_at_50_pop: ', np.mean(POS_at_50_pop[1:])/a)\n",
    "print('POS_at_100_pop: ', np.mean(POS_at_100_pop[1:])/a)\n",
    "print('NEG_at_1_pop: ', np.mean(NEG_at_1_pop[1:])/a)\n",
    "print('NEG_at_5_pop: ', np.mean(NEG_at_5_pop[1:])/a)\n",
    "print('NEG_at_10_pop: ', np.mean(NEG_at_10_pop[1:])/a)\n",
    "print('NEG_at_20_pop: ', np.mean(NEG_at_20_pop[1:])/a)\n",
    "print('NEG_at_50_pop: ', np.mean(NEG_at_50_pop[1:])/a)\n",
    "print('NEG_at_100_pop: ', np.mean(NEG_at_100_pop[1:])/a)\n",
    "print('users_DEL_pop: ', np.mean(users_DEL_pop[1:])/a)\n",
    "print('users_INS_pop: ', np.mean(users_INS_pop[1:])/a)\n",
    "print('rank_at_1_pop: ', np.mean(rank_at_1_pop[1:])/a)\n",
    "print('rank_at_5_pop: ', np.mean(rank_at_5_pop[1:])/a)\n",
    "print('rank_at_10_A_pop: ', np.mean(rank_at_10_A_pop[1:])/a)\n",
    "print('rank_at_20_A_pop: ', np.mean(rank_at_20_A_pop[1:])/a)\n",
    "print('rank_at_50_A_pop: ', np.mean(rank_at_50_A_pop[1:])/a)\n",
    "print('rank_at_100_A_pop: ', np.mean(rank_at_100_A_pop[1:])/a)\n",
    "print('rank_at_k_B_pop: ', np.mean(rank_at_k_B_pop[1:])/a)\n",
    "print('NDCG_at_1_pop: ', np.mean(NDCG_at_1_pop[1:])/a)\n",
    "print('NDCG_at_5_pop: ', np.mean(NDCG_at_5_pop[1:])/a)\n",
    "print('NDCG_at_10_pop: ', np.mean(NDCG_at_10_pop[1:])/a)\n",
    "print('NDCG_at_20_pop: ', np.mean(NDCG_at_20_pop[1:])/a)\n",
    "print('NDCG_at_50_pop: ', np.mean(NDCG_at_50_pop[1:])/a)\n",
    "print('NDCG_at_100_pop: ', np.mean(NDCG_at_100_pop[1:])/a)\n",
    "\n",
    "print('POS_at_1_lime: ', np.mean(POS_at_1_lime[1:])/a)\n",
    "print('pos_at_5_lime: ', np.mean(pos_at_5_lime[1:])/a)\n",
    "print('POS_at_10_lime: ', np.mean(POS_at_10_lime[1:])/a)\n",
    "print('POS_at_20_lime: ', np.mean(POS_at_20_lime[1:])/a)\n",
    "print('POS_at_50_lime: ', np.mean(POS_at_50_lime[1:])/a)\n",
    "print('POS_at_100_lime: ', np.mean(POS_at_100_lime[1:])/a)\n",
    "print('NEG_at_1_lime: ', np.mean(NEG_at_1_lime[1:])/a)\n",
    "print('NEG_at_5_lime: ', np.mean(NEG_at_5_lime[1:])/a)\n",
    "print('NEG_at_10_lime: ', np.mean(NEG_at_10_lime[1:])/a)\n",
    "print('NEG_at_20_lime: ', np.mean(NEG_at_20_lime[1:])/a)\n",
    "print('NEG_at_50_lime: ', np.mean(NEG_at_50_lime[1:])/a)\n",
    "print('NEG_at_100_lime: ', np.mean(NEG_at_100_lime[1:])/a)\n",
    "print('users_DEL_lime: ', np.mean(users_DEL_lime[1:])/a)\n",
    "print('users_INS_lime: ', np.mean(users_INS_lime[1:])/a)\n",
    "print('rank_at_1_lime: ', np.mean(rank_at_1_lime[1:])/a)\n",
    "print('rank_at_5_lime: ', np.mean(rank_at_5_lime[1:])/a)\n",
    "print('rank_at_10_A_lime: ', np.mean(rank_at_10_A_lime[1:])/a)\n",
    "print('rank_at_20_A_lime: ', np.mean(rank_at_20_A_lime[1:])/a)\n",
    "print('rank_at_50_A_lime: ', np.mean(rank_at_50_A_lime[1:])/a)\n",
    "print('rank_at_100_A_lime: ', np.mean(rank_at_100_A_lime[1:])/a)\n",
    "print('rank_at_k_B_lime: ', np.mean(rank_at_k_B_lime[1:])/a)\n",
    "print('NDCG_at_1_lime: ', np.mean(NDCG_at_1_lime[1:])/a)\n",
    "print('NDCG_at_5_lime: ', np.mean(NDCG_at_5_lime[1:])/a)\n",
    "print('NDCG_at_10_lime: ', np.mean(NDCG_at_10_lime[1:])/a)\n",
    "print('NDCG_at_20_lime: ', np.mean(NDCG_at_20_lime[1:])/a)\n",
    "print('NDCG_at_50_lime: ', np.mean(NDCG_at_50_lime[1:])/a)\n",
    "print('NDCG_at_100_lime: ', np.mean(NDCG_at_100_lime[1:])/a)\n",
    "\n",
    "print('POS_at_1_tf_idf: ', np.mean(POS_at_1_tf_idf[1:])/a)\n",
    "print('pos_at_5_tf_idf: ', np.mean(pos_at_5_tf_idf[1:])/a)\n",
    "print('POS_at_10_tf_idf: ', np.mean(POS_at_10_tf_idf[1:])/a)\n",
    "print('POS_at_20_tf_idf: ', np.mean(POS_at_20_tf_idf[1:])/a)\n",
    "print('POS_at_50_tf_idf: ', np.mean(POS_at_50_tf_idf[1:])/a)\n",
    "print('POS_at_100_tf_idf: ', np.mean(POS_at_100_tf_idf[1:])/a)\n",
    "print('NEG_at_1_tf_idf: ', np.mean(NEG_at_1_tf_idf[1:])/a)\n",
    "print('NEG_at_5_tf_idf: ', np.mean(NEG_at_5_tf_idf[1:])/a)\n",
    "print('NEG_at_10_tf_idf: ', np.mean(NEG_at_10_tf_idf[1:])/a)\n",
    "print('NEG_at_20_tf_idf: ', np.mean(NEG_at_20_tf_idf[1:])/a)\n",
    "print('NEG_at_50_tf_idf: ', np.mean(NEG_at_50_tf_idf[1:])/a)\n",
    "print('NEG_at_100_tf_idf: ', np.mean(NEG_at_100_tf_idf[1:])/a)\n",
    "print('users_DEL_tf_idf: ', np.mean(users_DEL_tf_idf[1:])/a)\n",
    "print('users_INS_tf_idf: ', np.mean(users_INS_tf_idf[1:])/a)\n",
    "print('rank_at_1_tf_idf: ', np.mean(rank_at_1_tf_idf[1:])/a)\n",
    "print('rank_at_5_tf_idf: ', np.mean(rank_at_5_tf_idf[1:])/a)\n",
    "print('rank_at_10_A_tf_idf: ', np.mean(rank_at_10_A_tf_idf[1:])/a)\n",
    "print('rank_at_20_A_tf_idf: ', np.mean(rank_at_20_A_tf_idf[1:])/a)\n",
    "print('rank_at_50_A_tf_idf: ', np.mean(rank_at_50_A_tf_idf[1:])/a)\n",
    "print('rank_at_100_A_tf_idf: ', np.mean(rank_at_100_A_tf_idf[1:])/a)\n",
    "print('rank_at_k_B_tf_idf: ', np.mean(rank_at_k_B_tf_idf[1:])/a)\n",
    "print('NDCG_at_1_tf_idf: ', np.mean(NDCG_at_1_tf_idf[1:])/a)\n",
    "print('NDCG_at_5_tf_idf: ', np.mean(NDCG_at_5_tf_idf[1:])/a)\n",
    "print('NDCG_at_10_tf_idf: ', np.mean(NDCG_at_10_tf_idf[1:])/a)\n",
    "print('NDCG_at_20_tf_idf: ', np.mean(NDCG_at_20_tf_idf[1:])/a)\n",
    "print('NDCG_at_50_tf_idf: ', np.mean(NDCG_at_50_tf_idf[1:])/a)\n",
    "print('NDCG_at_100_tf_idf: ', np.mean(NDCG_at_100_tf_idf[1:])/a)\n",
    "\n",
    "print('POS_at_1_shap: ', np.mean(POS_at_1_shap[1:])/a)\n",
    "print('POS_at_5_shap: ', np.mean(pos_at_5_shap[1:])/a)\n",
    "print('POS_at_10_shap: ', np.mean(POS_at_10_shap[1:])/a)\n",
    "print('POS_at_20_shap: ', np.mean(POS_at_20_shap[1:])/a)\n",
    "print('POS_at_50_shap: ', np.mean(POS_at_50_shap[1:])/a)\n",
    "print('POS_at_100_shap: ', np.mean(POS_at_100_shap[1:])/a)\n",
    "print('NEG_at_1_shap: ', np.mean(NEG_at_1_shap[1:])/a)\n",
    "print('NEG_at_5_shap: ', np.mean(NEG_at_5_shap[1:])/a)\n",
    "print('NEG_at_10_shap: ', np.mean(NEG_at_10_shap[1:])/a)\n",
    "print('NEG_at_20_shap: ', np.mean(NEG_at_20_shap[1:])/a)\n",
    "print('NEG_at_50_shap: ', np.mean(NEG_at_50_shap[1:])/a)\n",
    "print('NEG_at_100_shap: ', np.mean(NEG_at_100_shap[1:])/a)\n",
    "print('users_DEL_shap: ', np.mean(users_DEL_shap[1:])/a)\n",
    "print('users_INS_shap: ', np.mean(users_INS_shap[1:])/a)\n",
    "print('rank_at_1_shap: ', np.mean(rank_at_1_shap[1:])/a)\n",
    "print('rank_at_5_shap: ', np.mean(rank_at_5_shap[1:])/a)\n",
    "print('rank_at_10_A_shap: ', np.mean(rank_at_10_A_shap[1:])/a)\n",
    "print('rank_at_20_A_shap: ', np.mean(rank_at_20_A_shap[1:])/a)\n",
    "print('rank_at_50_A_shap: ', np.mean(rank_at_50_A_shap[1:])/a)\n",
    "print('rank_at_100_A_shap: ', np.mean(rank_at_100_A_shap[1:])/a)\n",
    "print('rank_at_k_B_shap: ', np.mean(rank_at_k_B_shap[1:])/a)\n",
    "print('NDCG_at_1_shap: ', np.mean(NDCG_at_1_shap[1:])/a)\n",
    "print('NDCG_at_5_shap: ', np.mean(NDCG_at_5_shap[1:])/a)\n",
    "print('NDCG_at_10_shap: ', np.mean(NDCG_at_10_shap[1:])/a)\n",
    "print('NDCG_at_20_shap: ', np.mean(NDCG_at_20_shap[1:])/a)\n",
    "print('NDCG_at_50_shap: ', np.mean(NDCG_at_50_shap[1:])/a)\n",
    "print('NDCG_at_100_shap: ', np.mean(NDCG_at_100_shap[1:])/a)\n",
    "\n",
    "print('POS_at_1_lxr: ', np.mean(POS_at_1_lxr[1:])/a)\n",
    "print('pos_at_5_lxr: ', np.mean(pos_at_5_lxr[1:])/a)\n",
    "print('POS_at_10_lxr: ', np.mean(POS_at_10_lxr[1:])/a)\n",
    "print('POS_at_20_lxr: ', np.mean(POS_at_20_lxr[1:])/a)\n",
    "print('POS_at_50_lxr: ', np.mean(POS_at_50_lxr[1:])/a)\n",
    "print('POS_at_100_lxr: ', np.mean(POS_at_100_lxr[1:])/a)\n",
    "print('NEG_at_1_lxr: ', np.mean(NEG_at_1_lxr[1:])/a)\n",
    "print('NEG_at_5_lxr: ', np.mean(NEG_at_5_lxr[1:])/a)\n",
    "print('NEG_at_10_lxr: ', np.mean(NEG_at_10_lxr[1:])/a)\n",
    "print('NEG_at_20_lxr: ', np.mean(NEG_at_20_lxr[1:])/a)\n",
    "print('NEG_at_50_lxr: ', np.mean(NEG_at_50_lxr[1:])/a)\n",
    "print('NEG_at_100_lxr: ', np.mean(NEG_at_100_lxr[1:])/a)\n",
    "print('users_DEL_lxr: ', np.mean(users_DEL_lxr[1:])/a)\n",
    "print('users_INS_lxr: ', np.mean(users_INS_lxr[1:])/a)\n",
    "print('rank_at_1_lxr: ', np.mean(rank_at_1_lxr[1:])/a)\n",
    "print('rank_at_5_lxr: ', np.mean(rank_at_5_lxr[1:])/a)\n",
    "print('rank_at_10_A_lxr: ', np.mean(rank_at_10_A_lxr[1:])/a)\n",
    "print('rank_at_20_A_lxr: ', np.mean(rank_at_20_A_lxr[1:])/a)\n",
    "print('rank_at_50_A_lxr: ', np.mean(rank_at_50_A_lxr[1:])/a)\n",
    "print('rank_at_100_A_lxr: ', np.mean(rank_at_100_A_lxr[1:])/a)\n",
    "print('rank_at_k_B_lxr: ', np.mean(rank_at_k_B_lxr[1:])/a)\n",
    "print('NDCG_at_1_lxr: ', np.mean(NDCG_at_1_lxr[1:])/a)\n",
    "print('NDCG_at_5_lxr: ', np.mean(NDCG_at_5_lxr[1:])/a)\n",
    "print('NDCG_at_10_lxr: ', np.mean(NDCG_at_10_lxr[1:])/a)\n",
    "print('NDCG_at_20_lxr: ', np.mean(NDCG_at_20_lxr[1:])/a)\n",
    "print('NDCG_at_50_lxr: ', np.mean(NDCG_at_50_lxr[1:])/a)\n",
    "print('NDCG_at_100_lxr: ', np.mean(NDCG_at_100_lxr[1:])/a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c093c738",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.DataFrame(columns = ['names', 'values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c91e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['POS_at_1_j_u', 'pos_at_5_j_u', 'POS_at_10_j_u', 'POS_at_20_j_u', 'POS_at_50_j_u', 'POS_at_100_j_u', 'NEG_at_1_j_u', 'NEG_at_5_j_u', 'NEG_at_10_j_u', 'NEG_at_20_j_u', 'NEG_at_50_j_u', 'NEG_at_100_j_u', 'users_DEL_j_u', 'users_INS_j_u', 'rank_at_1_j_u', 'rank_at_5_j_u', 'rank_at_10_A_j_u', 'rank_at_20_A_j_u', 'rank_at_50_A_j_u', 'rank_at_100_A_j_u', 'rank_at_k_B_j_u', 'NDCG_at_1_j_u', 'NDCG_at_5_j_u', 'NDCG_at_10_j_u', 'NDCG_at_20_j_u', 'NDCG_at_50_j_u', 'NDCG_at_100_j_u']\n",
    "values = [POS_at_1_j_u, pos_at_5_j_u, POS_at_10_j_u, POS_at_20_j_u, POS_at_50_j_u, POS_at_100_j_u, NEG_at_1_j_u, NEG_at_5_j_u, NEG_at_10_j_u, NEG_at_20_j_u, NEG_at_50_j_u, NEG_at_100_j_u, users_DEL_j_u, users_INS_j_u, rank_at_1_j_u, rank_at_5_j_u, rank_at_10_A_j_u, rank_at_20_A_j_u, rank_at_50_A_j_u, rank_at_100_A_j_u, rank_at_k_B_j_u, NDCG_at_1_j_u, NDCG_at_5_j_u, NDCG_at_10_j_u, NDCG_at_20_j_u, NDCG_at_50_j_u, NDCG_at_100_j_u]\n",
    "\n",
    "d = {'names':names, 'values': values}\n",
    "\n",
    "temp_df = pd.DataFrame(d)\n",
    "\n",
    "\n",
    "normalized_values = []\n",
    "for i in range(len(values)):\n",
    "    lst = []\n",
    "    for j in range(len(values[i])):\n",
    "        lst.append(values[i][j]/a)\n",
    "    normalized_values.append(lst)\n",
    "\n",
    "temp_df['normalized_values'] = normalized_values\n",
    "\n",
    "AUC = []\n",
    "for i in range(len(values)):\n",
    "    sublist = temp_df['normalized_values'][i][1:]\n",
    "    AUC.append(np.mean(sublist))\n",
    "\n",
    "temp_df['AUC'] = AUC\n",
    "\n",
    "full_df = pd.concat([full_df, temp_df], ignore_index = True)\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "\n",
    "names = ['POS_at_1_j_g', 'pos_at_5_j_g', 'POS_at_10_j_g', 'POS_at_20_j_g', 'POS_at_50_j_g', 'POS_at_100_j_g', 'NEG_at_1_j_g', 'NEG_at_5_j_g', 'NEG_at_10_j_g', 'NEG_at_20_j_g', 'NEG_at_50_j_g', 'NEG_at_100_j_g', 'users_DEL_j_g', 'users_INS_j_g', 'rank_at_1_j_g', 'rank_at_5_j_g', 'rank_at_10_A_j_g', 'rank_at_20_A_j_g', 'rank_at_50_A_j_g', 'rank_at_100_A_j_g', 'rank_at_k_B_j_g', 'NDCG_at_1_j_g', 'NDCG_at_5_j_g', 'NDCG_at_10_j_g', 'NDCG_at_20_j_g', 'NDCG_at_50_j_g', 'NDCG_at_100_j_g']\n",
    "values = [POS_at_1_j_g, pos_at_5_j_g, POS_at_10_j_g, POS_at_20_j_g, POS_at_50_j_g, POS_at_100_j_g, NEG_at_1_j_g, NEG_at_5_j_g, NEG_at_10_j_g, NEG_at_20_j_g, NEG_at_50_j_g, NEG_at_100_j_g, users_DEL_j_g, users_INS_j_g, rank_at_1_j_g, rank_at_5_j_g, rank_at_10_A_j_g, rank_at_20_A_j_g, rank_at_50_A_j_g, rank_at_100_A_j_g, rank_at_k_B_j_g, NDCG_at_1_j_g, NDCG_at_5_j_g, NDCG_at_10_j_g, NDCG_at_20_j_g, NDCG_at_50_j_g, NDCG_at_100_j_g]\n",
    "\n",
    "d = {'names':names, 'values': values}\n",
    "\n",
    "temp_df = pd.DataFrame(d)\n",
    "\n",
    "\n",
    "normalized_values = []\n",
    "for i in range(len(values)):\n",
    "    lst = []\n",
    "    for j in range(len(values[i])):\n",
    "        lst.append(values[i][j]/a)\n",
    "    normalized_values.append(lst)\n",
    "\n",
    "temp_df['normalized_values'] = normalized_values\n",
    "\n",
    "AUC = []\n",
    "for i in range(len(values)):\n",
    "    sublist = temp_df['normalized_values'][i][1:]\n",
    "    AUC.append(np.mean(sublist))\n",
    "\n",
    "temp_df['AUC'] = AUC\n",
    "\n",
    "full_df = pd.concat([full_df, temp_df], ignore_index = True)\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "\n",
    "names = ['POS_at_1_c_s', 'pos_at_5_c_s', 'POS_at_10_c_s', 'POS_at_20_c_s', 'POS_at_50_c_s', 'POS_at_100_c_s', 'NEG_at_1_c_s', 'NEG_at_5_c_s', 'NEG_at_10_c_s', 'NEG_at_20_c_s', 'NEG_at_50_c_s', 'NEG_at_100_c_s', 'users_DEL_c_s', 'users_INS_c_s', 'rank_at_1_c_s', 'rank_at_5_c_s', 'rank_at_10_A_c_s', 'rank_at_20_A_c_s', 'rank_at_50_A_c_s', 'rank_at_100_A_c_s', 'rank_at_k_B_c_s', 'NDCG_at_1_c_s', 'NDCG_at_5_c_s', 'NDCG_at_10_c_s', 'NDCG_at_20_c_s', 'NDCG_at_50_c_s', 'NDCG_at_100_c_s']\n",
    "values = [POS_at_1_c_s, pos_at_5_c_s, POS_at_10_c_s, POS_at_20_c_s, POS_at_50_c_s, POS_at_100_c_s, NEG_at_1_c_s, NEG_at_5_c_s, NEG_at_10_c_s, NEG_at_20_c_s, NEG_at_50_c_s, NEG_at_100_c_s, users_DEL_c_s, users_INS_c_s, rank_at_1_c_s, rank_at_5_c_s, rank_at_10_A_c_s, rank_at_20_A_c_s, rank_at_50_A_c_s, rank_at_100_A_c_s, rank_at_k_B_c_s, NDCG_at_1_c_s, NDCG_at_5_c_s, NDCG_at_10_c_s, NDCG_at_20_c_s, NDCG_at_50_c_s, NDCG_at_100_c_s]\n",
    "\n",
    "d = {'names':names, 'values': values}\n",
    "\n",
    "temp_df = pd.DataFrame(d)\n",
    "\n",
    "\n",
    "normalized_values = []\n",
    "for i in range(len(values)):\n",
    "    lst = []\n",
    "    for j in range(len(values[i])):\n",
    "        lst.append(values[i][j]/a)\n",
    "    normalized_values.append(lst)\n",
    "\n",
    "temp_df['normalized_values'] = normalized_values\n",
    "\n",
    "AUC = []\n",
    "for i in range(len(values)):\n",
    "    sublist = temp_df['normalized_values'][i][1:]\n",
    "    AUC.append(np.mean(sublist))\n",
    "\n",
    "temp_df['AUC'] = AUC\n",
    "\n",
    "full_df = pd.concat([full_df, temp_df], ignore_index = True)\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "\n",
    "names = ['POS_at_1_pop', 'pos_at_5_pop', 'POS_at_10_pop', 'POS_at_20_pop', 'POS_at_50_pop', 'POS_at_100_pop', 'NEG_at_1_pop', 'NEG_at_5_pop', 'NEG_at_10_pop', 'NEG_at_20_pop', 'NEG_at_50_pop', 'NEG_at_100_pop', 'users_DEL_pop', 'users_INS_pop', 'rank_at_1_pop', 'rank_at_5_pop', 'rank_at_10_A_pop', 'rank_at_20_A_pop', 'rank_at_50_A_pop', 'rank_at_100_A_pop', 'rank_at_k_B_pop', 'NDCG_at_1_pop', 'NDCG_at_5_pop', 'NDCG_at_10_pop', 'NDCG_at_20_pop', 'NDCG_at_50_pop', 'NDCG_at_100_pop']\n",
    "values = [POS_at_1_pop, pos_at_5_pop, POS_at_10_pop, POS_at_20_pop, POS_at_50_pop, POS_at_100_pop, NEG_at_1_pop, NEG_at_5_pop, NEG_at_10_pop, NEG_at_20_pop, NEG_at_50_pop, NEG_at_100_pop, users_DEL_pop, users_INS_pop, rank_at_1_pop, rank_at_5_pop, rank_at_10_A_pop, rank_at_20_A_pop, rank_at_50_A_pop, rank_at_100_A_pop, rank_at_k_B_pop, NDCG_at_1_pop, NDCG_at_5_pop, NDCG_at_10_pop, NDCG_at_20_pop, NDCG_at_50_pop, NDCG_at_100_pop]\n",
    "\n",
    "d = {'names':names, 'values': values}\n",
    "\n",
    "temp_df = pd.DataFrame(d)\n",
    "\n",
    "\n",
    "normalized_values = []\n",
    "for i in range(len(values)):\n",
    "    lst = []\n",
    "    for j in range(len(values[i])):\n",
    "        lst.append(values[i][j]/a)\n",
    "    normalized_values.append(lst)\n",
    "\n",
    "temp_df['normalized_values'] = normalized_values\n",
    "\n",
    "AUC = []\n",
    "for i in range(len(values)):\n",
    "    sublist = temp_df['normalized_values'][i][1:]\n",
    "    AUC.append(np.mean(sublist))\n",
    "\n",
    "temp_df['AUC'] = AUC\n",
    "\n",
    "full_df = pd.concat([full_df, temp_df], ignore_index = True)\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "\n",
    "names = ['POS_at_1_lime', 'pos_at_5_lime', 'POS_at_10_lime', 'POS_at_20_lime', 'POS_at_50_lime', 'POS_at_100_lime', 'NEG_at_1_lime', 'NEG_at_5_lime', 'NEG_at_10_lime', 'NEG_at_20_lime', 'NEG_at_50_lime', 'NEG_at_100_lime', 'users_DEL_lime', 'users_INS_lime', 'rank_at_1_lime', 'rank_at_5_lime', 'rank_at_10_A_lime', 'rank_at_20_A_lime', 'rank_at_50_A_lime', 'rank_at_100_A_lime', 'rank_at_k_B_lime', 'NDCG_at_1_lime', 'NDCG_at_5_lime', 'NDCG_at_10_lime', 'NDCG_at_20_lime', 'NDCG_at_50_lime', 'NDCG_at_100_lime']\n",
    "values = [POS_at_1_lime, pos_at_5_lime, POS_at_10_lime, POS_at_20_lime, POS_at_50_lime, POS_at_100_lime, NEG_at_1_lime, NEG_at_5_lime, NEG_at_10_lime, NEG_at_20_lime, NEG_at_50_lime, NEG_at_100_lime, users_DEL_lime, users_INS_lime, rank_at_1_lime, rank_at_5_lime, rank_at_10_A_lime, rank_at_20_A_lime, rank_at_50_A_lime, rank_at_100_A_lime, rank_at_k_B_lime, NDCG_at_1_lime, NDCG_at_5_lime, NDCG_at_10_lime, NDCG_at_20_lime, NDCG_at_50_lime, NDCG_at_100_lime]\n",
    "\n",
    "d = {'names':names, 'values': values}\n",
    "\n",
    "temp_df = pd.DataFrame(d)\n",
    "\n",
    "\n",
    "normalized_values = []\n",
    "for i in range(len(values)):\n",
    "    lst = []\n",
    "    for j in range(len(values[i])):\n",
    "        lst.append(values[i][j]/a)\n",
    "    normalized_values.append(lst)\n",
    "\n",
    "temp_df['normalized_values'] = normalized_values\n",
    "\n",
    "AUC = []\n",
    "for i in range(len(values)):\n",
    "    sublist = temp_df['normalized_values'][i][1:]\n",
    "    AUC.append(np.mean(sublist))\n",
    "\n",
    "temp_df['AUC'] = AUC\n",
    "\n",
    "full_df = pd.concat([full_df, temp_df], ignore_index = True)\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "\n",
    "names = ['POS_at_1_tf_idf', 'pos_at_5_tf_idf', 'POS_at_10_tf_idf', 'POS_at_20_tf_idf', 'POS_at_50_tf_idf', 'POS_at_100_tf_idf', 'NEG_at_1_tf_idf', 'NEG_at_5_tf_idf', 'NEG_at_10_tf_idf', 'NEG_at_20_tf_idf', 'NEG_at_50_tf_idf', 'NEG_at_100_tf_idf', 'users_DEL_tf_idf', 'users_INS_tf_idf', 'rank_at_1_tf_idf', 'rank_at_5_tf_idf', 'rank_at_10_A_tf_idf', 'rank_at_20_A_tf_idf', 'rank_at_50_A_tf_idf', 'rank_at_100_A_tf_idf', 'rank_at_k_B_tf_idf', 'NDCG_at_1_tf_idf', 'NDCG_at_5_tf_idf', 'NDCG_at_10_tf_idf', 'NDCG_at_20_tf_idf', 'NDCG_at_50_tf_idf', 'NDCG_at_100_tf_idf']\n",
    "values = [POS_at_1_tf_idf, pos_at_5_tf_idf, POS_at_10_tf_idf, POS_at_20_tf_idf, POS_at_50_tf_idf, POS_at_100_tf_idf, NEG_at_1_tf_idf, NEG_at_5_tf_idf, NEG_at_10_tf_idf, NEG_at_20_tf_idf, NEG_at_50_tf_idf, NEG_at_100_tf_idf, users_DEL_tf_idf, users_INS_tf_idf, rank_at_1_tf_idf, rank_at_5_tf_idf, rank_at_10_A_tf_idf, rank_at_20_A_tf_idf, rank_at_50_A_tf_idf, rank_at_100_A_tf_idf, rank_at_k_B_tf_idf, NDCG_at_1_tf_idf, NDCG_at_5_tf_idf, NDCG_at_10_tf_idf, NDCG_at_20_tf_idf, NDCG_at_50_tf_idf, NDCG_at_100_tf_idf]\n",
    "\n",
    "d = {'names':names, 'values': values}\n",
    "\n",
    "temp_df = pd.DataFrame(d)\n",
    "\n",
    "\n",
    "normalized_values = []\n",
    "for i in range(len(values)):\n",
    "    lst = []\n",
    "    for j in range(len(values[i])):\n",
    "        lst.append(values[i][j]/a)\n",
    "    normalized_values.append(lst)\n",
    "\n",
    "temp_df['normalized_values'] = normalized_values\n",
    "\n",
    "AUC = []\n",
    "for i in range(len(values)):\n",
    "    sublist = temp_df['normalized_values'][i][1:]\n",
    "    AUC.append(np.mean(sublist))\n",
    "\n",
    "temp_df['AUC'] = AUC\n",
    "\n",
    "full_df = pd.concat([full_df, temp_df], ignore_index = True)\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "names = ['POS_at_1_shap', 'pos_at_5_shap', 'POS_at_10_shap', 'POS_at_20_shap', 'POS_at_50_shap', 'POS_at_100_shap', 'NEG_at_1_shap', 'NEG_at_5_shap', 'NEG_at_10_shap', 'NEG_at_20_shap', 'NEG_at_50_shap', 'NEG_at_100_shap', 'users_DEL_shap', 'users_INS_shap', 'rank_at_1_shap', 'rank_at_5_shap', 'rank_at_10_A_shap', 'rank_at_20_A_shap', 'rank_at_50_A_shap', 'rank_at_100_A_shap', 'rank_at_k_B_shap', 'NDCG_at_1_shap', 'NDCG_at_5_shap', 'NDCG_at_10_shap', 'NDCG_at_20_shap', 'NDCG_at_50_shap', 'NDCG_at_100_shap']\n",
    "values = [POS_at_1_shap, pos_at_5_shap, POS_at_10_shap, POS_at_20_shap, POS_at_50_shap, POS_at_100_shap, NEG_at_1_shap, NEG_at_5_shap, NEG_at_10_shap, NEG_at_20_shap, NEG_at_50_shap, NEG_at_100_shap, users_DEL_shap, users_INS_shap, rank_at_1_shap, rank_at_5_shap, rank_at_10_A_shap, rank_at_20_A_shap, rank_at_50_A_shap, rank_at_100_A_shap, rank_at_k_B_shap, NDCG_at_1_shap, NDCG_at_5_shap, NDCG_at_10_shap, NDCG_at_20_shap, NDCG_at_50_shap, NDCG_at_100_shap]\n",
    "\n",
    "d = {'names':names, 'values': values}\n",
    "\n",
    "temp_df = pd.DataFrame(d)\n",
    "\n",
    "\n",
    "normalized_values = []\n",
    "for i in range(len(values)):\n",
    "    lst = []\n",
    "    for j in range(len(values[i])):\n",
    "        lst.append(values[i][j]/a)\n",
    "    normalized_values.append(lst)\n",
    "\n",
    "temp_df['normalized_values'] = normalized_values\n",
    "\n",
    "AUC = []\n",
    "for i in range(len(values)):\n",
    "    sublist = temp_df['normalized_values'][i][1:]\n",
    "    AUC.append(np.mean(sublist))\n",
    "\n",
    "temp_df['AUC'] = AUC\n",
    "\n",
    "full_df = pd.concat([full_df, temp_df], ignore_index = True)\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "names = ['POS_at_1_lxr', 'pos_at_5_lxr', 'POS_at_10_lxr', 'POS_at_20_lxr', 'POS_at_50_lxr', 'POS_at_100_lxr', 'NEG_at_1_lxr', 'NEG_at_5_lxr', 'NEG_at_10_lxr', 'NEG_at_20_lxr', 'NEG_at_50_lxr', 'NEG_at_100_lxr', 'users_DEL_lxr', 'users_INS_lxr', 'rank_at_1_lxr', 'rank_at_5_lxr', 'rank_at_10_A_lxr', 'rank_at_20_A_lxr', 'rank_at_50_A_lxr', 'rank_at_100_A_lxr', 'rank_at_k_B_lxr', 'NDCG_at_1_lxr', 'NDCG_at_5_lxr', 'NDCG_at_10_lxr', 'NDCG_at_20_lxr', 'NDCG_at_50_lxr', 'NDCG_at_100_lxr']\n",
    "values = [POS_at_1_lxr, pos_at_5_lxr, POS_at_10_lxr, POS_at_20_lxr, POS_at_50_lxr, POS_at_100_lxr, NEG_at_1_lxr, NEG_at_5_lxr, NEG_at_10_lxr, NEG_at_20_lxr, NEG_at_50_lxr, NEG_at_100_lxr, users_DEL_lxr, users_INS_lxr, rank_at_1_lxr, rank_at_5_lxr, rank_at_10_A_lxr, rank_at_20_A_lxr, rank_at_50_A_lxr, rank_at_100_A_lxr, rank_at_k_B_lxr, NDCG_at_1_lxr, NDCG_at_5_lxr, NDCG_at_10_lxr, NDCG_at_20_lxr, NDCG_at_50_lxr, NDCG_at_100_lxr]\n",
    "\n",
    "d = {'names':names, 'values': values}\n",
    "\n",
    "temp_df = pd.DataFrame(d)\n",
    "\n",
    "\n",
    "normalized_values = []\n",
    "for i in range(len(values)):\n",
    "    lst = []\n",
    "    for j in range(len(values[i])):\n",
    "        lst.append(values[i][j]/a)\n",
    "    normalized_values.append(lst)\n",
    "\n",
    "temp_df['normalized_values'] = normalized_values\n",
    "\n",
    "AUC = []\n",
    "for i in range(len(values)):\n",
    "    sublist = temp_df['normalized_values'][i][1:]\n",
    "    AUC.append(np.mean(sublist))\n",
    "\n",
    "temp_df['AUC'] = AUC\n",
    "\n",
    "full_df = pd.concat([full_df, temp_df], ignore_index = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
