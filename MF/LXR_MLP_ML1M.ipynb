{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afc5481",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Tanh\n",
    "from torch.nn import Softmax\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cb33cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "    \n",
    "from scipy import sparse\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "SEED = 3\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d8670b-4527-4e7d-93db-dabda7e574dd",
   "metadata": {},
   "source": [
    "## Load Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc1f045-4b6d-404d-a465-11a7032b75f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DP_DIR = \"Data_preprocessing\"\n",
    "export_dir = Path(os.getcwd())\n",
    "files_path = Path(export_dir.parent, DP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9333a72-3039-44d6-b38a-f5ae18023af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(files_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a2c4eb-91a5-4852-b8d4-a8e23311e151",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = 'jaccard_based_sim_ML1.pkl'\n",
    "\n",
    "with open(Path(files_path,file_path), 'rb') as f:\n",
    "    jaccard_based_sim = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7222810-6617-42cb-9794-b3b194fd981e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = 'items_values_dict_ML1.pkl'\n",
    "\n",
    "with open(Path(files_path,file_path), 'rb') as f:\n",
    "    items_values_dict = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d69ab8-fcc7-4985-9ffc-c4edc9404fbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = 'user_similarities_Jaccard_ML1.pkl'\n",
    "\n",
    "with open(Path(files_path,file_path), 'rb') as f:\n",
    "    user_similarities_Jaccard = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ada5339-cb46-407d-851b-f208738eb520",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = 'cosine_items_ML1.pkl'\n",
    "\n",
    "with open(Path(files_path,file_path), 'rb') as f:\n",
    "    cosine_items_dict = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3e4efa-1c12-4045-8eb4-43fb9047dc7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cosine_items = cosine_items_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e027c20-6f3d-4bfb-96ea-64ded575f683",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'tf_idf_items_ML1.pkl'\n",
    "\n",
    "with open(Path(files_path,file_path), 'rb') as f:\n",
    "    tf_idf_items = pickle.load(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450a0368-41f8-4501-9e32-059966c3efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'pop_dict.pkl'\n",
    "\n",
    "with open(Path(files_path,file_path), 'rb') as f:\n",
    "    popularity_dict = pickle.load(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e820bac2-ab4f-4650-9a72-76d50d67aca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output from shap package after creating explainer with KernelExplainer\n",
    "#with the first element put to user_id these shapley values relate to\n",
    "file_path = 'shap_values.pkl'\n",
    "\n",
    "with open(Path(files_path,file_path), 'rb') as f:\n",
    "    shap_values = pickle.load(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2999317e-1195-4f25-a7b9-7a5348e15a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output from clustering that was done on the training set\n",
    "#to map num_items to n clusters and run shap package on clusters\n",
    "#instead of the items\n",
    "file_path = 'item_to_cluster_ML1.pkl'\n",
    "\n",
    "with open(Path(files_path,file_path), 'rb') as f:\n",
    "    item_to_cluster = pickle.load(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5ac75b-5892-4227-9a30-7048e3c43b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LIME import\n",
    "import importlib\n",
    "import ipynb\n",
    "from ipynb.fs.defs.lime import distance_to_proximity, LimeBase, get_lime_args, gaussian_kernel, mlp_get_lime_args\n",
    "importlib.reload(ipynb.fs.defs.lime)\n",
    "from ipynb.fs.defs.lime import distance_to_proximity, LimeBase, get_lime_args, gaussian_kernel, mlp_get_lime_args\n",
    "'''\n",
    "distance_to_proximity(distances_list) - takes distances from origin user and returns proximity\n",
    "LimeBase() - class that gets kernel function\n",
    "get_lime_args(user_vetor, item_id, model, items_array, min_pert = 10, max_pert = 20, num_of_perturbations = 5, seed = 0) - \n",
    "    returns neighborhood_data, neighborhood_labels, distances, item_id \n",
    "'''\n",
    "\n",
    "lime = LimeBase(distance_to_proximity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fa9561-4551-4058-8fb4-45a36b5a1f97",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5e7530-31df-4896-a82f-ca5cbf513e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_G(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(MLP_G, self).__init__()\n",
    "        self.linear_x = nn.Linear(input_size, hidden_size, bias = False)\n",
    "        self.linear_y = nn.Linear(input_size, hidden_size, bias = False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, user, item):\n",
    "        user_representation = self.linear_x(user.float())\n",
    "        item_representation = self.linear_y(item.float())\n",
    "        dot_prod = torch.matmul(user_representation, item_representation.T)\n",
    "        dot_sigmoid = self.sigmoid(dot_prod)\n",
    "        \n",
    "        return dot_sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041f1d25-56b4-413f-9676-f249d709d549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recommender model\n",
    "class Recommender_G(nn.Module):\n",
    "    def __init__(self, num_items, hidden_size):\n",
    "        super(Recommender_G, self).__init__()\n",
    "        self.mlp = MLP_G(num_items, hidden_size).to(device)\n",
    "\n",
    "    def forward(self, user_vector, item_vector):\n",
    "        user_vector = user_vector.to(device)\n",
    "        item_vector = item_vector.to(device)\n",
    "        output = self.mlp(user_vector, item_vector)\n",
    "        return output.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffa3fc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Explainer model\n",
    "class Explainer_G(nn.Module):\n",
    "    def __init__(self, recommender_model_g, input_size, hidden_size):\n",
    "        super(Explainer_G, self).__init__()\n",
    "        \n",
    "        backbone_children = list(recommender_model_g.children())[0]\n",
    "\n",
    "        self.slice1 = nn.Sequential(*list(backbone_children.children())[:1])\n",
    "        self.slice2 = nn.Sequential(*list(backbone_children.children())[1:2])\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features = hidden_size*2, out_features=hidden_size*3),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features = hidden_size*3, out_features=input_size),\n",
    "            nn.Sigmoid()\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        slice1_output = self.slice1(user.float())\n",
    "        slice2_output = self.slice2(item.float())\n",
    "        combined_output = torch.cat((slice1_output, slice2_output), dim=-1)\n",
    "        mask = self.bottleneck(combined_output).to(device)\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417e24e8",
   "metadata": {},
   "source": [
    "### LXR Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee527d7-301d-4f84-a2a7-3729f4af99bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossModelCombined(torch.nn.Module):\n",
    "    def __init__(self, alpha_parameter, recommender_model, explainer_model, hidden_size):\n",
    "        super().__init__()\n",
    "            \n",
    "        self.recommender_model =  recommender_model\n",
    "        self.explainer_model= explainer_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.alpha_parameter = alpha_parameter\n",
    "        \n",
    "    def forward(self,user_hist, y_positive):\n",
    "      \n",
    "        user_hist = user_hist.to(device)\n",
    "        y_positive = torch.tensor(y_positive).float().to(device)\n",
    "        \n",
    "        mask = self.explainer_model(user_hist,y_positive).to(device)\n",
    "        x_masked = user_hist * mask\n",
    "        y_positive_masked = self.recommender_model(x_masked,y_positive).unsqueeze(0).to(device)      \n",
    "        pred_loss = -torch.log(y_positive_masked).to(device)\n",
    "\n",
    "        mask_loss = torch.mean(torch.abs(mask)).to(device)\n",
    "        comb_loss = pred_loss + self.alpha_parameter * mask_loss \n",
    "        \n",
    "        return x_masked, comb_loss  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc0280c-1cd6-4c48-9557-b27d9daa15b7",
   "metadata": {},
   "source": [
    "## Load models and train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96e4c1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "hidden_dim = 20\n",
    "num_users = 6040\n",
    "num_items = 3706\n",
    "print(\"num_users is \", num_users)\n",
    "print(\"num_items is \", num_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cab73e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rec_model = Recommender_G(num_items, hidden_dim)\n",
    "rec_model.load_state_dict(torch.load(Path(files_path,'recommender_model.pt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd3b70a-bb23-4a27-a5a6-6f45e604e850",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in rec_model.parameters():\n",
    "    param.requires_grad= False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe6ce00-aba2-456e-9d2e-7a669d82a9b6",
   "metadata": {},
   "source": [
    "## Read data from the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a95439-e2de-4bfb-aec4-46d7274b7793",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_mixed = pd.read_csv(Path(files_path,'train_data_mixed.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f451f90-03f1-4207-ab58-ff1ee7ebd9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(Path(files_path,'test_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad84345-6533-4680-9dd8-1b14137fd15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'items_values_dict_ML1.pkl'\n",
    "\n",
    "with open(Path(files_path,file_path), 'rb') as f:\n",
    "    items_values_dict = pickle.load(f) # deserialize using load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2920eb41-d5cf-40e2-b46d-393eaab30d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'prob_dict.pkl'\n",
    "with open(Path(files_path,file_path), 'rb') as f:\n",
    "    prob_dict = pickle.load(f) # deserialize using load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5492be4d-77a2-4027-bf9c-02a59b4e0b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_values= pd.read_csv(Path(files_path, 'items_values.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfc5b9b-dd0b-4eca-9e86-3ad934735248",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_array = items_values.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a10384-2767-45f2-9f38-149755c9b20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array = train_data_mixed.to_numpy()\n",
    "test_array = test_data.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4601b8",
   "metadata": {},
   "source": [
    "#### Top k dictionaries for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a154033f-5fed-41a0-88f7-26de7d2e664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'topk_train.pkl'\n",
    "\n",
    "with open(Path(files_path,file_path), 'rb') as f:\n",
    "    topk_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47f1600-5173-425b-b909-5ce178903677",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'topk_test.pkl'\n",
    "\n",
    "with open(Path(files_path,file_path), 'rb') as f:\n",
    "    topk_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc070a4-4f87-4149-8c8d-9af2bb82307f",
   "metadata": {},
   "source": [
    "## Metrics calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1b2bed-e049-43af-972f-26c13e700e0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_top_k(user_vector, original_user_vector, num_items, model, top_k):\n",
    "    item_prob_dict = {}\n",
    "    user_tensor = torch.Tensor(user_vector).to(device)\n",
    "    item_tensor = torch.FloatTensor(items_array).to(device)\n",
    "    output_model = [float(i) for i in model(user_tensor, item_tensor).cpu().detach().numpy()]\n",
    "    \n",
    "    original_user_vector = np.array(original_user_vector.cpu())\n",
    "    neg = np.ones_like(original_user_vector)- original_user_vector\n",
    "    output = neg*output_model\n",
    "    for i in range(len(output)):\n",
    "        item_prob_dict[i]=output[i]\n",
    "\n",
    "    sorted_items_by_prob  = sorted(item_prob_dict.items(), key=lambda item: item[1],reverse=True)\n",
    "\n",
    "    return dict(sorted_items_by_prob[0:top_k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf100df-077d-4140-bc76-695ea4ce8fd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_index_in_the_list(user_vector, original_user_vector, item_id, num_items, model):\n",
    "    top_k_list = list(get_top_k(user_vector, original_user_vector, num_items, model, num_items).keys())\n",
    "    return top_k_list.index(item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5addf221-c6e9-49d1-9c2f-c0776c2aff0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_ndcg(ranked_list, target_item):\n",
    "    if target_item not in ranked_list:\n",
    "        return 0.0\n",
    "\n",
    "    target_idx = torch.tensor(ranked_list.index(target_item), device=device)\n",
    "    ndcg = torch.reciprocal(torch.log2(target_idx + 2))\n",
    "\n",
    "    return ndcg.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c0b654-a758-4824-8fb0-357d4c40c34f",
   "metadata": {},
   "source": [
    "## Mask calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77da4b19",
   "metadata": {},
   "source": [
    "#### LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d42338-5589-4575-9d2e-9f19d91b004f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_LIME_mask(x, item_id, items_array, min_pert, max_pert, num_of_perturbations, kernel_func, feature_selection, model, num_samples=10, method = 'POS'):\n",
    "    user_hist = x \n",
    "    # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    lime.kernel_fn = kernel_func\n",
    "    neighborhood_data, neighborhood_labels, distances, item_id = mlp_get_lime_args(user_hist, item_id, model, items_array, min_pert = min_pert, max_pert = max_pert, num_of_perturbations = num_of_perturbations, seed = item_id)\n",
    "  \n",
    "    most_pop_items  = lime.explain_instance_with_data(neighborhood_data, neighborhood_labels, distances, item_id, num_samples, feature_selection, pos_neg='POS')\n",
    "    most_pop_items_dict =  {key: value for key, value in most_pop_items}\n",
    "    \n",
    "    return most_pop_items_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8e3865",
   "metadata": {},
   "source": [
    "#### LXR based similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f8b5d6-617e-4ff1-bdac-7ad826053bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_LXR_mask(x, y_true, item_id, model_combined):\n",
    "    \n",
    "    user_hist = torch.tensor(x) \n",
    "    user_hist[item_id] = 0 \n",
    "\n",
    "    x_masked_g, loss_comb_g = model_combined(user_hist, y_true)\n",
    "    \n",
    "    x_masked_g = x_masked_g.to(device)\n",
    "    #item_sim_dict = {i: v for i, v in enumerate(x_masked_l)}\n",
    "    item_sim_dict = {i: x_masked_g[i].item() for i in range(x_masked_g.numel())}\n",
    "    \n",
    "    return item_sim_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1747f108",
   "metadata": {},
   "source": [
    "#### Genre based similarities using Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617c4c4a-de4c-42e5-8572-3acf8ad93355",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_jaccard_g_mask(x, item_id, num_items,  jaccard_based_sim,num_samples=10, method = 'POS'):\n",
    "    user_hist = torch.tensor(x) # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    item_jaccard_dict = {}\n",
    "    for i in range(num_items): \n",
    "        if(user_hist[i] == 1): # iterate all positive items of the user\n",
    "            if((i,item_id) in jaccard_based_sim.keys()):\n",
    "                item_jaccard_dict[i]=jaccard_based_sim[(i,item_id)] # add Jaccard similarity between items\n",
    "            else:\n",
    "                item_jaccard_dict[i] = 0\n",
    "    \n",
    "    return item_jaccard_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b141c736",
   "metadata": {},
   "source": [
    "#### User based similarities using Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a902eb-5acc-40b9-a170-c7025aee3b7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_jaccard_u_mask(x, item_id, num_items, user_based_Jaccard_sim,num_samples=10, method = 'POS'):\n",
    "    user_hist = torch.tensor(x) # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    item_jaccard_dict = {}\n",
    "    for i in range(num_items): \n",
    "        if(user_hist[i] == 1): # iterate all positive items of the user\n",
    "            if((i,item_id) in user_based_Jaccard_sim.keys()):\n",
    "                item_jaccard_dict[i]=user_based_Jaccard_sim[(i,item_id)] # add Jaccard similarity between items\n",
    "            else:\n",
    "                item_jaccard_dict[i] = 0\n",
    "            \n",
    "    return item_jaccard_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d094b333",
   "metadata": {},
   "source": [
    "#### Cosine based similarities between users and items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad6767b-0c7b-49de-8a9e-625ddb771891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_cosine_mask(x, item_id, num_items, item_cosine, num_samples=10, method = 'POS'):\n",
    "\n",
    "    user_hist = torch.tensor(x) # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    item_cosine_dict = {}\n",
    "        \n",
    "    for i in range(num_items): \n",
    "        if(user_hist[i] == 1): # iterate all positive items of the user\n",
    "            if((i,item_id) in item_cosine.keys()):\n",
    "                item_cosine_dict[i]=item_cosine[(i,item_id)] # add cosine similarity between items\n",
    "            else:\n",
    "                item_cosine_dict[i] = 0\n",
    "    \n",
    "    return item_cosine_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067ae902",
   "metadata": {},
   "source": [
    "#### tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b55846-3cdf-440d-8915-24097bd0e02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tf_idf_mask(x, item_id, num_items, tf_idf_sim):\n",
    "    \n",
    "    x = x.cpu().detach().numpy()\n",
    "    x[item_id] = 0\n",
    "    \n",
    "    positive_items = np.where(x == 1)[0]\n",
    "    tf_idf_dict = {i: tf_idf_sim.get((i, item_id), 0) for i in positive_items}\n",
    "    \n",
    "    return tf_idf_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad8ef4e",
   "metadata": {},
   "source": [
    "#### Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960f4701-a38b-485d-bc34-942d4fe5e6f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_pop_mask(x, item_id, num_items, num_samples=10, method = 'POS'):\n",
    "    user_hist = torch.tensor(x) # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    item_pop_dict = {}\n",
    "    for i in range(num_items): \n",
    "        if(user_hist[i] == 1): # iterate over all positive items of the user\n",
    "            item_pop_dict[i]=popularity_dict[i] # add the pop of the item to the dictionary\n",
    "            \n",
    "            \n",
    "    return item_pop_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc73fe52",
   "metadata": {},
   "source": [
    "#### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c16852-3069-4037-9afc-fec28cefab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_shapley_mask(user_vector, user_id, model, shap_values, item_to_cluster):\n",
    "        \n",
    "        item_shap = {}\n",
    "        shapley_values = shap_values[shap_values[:, 0].astype(int) == user_id][:,2:]\n",
    "        user_vector = user_vector.cpu().detach().numpy().astype(int)\n",
    "        \n",
    "        for i in np.where(user_vector.astype(int) == 1)[0]:\n",
    "            items_cluster = item_to_cluster[i]\n",
    "            item_shap[i] = shapley_values.T[int(items_cluster)][0]\n",
    "  \n",
    "        return item_shap     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd82b51-29e3-40d6-a970-018a687013ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_user_metrics(user_vector, item_id, k, num_of_bins, num_items, recommender_model, model_combined, y_pred=None,\n",
    "                        mask_type=None, user_id=None):\n",
    "    user_tensor = torch.FloatTensor(user_vector).to(device)\n",
    "\n",
    "    item_vector = items_values_dict[item_id]\n",
    "    item_tensor = torch.FloatTensor(item_vector).to(device)\n",
    "\n",
    "    POS_masked = user_tensor\n",
    "    NEG_masked = user_tensor\n",
    "    POS_masked[item_id] = 0\n",
    "    NEG_masked[item_id] = 0\n",
    "\n",
    "    user_hist_size = np.sum(user_vector)\n",
    "\n",
    "    bins = [0] + [len(x) for x in np.array_split(np.arange(np.sum(user_vector)), num_of_bins, axis=0)]\n",
    "\n",
    "    POS_at_1 = [0] * (len(bins))\n",
    "    POS_at_5 = [0] * (len(bins))\n",
    "    POS_at_10 = [0] * (len(bins))\n",
    "    POS_at_20 = [0] * (len(bins))\n",
    "    POS_at_50 = [0] * (len(bins))\n",
    "    POS_at_100 = [0] * (len(bins))\n",
    "\n",
    "    NEG_at_1 = [0] * (len(bins))\n",
    "    NEG_at_5 = [0] * (len(bins))\n",
    "    NEG_at_10 = [0] * (len(bins))\n",
    "    NEG_at_20 = [0] * (len(bins))\n",
    "    NEG_at_50 = [0] * (len(bins))\n",
    "    NEG_at_100 = [0] * (len(bins))\n",
    "\n",
    "    DEL = [0] * (len(bins))\n",
    "    INS = [0] * (len(bins))\n",
    "\n",
    "    rankA_at_1 = [0] * (len(bins))\n",
    "    rankA_at_5 = [0] * (len(bins))\n",
    "    rankA_at_10 = [0] * (len(bins))\n",
    "    rankA_at_20 = [0] * (len(bins))\n",
    "    rankA_at_50 = [0] * (len(bins))\n",
    "    rankA_at_100 = [0] * (len(bins))\n",
    "\n",
    "    rankB = [0] * (len(bins))\n",
    "\n",
    "    NDCG_at_1 = [0] * (len(bins))\n",
    "    NDCG_at_5 = [0] * (len(bins))\n",
    "    NDCG_at_10 = [0] * (len(bins))\n",
    "    NDCG_at_20 = [0] * (len(bins))\n",
    "    NDCG_at_50 = [0] * (len(bins))\n",
    "    NDCG_at_100 = [0] * (len(bins))\n",
    "\n",
    "    total_items = 0\n",
    "\n",
    "    for i in range(len(bins)):\n",
    "        total_items += bins[i]\n",
    "\n",
    "        if i == 0:\n",
    "            if mask_type == 'jaccard_g':\n",
    "                sim_items = find_jaccard_g_mask(POS_masked, item_id, num_items, jaccard_based_sim, num_samples=bins[i])\n",
    "            elif mask_type == 'jaccard_u':\n",
    "                sim_items = find_jaccard_u_mask(POS_masked, item_id, num_items, user_similarities_Jaccard,\n",
    "                                                num_samples=bins[i])\n",
    "            elif mask_type == 'cosine':\n",
    "                sim_items = find_cosine_mask(POS_masked, item_id, num_items, cosine_items, num_samples=bins[i])\n",
    "            elif mask_type == 'tf_idf':\n",
    "                sim_items = find_tf_idf_mask(POS_masked, item_id, num_items, tf_idf_items)\n",
    "            elif mask_type == 'pop':\n",
    "                sim_items = find_pop_mask(POS_masked, item_id, num_items, num_samples=bins[i])\n",
    "            elif mask_type == 'shap':\n",
    "                sim_items = find_shapley_mask(POS_masked, user_id, recommender_model, shap_values, item_to_cluster)\n",
    "            elif mask_type == 'lime':\n",
    "                sim_items = find_LIME_mask(user_vector, item_id, items_array, 50, 100, 150, distance_to_proximity,\n",
    "                                           'highest_weights', model=recommender_model, num_samples=user_hist_size)\n",
    "            elif mask_type == 'lxr':\n",
    "                sim_items = find_LXR_mask(POS_masked, item_vector, item_id, model_combined)\n",
    "            else:\n",
    "                raise Exception(\"Wrong mask type\")\n",
    "\n",
    "        POS_sim_items = list(sorted(sim_items.items(), key=lambda item: item[1], reverse=True))[0:user_hist_size]\n",
    "        NEG_sim_items = list(sorted(dict(POS_sim_items).items(), key=lambda item: item[1], reverse=False))\n",
    "\n",
    "        POS_masked = torch.zeros_like(user_tensor, dtype=torch.float32, device=device)\n",
    "        for j in POS_sim_items[:total_items]:\n",
    "            POS_masked[j[0]] = 1\n",
    "        POS_masked = user_tensor - POS_masked\n",
    "\n",
    "        NEG_masked = torch.zeros_like(user_tensor, dtype=torch.float32, device=device)\n",
    "        for j in NEG_sim_items[:total_items]:\n",
    "            NEG_masked[j[0]] = 1\n",
    "        NEG_masked = user_tensor - NEG_masked  # remove the masked items from the user history \n",
    "\n",
    "        POS_ranked_list = get_top_k(POS_masked, user_tensor, num_items, recommender_model, num_items)\n",
    "        POS_index = list(POS_ranked_list.keys()).index(item_id) + 1\n",
    "        NEG_index = get_index_in_the_list(NEG_masked, user_tensor, item_id, num_items, recommender_model) + 1\n",
    "\n",
    "        # for pos:\n",
    "        POS_at_1[i] = 1 if POS_index <= 1 else 0\n",
    "        POS_at_5[i] = 1 if POS_index <= 5 else 0\n",
    "        POS_at_10[i] = 1 if POS_index <= 10 else 0\n",
    "        POS_at_20[i] = 1 if POS_index <= 20 else 0\n",
    "        POS_at_50[i] = 1 if POS_index <= 50 else 0\n",
    "        POS_at_100[i] = 1 if POS_index <= 100 else 0\n",
    "\n",
    "        # for neg:\n",
    "        NEG_at_1[i] = 1 if NEG_index <= 1 else 0\n",
    "        NEG_at_5[i] = 1 if NEG_index <= 5 else 0\n",
    "        NEG_at_10[i] = 1 if NEG_index <= 10 else 0\n",
    "        NEG_at_20[i] = 1 if NEG_index <= 20 else 0\n",
    "        NEG_at_50[i] = 1 if NEG_index <= 50 else 0\n",
    "        NEG_at_100[i] = 1 if NEG_index <= 100 else 0\n",
    "        # for del:\n",
    "        DEL[i] = float(recommender_model(POS_masked, item_tensor).detach().cpu().numpy())\n",
    "\n",
    "        # for ins:\n",
    "        INS[i] = float(recommender_model(user_tensor - POS_masked, item_tensor).detach().cpu().numpy())\n",
    "\n",
    "        # for rankA:\n",
    "        rankA_at_1[i] = max(0, (1 + 1 - POS_index) / 10)\n",
    "        rankA_at_5[i] = max(0, (5 + 1 - POS_index) / 20)\n",
    "        rankA_at_10[i] = max(0, (10 + 1 - POS_index) / 10)\n",
    "        rankA_at_20[i] = max(0, (20 + 1 - POS_index) / 20)\n",
    "        rankA_at_50[i] = max(0, (50 + 1 - POS_index) / 50)\n",
    "        rankA_at_100[i] = max(0, (100 + 1 - POS_index) / 100)\n",
    "\n",
    "        # for rankB:\n",
    "        rankB[i] = 1 / POS_index\n",
    "\n",
    "        # for NDCG:\n",
    "        NDCG_at_1[i] = get_ndcg(list(POS_ranked_list.keys())[:1], item_id)\n",
    "        NDCG_at_5[i] = get_ndcg(list(POS_ranked_list.keys())[:5], item_id)\n",
    "        NDCG_at_10[i] = get_ndcg(list(POS_ranked_list.keys())[:10], item_id)\n",
    "        NDCG_at_20[i] = get_ndcg(list(POS_ranked_list.keys())[:20], item_id)\n",
    "        NDCG_at_50[i] = get_ndcg(list(POS_ranked_list.keys())[:50], item_id)\n",
    "        NDCG_at_100[i] = get_ndcg(list(POS_ranked_list.keys())[:100], item_id)\n",
    "\n",
    "    res = [POS_at_1, POS_at_5, POS_at_10, POS_at_20, POS_at_50, POS_at_100, NEG_at_1, NEG_at_5, NEG_at_10, NEG_at_20,\n",
    "           NEG_at_50, NEG_at_100, DEL, INS, rankA_at_1, rankA_at_5, rankA_at_10, rankA_at_20, rankA_at_50, rankA_at_100,\n",
    "           rankB, NDCG_at_1, NDCG_at_5, NDCG_at_10, NDCG_at_20, NDCG_at_50, NDCG_at_100]\n",
    "    for i in range(len(res)):\n",
    "        res[i] = np.array(res[i])\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187a3c45-f0fd-40ec-bdf9-aefdc14ec82c",
   "metadata": {},
   "source": [
    "## LXR training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075a4a8b-04b6-4391-a70a-bce22d900344",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get users vectors to create topk\n",
    "unique_indices = np.unique(train_array[:,-3], return_index=True, axis=0)[1]\n",
    "\n",
    "# create a new array with only the unique users\n",
    "train_unique_arr = train_array[unique_indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe054c3b-9c29-4f77-82f5-06ffba6dc2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_model_g = Explainer_G(rec_model, num_items, hidden_dim).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb999b3f-c39c-4cb1-b2e5-32387d242246",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LXR training\n",
    "\n",
    "train_losses = []\n",
    "epochs = 10\n",
    "hidden_dim = 10\n",
    "num_of_bins = 10\n",
    "alpha_parameter = 0.5\n",
    "\n",
    "model_combined = LossModelCombined(alpha_parameter, rec_model, explainer_model_g, hidden_dim).to(device)\n",
    "optimizer_comb = torch.optim.Adam(model_combined.parameters(), lr=0.001)\n",
    "    \n",
    "#Train LXR\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    \n",
    "    for i in range(train_unique_arr.shape[0]):\n",
    "        \n",
    "        #user data\n",
    "        user_id = train_unique_arr[i][-3]\n",
    "        user_vector = train_unique_arr[i][:-3]\n",
    "        #get top1 of this user for LRX training\n",
    "        top1_item = np.argmax(topk_train[user_id])\n",
    "       \n",
    "        positive_sample = torch.FloatTensor(items_values_dict[top1_item]).to(device)\n",
    "        \n",
    "        user_vector[top1_item] = 0 \n",
    "        user_tensor = torch.FloatTensor(user_vector).to(device)\n",
    "\n",
    "        #train model        \n",
    "        optimizer_comb.zero_grad()\n",
    "        x_masked_g, loss_comb_g = model_combined(user_tensor, positive_sample)#, negative_sample)\n",
    "        \n",
    "        train_loss+=loss_comb_g.item()\n",
    "            \n",
    "        loss_comb_g.backward()\n",
    "        optimizer_comb.step()\n",
    "   \n",
    "    train_losses.append(train_loss/train_unique_arr.shape[0])\n",
    "\n",
    "    print(f\"Epoch {epoch}, Train Loss {train_loss/train_unique_arr.shape[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6490426-daf5-401f-8bde-f767660afb95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model_combined.state_dict(), 'mlp_model_combined_10_epochs_05_new_explainer_tahn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab9629-7c3a-4f52-9779-4940bd3d7dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(explainer_model_g.state_dict(), 'mlp_explainer_model_10_epochs_05_new_explainer_tanh.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b200a1-f13d-4e15-bd58-2d202e7e5f32",
   "metadata": {},
   "source": [
    "## LXR on testing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934651c9-7d21-4009-87db-488a922b622d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Evaluate the model on the test set\n",
    "num_of_bins = 11\n",
    "k = 100\n",
    "\n",
    "POS_at_1_shap = np.zeros(num_of_bins)\n",
    "pos_at_5_shap = np.zeros(num_of_bins)\n",
    "POS_at_10_shap = np.zeros(num_of_bins)\n",
    "POS_at_20_shap = np.zeros(num_of_bins)\n",
    "POS_at_50_shap = np.zeros(num_of_bins)\n",
    "POS_at_100_shap = np.zeros(num_of_bins)\n",
    "NEG_at_1_shap = np.zeros(num_of_bins)\n",
    "NEG_at_5_shap = np.zeros(num_of_bins)\n",
    "NEG_at_10_shap = np.zeros(num_of_bins)\n",
    "NEG_at_20_shap = np.zeros(num_of_bins)\n",
    "NEG_at_50_shap = np.zeros(num_of_bins)\n",
    "NEG_at_100_shap = np.zeros(num_of_bins)\n",
    "users_DEL_shap = np.zeros(num_of_bins)\n",
    "users_INS_shap = np.zeros(num_of_bins)\n",
    "rank_at_1_shap = np.zeros(num_of_bins)\n",
    "rank_at_5_shap = np.zeros(num_of_bins)\n",
    "rank_at_10_A_shap = np.zeros(num_of_bins)\n",
    "rank_at_20_A_shap = np.zeros(num_of_bins)\n",
    "rank_at_50_A_shap = np.zeros(num_of_bins)\n",
    "rank_at_100_A_shap = np.zeros(num_of_bins)\n",
    "rank_at_k_B_shap = np.zeros(num_of_bins)\n",
    "NDCG_at_1_shap = np.zeros(num_of_bins)\n",
    "NDCG_at_5_shap = np.zeros(num_of_bins)\n",
    "NDCG_at_10_shap = np.zeros(num_of_bins)\n",
    "NDCG_at_20_shap = np.zeros(num_of_bins)\n",
    "NDCG_at_50_shap = np.zeros(num_of_bins)\n",
    "NDCG_at_100_shap = np.zeros(num_of_bins)\n",
    "\n",
    "POS_at_1_j_g = np.zeros(num_of_bins)\n",
    "pos_at_5_j_g = np.zeros(num_of_bins)\n",
    "POS_at_10_j_g = np.zeros(num_of_bins)\n",
    "POS_at_20_j_g = np.zeros(num_of_bins)\n",
    "POS_at_50_j_g = np.zeros(num_of_bins)\n",
    "POS_at_100_j_g = np.zeros(num_of_bins)\n",
    "NEG_at_1_j_g = np.zeros(num_of_bins)\n",
    "NEG_at_5_j_g = np.zeros(num_of_bins)\n",
    "NEG_at_10_j_g = np.zeros(num_of_bins)\n",
    "NEG_at_20_j_g = np.zeros(num_of_bins)\n",
    "NEG_at_50_j_g = np.zeros(num_of_bins)\n",
    "NEG_at_100_j_g = np.zeros(num_of_bins)\n",
    "users_DEL_j_g = np.zeros(num_of_bins)\n",
    "users_INS_j_g = np.zeros(num_of_bins)\n",
    "rank_at_1_j_g = np.zeros(num_of_bins)\n",
    "rank_at_5_j_g = np.zeros(num_of_bins)\n",
    "rank_at_10_A_j_g = np.zeros(num_of_bins)\n",
    "rank_at_20_A_j_g = np.zeros(num_of_bins)\n",
    "rank_at_50_A_j_g = np.zeros(num_of_bins)\n",
    "rank_at_100_A_j_g = np.zeros(num_of_bins)\n",
    "rank_at_k_B_j_g = np.zeros(num_of_bins)\n",
    "NDCG_at_1_j_g = np.zeros(num_of_bins)\n",
    "NDCG_at_5_j_g = np.zeros(num_of_bins)\n",
    "NDCG_at_10_j_g = np.zeros(num_of_bins)\n",
    "NDCG_at_20_j_g = np.zeros(num_of_bins)\n",
    "NDCG_at_50_j_g = np.zeros(num_of_bins)\n",
    "NDCG_at_100_j_g = np.zeros(num_of_bins)\n",
    "\n",
    "\n",
    "POS_at_1_j_u = np.zeros(num_of_bins)\n",
    "pos_at_5_j_u = np.zeros(num_of_bins)\n",
    "POS_at_10_j_u = np.zeros(num_of_bins)\n",
    "POS_at_20_j_u = np.zeros(num_of_bins)\n",
    "POS_at_50_j_u = np.zeros(num_of_bins)\n",
    "POS_at_100_j_u = np.zeros(num_of_bins)\n",
    "NEG_at_1_j_u = np.zeros(num_of_bins)\n",
    "NEG_at_5_j_u = np.zeros(num_of_bins)\n",
    "NEG_at_10_j_u = np.zeros(num_of_bins)\n",
    "NEG_at_20_j_u = np.zeros(num_of_bins)\n",
    "NEG_at_50_j_u = np.zeros(num_of_bins)\n",
    "NEG_at_100_j_u = np.zeros(num_of_bins)\n",
    "users_DEL_j_u = np.zeros(num_of_bins)\n",
    "users_INS_j_u = np.zeros(num_of_bins)\n",
    "rank_at_1_j_u = np.zeros(num_of_bins)\n",
    "rank_at_5_j_u = np.zeros(num_of_bins)\n",
    "rank_at_10_A_j_u = np.zeros(num_of_bins)\n",
    "rank_at_20_A_j_u = np.zeros(num_of_bins)\n",
    "rank_at_50_A_j_u = np.zeros(num_of_bins)\n",
    "rank_at_100_A_j_u = np.zeros(num_of_bins)\n",
    "rank_at_k_B_j_u = np.zeros(num_of_bins)\n",
    "NDCG_at_1_j_u = np.zeros(num_of_bins)\n",
    "NDCG_at_5_j_u = np.zeros(num_of_bins)\n",
    "NDCG_at_10_j_u = np.zeros(num_of_bins)\n",
    "NDCG_at_20_j_u = np.zeros(num_of_bins)\n",
    "NDCG_at_50_j_u = np.zeros(num_of_bins)\n",
    "NDCG_at_100_j_u = np.zeros(num_of_bins)\n",
    "\n",
    "\n",
    "POS_at_1_c_s = np.zeros(num_of_bins)\n",
    "pos_at_5_c_s = np.zeros(num_of_bins)\n",
    "POS_at_10_c_s = np.zeros(num_of_bins)\n",
    "POS_at_20_c_s = np.zeros(num_of_bins)\n",
    "POS_at_50_c_s = np.zeros(num_of_bins)\n",
    "POS_at_100_c_s = np.zeros(num_of_bins)\n",
    "NEG_at_1_c_s = np.zeros(num_of_bins)\n",
    "NEG_at_5_c_s = np.zeros(num_of_bins)\n",
    "NEG_at_10_c_s = np.zeros(num_of_bins)\n",
    "NEG_at_20_c_s= np.zeros(num_of_bins)\n",
    "NEG_at_50_c_s = np.zeros(num_of_bins)\n",
    "NEG_at_100_c_s = np.zeros(num_of_bins)\n",
    "users_DEL_c_s = np.zeros(num_of_bins)\n",
    "users_INS_c_s = np.zeros(num_of_bins)\n",
    "rank_at_1_c_s = np.zeros(num_of_bins)\n",
    "rank_at_5_c_s = np.zeros(num_of_bins)\n",
    "rank_at_10_A_c_s = np.zeros(num_of_bins)\n",
    "rank_at_20_A_c_s = np.zeros(num_of_bins)\n",
    "rank_at_50_A_c_s = np.zeros(num_of_bins)\n",
    "rank_at_100_A_c_s = np.zeros(num_of_bins)\n",
    "rank_at_k_B_c_s = np.zeros(num_of_bins)\n",
    "NDCG_at_1_c_s = np.zeros(num_of_bins)\n",
    "NDCG_at_5_c_s = np.zeros(num_of_bins)\n",
    "NDCG_at_10_c_s = np.zeros(num_of_bins)\n",
    "NDCG_at_20_c_s = np.zeros(num_of_bins)\n",
    "NDCG_at_50_c_s = np.zeros(num_of_bins)\n",
    "NDCG_at_100_c_s = np.zeros(num_of_bins)\n",
    "\n",
    "\n",
    "POS_at_1_pop = np.zeros(num_of_bins)\n",
    "pos_at_5_pop = np.zeros(num_of_bins)\n",
    "POS_at_10_pop = np.zeros(num_of_bins)\n",
    "POS_at_20_pop = np.zeros(num_of_bins)\n",
    "POS_at_50_pop = np.zeros(num_of_bins)\n",
    "POS_at_100_pop = np.zeros(num_of_bins)\n",
    "NEG_at_1_pop = np.zeros(num_of_bins)\n",
    "NEG_at_5_pop = np.zeros(num_of_bins)\n",
    "NEG_at_10_pop = np.zeros(num_of_bins)\n",
    "NEG_at_20_pop = np.zeros(num_of_bins)\n",
    "NEG_at_50_pop = np.zeros(num_of_bins)\n",
    "NEG_at_100_pop = np.zeros(num_of_bins)\n",
    "users_DEL_pop = np.zeros(num_of_bins)\n",
    "users_INS_pop = np.zeros(num_of_bins)\n",
    "rank_at_1_pop = np.zeros(num_of_bins)\n",
    "rank_at_5_pop = np.zeros(num_of_bins)\n",
    "rank_at_10_A_pop = np.zeros(num_of_bins)\n",
    "rank_at_20_A_pop = np.zeros(num_of_bins)\n",
    "rank_at_50_A_pop = np.zeros(num_of_bins)\n",
    "rank_at_100_A_pop = np.zeros(num_of_bins)\n",
    "rank_at_k_B_pop = np.zeros(num_of_bins)\n",
    "NDCG_at_1_pop = np.zeros(num_of_bins)\n",
    "NDCG_at_5_pop = np.zeros(num_of_bins)\n",
    "NDCG_at_10_pop = np.zeros(num_of_bins)\n",
    "NDCG_at_20_pop = np.zeros(num_of_bins)\n",
    "NDCG_at_50_pop = np.zeros(num_of_bins)\n",
    "NDCG_at_100_pop = np.zeros(num_of_bins)\n",
    "\n",
    "POS_at_1_lime = np.zeros(num_of_bins)\n",
    "pos_at_5_lime = np.zeros(num_of_bins)\n",
    "POS_at_10_lime = np.zeros(num_of_bins)\n",
    "POS_at_20_lime = np.zeros(num_of_bins)\n",
    "POS_at_50_lime = np.zeros(num_of_bins)\n",
    "POS_at_100_lime = np.zeros(num_of_bins)\n",
    "NEG_at_1_lime = np.zeros(num_of_bins)\n",
    "NEG_at_5_lime = np.zeros(num_of_bins)\n",
    "NEG_at_10_lime = np.zeros(num_of_bins)\n",
    "NEG_at_20_lime = np.zeros(num_of_bins)\n",
    "NEG_at_50_lime = np.zeros(num_of_bins)\n",
    "NEG_at_100_lime = np.zeros(num_of_bins)\n",
    "users_DEL_lime = np.zeros(num_of_bins)\n",
    "users_INS_lime = np.zeros(num_of_bins)\n",
    "rank_at_1_lime = np.zeros(num_of_bins)\n",
    "rank_at_5_lime = np.zeros(num_of_bins)\n",
    "rank_at_10_A_lime = np.zeros(num_of_bins)\n",
    "rank_at_20_A_lime = np.zeros(num_of_bins)\n",
    "rank_at_50_A_lime = np.zeros(num_of_bins)\n",
    "rank_at_100_A_lime = np.zeros(num_of_bins)\n",
    "rank_at_k_B_lime = np.zeros(num_of_bins)\n",
    "NDCG_at_1_lime = np.zeros(num_of_bins)\n",
    "NDCG_at_5_lime = np.zeros(num_of_bins)\n",
    "NDCG_at_10_lime = np.zeros(num_of_bins)\n",
    "NDCG_at_20_lime = np.zeros(num_of_bins)\n",
    "NDCG_at_50_lime = np.zeros(num_of_bins)\n",
    "NDCG_at_100_lime = np.zeros(num_of_bins)\n",
    "\n",
    "POS_at_1_tf_idf = np.zeros(num_of_bins)\n",
    "pos_at_5_tf_idf = np.zeros(num_of_bins)\n",
    "POS_at_10_tf_idf = np.zeros(num_of_bins)\n",
    "POS_at_20_tf_idf = np.zeros(num_of_bins)\n",
    "POS_at_50_tf_idf = np.zeros(num_of_bins)\n",
    "POS_at_100_tf_idf = np.zeros(num_of_bins)\n",
    "NEG_at_1_tf_idf = np.zeros(num_of_bins)\n",
    "NEG_at_5_tf_idf = np.zeros(num_of_bins)\n",
    "NEG_at_10_tf_idf = np.zeros(num_of_bins)\n",
    "NEG_at_20_tf_idf = np.zeros(num_of_bins)\n",
    "NEG_at_50_tf_idf = np.zeros(num_of_bins)\n",
    "NEG_at_100_tf_idf = np.zeros(num_of_bins)\n",
    "users_DEL_tf_idf = np.zeros(num_of_bins)\n",
    "users_INS_tf_idf = np.zeros(num_of_bins)\n",
    "rank_at_1_tf_idf = np.zeros(num_of_bins)\n",
    "rank_at_5_tf_idf = np.zeros(num_of_bins)\n",
    "rank_at_10_A_tf_idf = np.zeros(num_of_bins)\n",
    "rank_at_20_A_tf_idf = np.zeros(num_of_bins)\n",
    "rank_at_50_A_tf_idf = np.zeros(num_of_bins)\n",
    "rank_at_100_A_tf_idf = np.zeros(num_of_bins)\n",
    "rank_at_k_B_tf_idf = np.zeros(num_of_bins)\n",
    "NDCG_at_1_tf_idf = np.zeros(num_of_bins)\n",
    "NDCG_at_5_tf_idf = np.zeros(num_of_bins)\n",
    "NDCG_at_10_tf_idf = np.zeros(num_of_bins)\n",
    "NDCG_at_20_tf_idf = np.zeros(num_of_bins)\n",
    "NDCG_at_50_tf_idf = np.zeros(num_of_bins)\n",
    "NDCG_at_100_tf_idf = np.zeros(num_of_bins)\n",
    "\n",
    "POS_at_1_lxr = np.zeros(num_of_bins)\n",
    "pos_at_5_lxr = np.zeros(num_of_bins)\n",
    "POS_at_10_lxr = np.zeros(num_of_bins)\n",
    "POS_at_20_lxr = np.zeros(num_of_bins)\n",
    "POS_at_50_lxr = np.zeros(num_of_bins)\n",
    "POS_at_100_lxr = np.zeros(num_of_bins)\n",
    "NEG_at_1_lxr = np.zeros(num_of_bins)\n",
    "NEG_at_5_lxr = np.zeros(num_of_bins)\n",
    "NEG_at_10_lxr = np.zeros(num_of_bins)\n",
    "NEG_at_20_lxr = np.zeros(num_of_bins)\n",
    "NEG_at_50_lxr = np.zeros(num_of_bins)\n",
    "NEG_at_100_lxr = np.zeros(num_of_bins)\n",
    "users_DEL_lxr = np.zeros(num_of_bins)\n",
    "users_INS_lxr = np.zeros(num_of_bins)\n",
    "rank_at_1_lxr = np.zeros(num_of_bins)\n",
    "rank_at_5_lxr = np.zeros(num_of_bins)\n",
    "rank_at_10_A_lxr = np.zeros(num_of_bins)\n",
    "rank_at_20_A_lxr = np.zeros(num_of_bins)\n",
    "rank_at_50_A_lxr = np.zeros(num_of_bins)\n",
    "rank_at_100_A_lxr = np.zeros(num_of_bins)\n",
    "rank_at_k_B_lxr = np.zeros(num_of_bins)\n",
    "NDCG_at_1_lxr = np.zeros(num_of_bins)\n",
    "NDCG_at_5_lxr = np.zeros(num_of_bins)\n",
    "NDCG_at_10_lxr = np.zeros(num_of_bins)\n",
    "NDCG_at_20_lxr = np.zeros(num_of_bins)\n",
    "NDCG_at_50_lxr = np.zeros(num_of_bins)\n",
    "NDCG_at_100_lxr = np.zeros(num_of_bins)\n",
    "\n",
    "num_of_bins = 10\n",
    "\n",
    "explainer_model_g.eval()\n",
    "model_combined.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(test_array.shape[0]):\n",
    "            \n",
    "        user_id = test_array[i][-2]\n",
    "        user_vector = test_array[i][:-2]\n",
    "\n",
    "        y_pred = topk_test[user_id]\n",
    "        item_id = np.argmax(y_pred)\n",
    "\n",
    "        item_vector = items_values_dict[item_id]\n",
    "        item_tensor = torch.FloatTensor(item_vector).to(device)\n",
    "\n",
    "        user_vector[item_id] = 0\n",
    "        user_tensor = torch.FloatTensor(user_vector).to(device)\n",
    "\n",
    "        ### Jaccard user:\n",
    "        res = single_user_metrics(user_vector, item_id, k, num_of_bins, num_items, rec_model, model_combined, mask_type= 'jaccard_u')\n",
    "        POS_at_1_j_u += res[0]\n",
    "        pos_at_5_j_u += res[1]\n",
    "        POS_at_10_j_u += res[2]\n",
    "        POS_at_20_j_u += res[3]\n",
    "        POS_at_50_j_u += res[4]\n",
    "        POS_at_100_j_u += res[5]\n",
    "        NEG_at_1_j_u += res[6]\n",
    "        NEG_at_5_j_u += res[7]\n",
    "        NEG_at_10_j_u += res[8]\n",
    "        NEG_at_20_j_u += res[9]\n",
    "        NEG_at_50_j_u += res[10]\n",
    "        NEG_at_100_j_u += res[11]\n",
    "        users_DEL_j_u += res[12]\n",
    "        users_INS_j_u += res[13]\n",
    "        rank_at_1_j_u += res[14]\n",
    "        rank_at_5_j_u += res[15]\n",
    "        rank_at_10_A_j_u += res[16]\n",
    "        rank_at_20_A_j_u += res[17]\n",
    "        rank_at_50_A_j_u += res[18]\n",
    "        rank_at_100_A_j_u += res[19]\n",
    "        rank_at_k_B_j_u += res[20]\n",
    "        NDCG_at_1_j_u += res[21]\n",
    "        NDCG_at_5_j_u += res[22]\n",
    "        NDCG_at_10_j_u += res[23]\n",
    "        NDCG_at_20_j_u += res[24]\n",
    "        NDCG_at_50_j_u += res[25]\n",
    "        NDCG_at_100_j_u += res[26]\n",
    "\n",
    "        ### cosine similarity:\n",
    "        res = single_user_metrics(user_vector, item_id, k, num_of_bins, num_items, rec_model, model_combined, mask_type= 'cosine')\n",
    "        POS_at_1_c_s += res[0]\n",
    "        pos_at_5_c_s += res[1]\n",
    "        POS_at_10_c_s += res[2]\n",
    "        POS_at_20_c_s += res[3]\n",
    "        POS_at_50_c_s += res[4]\n",
    "        POS_at_100_c_s += res[5]\n",
    "        NEG_at_1_c_s += res[6]\n",
    "        NEG_at_5_c_s += res[7]\n",
    "        NEG_at_10_c_s += res[8]\n",
    "        NEG_at_20_c_s += res[9]\n",
    "        NEG_at_50_c_s += res[10]\n",
    "        NEG_at_100_c_s += res[11]\n",
    "        users_DEL_c_s += res[12]\n",
    "        users_INS_c_s += res[13]\n",
    "        rank_at_1_c_s += res[14]\n",
    "        rank_at_5_c_s += res[15]\n",
    "        rank_at_10_A_c_s += res[16]\n",
    "        rank_at_20_A_c_s += res[17]\n",
    "        rank_at_50_A_c_s += res[18]\n",
    "        rank_at_100_A_c_s += res[19]\n",
    "        rank_at_k_B_c_s += res[20]\n",
    "        NDCG_at_1_c_s += res[21]\n",
    "        NDCG_at_5_c_s += res[22]\n",
    "        NDCG_at_10_c_s += res[23]\n",
    "        NDCG_at_20_c_s += res[24]\n",
    "        NDCG_at_50_c_s += res[25]\n",
    "        NDCG_at_100_c_s += res[26]\n",
    "        \n",
    "        ## tf-idf similarity:\n",
    "        res = single_user_metrics(user_vector, item_id, k, num_of_bins, num_items, rec_model, model_combined, mask_type= 'tf_idf')\n",
    "        POS_at_1_tf_idf += res[0]\n",
    "        pos_at_5_tf_idf += res[1]\n",
    "        POS_at_10_tf_idf += res[2]\n",
    "        POS_at_20_tf_idf += res[3]\n",
    "        POS_at_50_tf_idf += res[4]\n",
    "        POS_at_100_tf_idf += res[5]\n",
    "        NEG_at_1_tf_idf += res[6]\n",
    "        NEG_at_5_tf_idf += res[7]\n",
    "        NEG_at_10_tf_idf += res[8]\n",
    "        NEG_at_20_tf_idf += res[9]\n",
    "        NEG_at_50_tf_idf += res[10]\n",
    "        NEG_at_100_tf_idf += res[11]\n",
    "        users_DEL_tf_idf += res[12]\n",
    "        users_INS_tf_idf += res[13]\n",
    "        rank_at_1_tf_idf += res[14]\n",
    "        rank_at_5_tf_idf += res[15]\n",
    "        rank_at_10_A_tf_idf += res[16]\n",
    "        rank_at_20_A_tf_idf += res[17]\n",
    "        rank_at_50_A_tf_idf += res[18]\n",
    "        rank_at_100_A_tf_idf += res[19]\n",
    "        rank_at_k_B_tf_idf += res[20]\n",
    "        NDCG_at_1_tf_idf += res[21]\n",
    "        NDCG_at_5_tf_idf += res[22]\n",
    "        NDCG_at_10_tf_idf += res[23]\n",
    "        NDCG_at_20_tf_idf += res[24]\n",
    "        NDCG_at_50_tf_idf += res[25]\n",
    "        NDCG_at_100_tf_idf += res[26]\n",
    "\n",
    "        ### pop:\n",
    "        res = single_user_metrics(user_vector, item_id, k, num_of_bins, num_items, rec_model, model_combined, mask_type= 'pop')    \n",
    "        POS_at_1_pop += res[0]\n",
    "        pos_at_5_pop += res[1]\n",
    "        POS_at_10_pop += res[2]\n",
    "        POS_at_20_pop += res[3]\n",
    "        POS_at_50_pop += res[4]\n",
    "        POS_at_100_pop += res[5]\n",
    "        NEG_at_1_pop += res[6]\n",
    "        NEG_at_5_pop += res[7]\n",
    "        NEG_at_10_pop += res[8]\n",
    "        NEG_at_20_pop += res[9]\n",
    "        NEG_at_50_pop += res[10]\n",
    "        NEG_at_100_pop += res[11]\n",
    "        users_DEL_pop += res[12]\n",
    "        users_INS_pop += res[13]\n",
    "        rank_at_1_pop += res[14]\n",
    "        rank_at_5_pop += res[15]\n",
    "        rank_at_10_A_pop += res[16]\n",
    "        rank_at_20_A_pop += res[17]\n",
    "        rank_at_50_A_pop += res[18]\n",
    "        rank_at_100_A_pop += res[19]\n",
    "        rank_at_k_B_pop += res[20]\n",
    "        NDCG_at_1_pop += res[21]\n",
    "        NDCG_at_5_pop += res[22]\n",
    "        NDCG_at_10_pop += res[23]\n",
    "        NDCG_at_20_pop += res[24]\n",
    "        NDCG_at_50_pop += res[25]\n",
    "        NDCG_at_100_pop += res[26]\n",
    "\n",
    "        ### LXR:\n",
    "        res = single_user_metrics(user_vector, item_id, k, num_of_bins, num_items, rec_model, model_combined, mask_type= 'lxr')    \n",
    "        POS_at_1_lxr += res[0]\n",
    "        pos_at_5_lxr += res[1]\n",
    "        POS_at_10_lxr += res[2]\n",
    "        POS_at_20_lxr += res[3]\n",
    "        POS_at_50_lxr += res[4]\n",
    "        POS_at_100_lxr += res[5]\n",
    "        NEG_at_1_lxr += res[6]\n",
    "        NEG_at_5_lxr += res[7]\n",
    "        NEG_at_10_lxr += res[8]\n",
    "        NEG_at_20_lxr += res[9]\n",
    "        NEG_at_50_lxr += res[10]\n",
    "        NEG_at_100_lxr += res[11]\n",
    "        users_DEL_lxr += res[12]\n",
    "        users_INS_lxr += res[13]\n",
    "        rank_at_1_lxr += res[14]\n",
    "        rank_at_5_lxr += res[15]\n",
    "        rank_at_10_A_lxr += res[16]\n",
    "        rank_at_20_A_lxr += res[17]\n",
    "        rank_at_50_A_lxr += res[18]\n",
    "        rank_at_100_A_lxr += res[19]\n",
    "        rank_at_k_B_lxr += res[20]\n",
    "        NDCG_at_1_lxr += res[21]\n",
    "        NDCG_at_5_lxr += res[22]\n",
    "        NDCG_at_10_lxr += res[23]\n",
    "        NDCG_at_20_lxr += res[24]\n",
    "        NDCG_at_50_lxr += res[25]\n",
    "        NDCG_at_100_lxr += res[26]\n",
    "        \n",
    "        ### Lime:\n",
    "        res = single_user_metrics(user_vector, item_id, k, num_of_bins, num_items, rec_model, model_combined, mask_type= 'lime')    \n",
    "        POS_at_1_lime += res[0]\n",
    "        pos_at_5_lime += res[1]\n",
    "        POS_at_10_lime += res[2]\n",
    "        POS_at_20_lime += res[3]\n",
    "        POS_at_50_lime += res[4]\n",
    "        POS_at_100_lime += res[5]\n",
    "        NEG_at_1_lime += res[6]\n",
    "        NEG_at_5_lime += res[7]\n",
    "        NEG_at_10_lime += res[8]\n",
    "        NEG_at_20_lime += res[9]\n",
    "        NEG_at_50_lime += res[10]\n",
    "        NEG_at_100_lime += res[11]\n",
    "        users_DEL_lime += res[12]\n",
    "        users_INS_lime += res[13]\n",
    "        rank_at_1_lime += res[14]\n",
    "        rank_at_5_lime += res[15]\n",
    "        rank_at_10_A_lime += res[16]\n",
    "        rank_at_20_A_lime += res[17]\n",
    "        rank_at_50_A_lime += res[18]\n",
    "        rank_at_100_A_lime += res[19]\n",
    "        rank_at_k_B_lime += res[20]\n",
    "        NDCG_at_1_lime += res[21]\n",
    "        NDCG_at_5_lime += res[22]\n",
    "        NDCG_at_10_lime += res[23]\n",
    "        NDCG_at_20_lime += res[24]\n",
    "        NDCG_at_50_lime += res[25]\n",
    "        NDCG_at_100_lime += res[26]\n",
    "        \n",
    "        ### SHAP:\n",
    "        res = single_user_metrics(user_vector,item_id, k, num_of_bins, num_items, rec_model, model_combined,  user_id = user_id, mask_type= 'shap')    \n",
    "           \n",
    "        POS_at_1_shap += res[0]\n",
    "        pos_at_5_shap += res[1]\n",
    "        POS_at_10_shap += res[2]\n",
    "        POS_at_20_shap += res[3]\n",
    "        POS_at_50_shap += res[4]\n",
    "        POS_at_100_shap += res[5]\n",
    "        NEG_at_1_shap += res[6]\n",
    "        NEG_at_5_shap += res[7]\n",
    "        NEG_at_10_shap += res[8]\n",
    "        NEG_at_20_shap += res[9]\n",
    "        NEG_at_50_shap += res[10]\n",
    "        NEG_at_100_shap += res[11]\n",
    "        users_DEL_shap += res[12]\n",
    "        users_INS_shap += res[13]\n",
    "        rank_at_1_shap += res[14]\n",
    "        rank_at_5_shap += res[15]\n",
    "        rank_at_10_A_shap += res[16]\n",
    "        rank_at_20_A_shap += res[17]\n",
    "        rank_at_50_A_shap += res[18]\n",
    "        rank_at_100_A_shap += res[19]\n",
    "        rank_at_k_B_shap += res[20]\n",
    "        NDCG_at_1_shap += res[21]\n",
    "        NDCG_at_5_shap += res[22]\n",
    "        NDCG_at_10_shap += res[23]\n",
    "        NDCG_at_20_shap += res[24]\n",
    "        NDCG_at_50_shap += res[25]\n",
    "        NDCG_at_100_shap += res[26]\n",
    "\n",
    "\n",
    "        if(i%100 == 0):\n",
    "            print(i)\n",
    "           \n",
    "    a = test_array.shape[0]\n",
    "\n",
    "print('POS_at_1_j_u: ', np.mean(POS_at_1_j_u[1:])/a)\n",
    "print('pos_at_5_j_u: ', np.mean(pos_at_5_j_u[1:])/a)\n",
    "print('POS_at_10_j_u: ', np.mean(POS_at_10_j_u[1:])/a)\n",
    "print('POS_at_20_j_u: ', np.mean(POS_at_20_j_u[1:])/a)\n",
    "print('POS_at_50_j_u: ', np.mean(POS_at_50_j_u[1:])/a)\n",
    "print('POS_at_100_j_u: ', np.mean(POS_at_100_j_u[1:])/a)\n",
    "print('NEG_at_1_j_u: ', np.mean(NEG_at_1_j_u[1:])/a)\n",
    "print('NEG_at_5_j_u: ', np.mean(NEG_at_5_j_u[1:])/a)\n",
    "print('NEG_at_10_j_u: ', np.mean(NEG_at_10_j_u[1:])/a)\n",
    "print('NEG_at_20_j_u: ', np.mean(NEG_at_20_j_u[1:])/a)\n",
    "print('NEG_at_50_j_u: ', np.mean(NEG_at_50_j_u[1:])/a)\n",
    "print('NEG_at_100_j_u: ', np.mean(NEG_at_100_j_u[1:])/a)\n",
    "print('users_DEL_j_u: ', np.mean(users_DEL_j_u[1:])/a)\n",
    "print('users_INS_j_u: ', np.mean(users_INS_j_u[1:])/a)\n",
    "print('rank_at_1_j_u: ', np.mean(rank_at_1_j_u[1:])/a)\n",
    "print('rank_at_5_j_u: ', np.mean(rank_at_5_j_u[1:])/a)\n",
    "print('rank_at_10_A_j_u: ', np.mean(rank_at_10_A_j_u[1:])/a)\n",
    "print('rank_at_20_A_j_u: ', np.mean(rank_at_20_A_j_u[1:])/a)\n",
    "print('rank_at_50_A_j_u: ', np.mean(rank_at_50_A_j_u[1:])/a)\n",
    "print('rank_at_100_A_j_u: ', np.mean(rank_at_100_A_j_u[1:])/a)\n",
    "print('rank_at_k_B_j_u: ', np.mean(rank_at_k_B_j_u[1:])/a)\n",
    "print('NDCG_at_1_j_u: ', np.mean(NDCG_at_1_j_u[1:])/a)\n",
    "print('NDCG_at_5_j_u: ', np.mean(NDCG_at_5_j_u[1:])/a)\n",
    "print('NDCG_at_10_j_u: ', np.mean(NDCG_at_10_j_u[1:])/a)\n",
    "print('NDCG_at_20_j_u: ', np.mean(NDCG_at_20_j_u[1:])/a)\n",
    "print('NDCG_at_50_j_u: ', np.mean(NDCG_at_50_j_u[1:])/a)\n",
    "print('NDCG_at_100_j_u: ', np.mean(NDCG_at_100_j_u[1:])/a)\n",
    "\n",
    "\n",
    "print('POS_at_1_c_s: ', np.mean(POS_at_1_c_s[1:])/a)\n",
    "print('pos_at_5_c_s: ', np.mean(pos_at_5_c_s[1:])/a)\n",
    "print('POS_at_10_c_s: ', np.mean(POS_at_10_c_s[1:])/a)\n",
    "print('POS_at_20_c_s: ', np.mean(POS_at_20_c_s[1:])/a)\n",
    "print('POS_at_50_c_s: ', np.mean(POS_at_50_c_s[1:])/a)\n",
    "print('POS_at_100_c_s: ', np.mean(POS_at_100_c_s[1:])/a)\n",
    "print('NEG_at_1_c_s: ', np.mean(NEG_at_1_c_s[1:])/a)\n",
    "print('NEG_at_5_c_s: ', np.mean(NEG_at_5_c_s[1:])/a)\n",
    "print('NEG_at_10_c_s: ', np.mean(NEG_at_10_c_s[1:])/a)\n",
    "print('NEG_at_20_c_s: ', np.mean(NEG_at_20_c_s[1:])/a)\n",
    "print('NEG_at_50_c_s: ', np.mean(NEG_at_50_c_s[1:])/a)\n",
    "print('NEG_at_100_c_s: ', np.mean(NEG_at_100_c_s[1:])/a)\n",
    "print('users_DEL_c_s: ', np.mean(users_DEL_c_s[1:])/a)\n",
    "print('users_INS_c_s: ', np.mean(users_INS_c_s[1:])/a)\n",
    "print('rank_at_1_c_s: ', np.mean(rank_at_1_c_s[1:])/a)\n",
    "print('rank_at_5_c_s: ', np.mean(rank_at_5_c_s[1:])/a)\n",
    "print('rank_at_10_A_c_s: ', np.mean(rank_at_10_A_c_s[1:])/a)\n",
    "print('rank_at_20_A_c_s: ', np.mean(rank_at_20_A_c_s[1:])/a)\n",
    "print('rank_at_50_A_c_s: ', np.mean(rank_at_50_A_c_s[1:])/a)\n",
    "print('rank_at_100_A_c_s: ', np.mean(rank_at_100_A_c_s[1:])/a)\n",
    "print('rank_at_k_B_c_s: ', np.mean(rank_at_k_B_c_s[1:])/a)\n",
    "print('NDCG_at_1_c_s: ', np.mean(NDCG_at_1_c_s[1:])/a)\n",
    "print('NDCG_at_5_c_s: ', np.mean(NDCG_at_5_c_s[1:])/a)\n",
    "print('NDCG_at_10_c_s: ', np.mean(NDCG_at_10_c_s[1:])/a)\n",
    "print('NDCG_at_20_c_s: ', np.mean(NDCG_at_20_c_s[1:])/a)\n",
    "print('NDCG_at_50_c_s: ', np.mean(NDCG_at_50_c_s[1:])/a)\n",
    "print('NDCG_at_100_c_s: ', np.mean(NDCG_at_100_c_s[1:])/a)\n",
    "\n",
    "print('POS_at_1_pop: ', np.mean(POS_at_1_pop[1:])/a)\n",
    "print('pos_at_5_pop: ', np.mean(pos_at_5_pop[1:])/a)\n",
    "print('POS_at_10_pop: ', np.mean(POS_at_10_pop[1:])/a)\n",
    "print('POS_at_20_pop: ', np.mean(POS_at_20_pop[1:])/a)\n",
    "print('POS_at_50_pop: ', np.mean(POS_at_50_pop[1:])/a)\n",
    "print('POS_at_100_pop: ', np.mean(POS_at_100_pop[1:])/a)\n",
    "print('NEG_at_1_pop: ', np.mean(NEG_at_1_pop[1:])/a)\n",
    "print('NEG_at_5_pop: ', np.mean(NEG_at_5_pop[1:])/a)\n",
    "print('NEG_at_10_pop: ', np.mean(NEG_at_10_pop[1:])/a)\n",
    "print('NEG_at_20_pop: ', np.mean(NEG_at_20_pop[1:])/a)\n",
    "print('NEG_at_50_pop: ', np.mean(NEG_at_50_pop[1:])/a)\n",
    "print('NEG_at_100_pop: ', np.mean(NEG_at_100_pop[1:])/a)\n",
    "print('users_DEL_pop: ', np.mean(users_DEL_pop[1:])/a)\n",
    "print('users_INS_pop: ', np.mean(users_INS_pop[1:])/a)\n",
    "print('rank_at_1_pop: ', np.mean(rank_at_1_pop[1:])/a)\n",
    "print('rank_at_5_pop: ', np.mean(rank_at_5_pop[1:])/a)\n",
    "print('rank_at_10_A_pop: ', np.mean(rank_at_10_A_pop[1:])/a)\n",
    "print('rank_at_20_A_pop: ', np.mean(rank_at_20_A_pop[1:])/a)\n",
    "print('rank_at_50_A_pop: ', np.mean(rank_at_50_A_pop[1:])/a)\n",
    "print('rank_at_100_A_pop: ', np.mean(rank_at_100_A_pop[1:])/a)\n",
    "print('rank_at_k_B_pop: ', np.mean(rank_at_k_B_pop[1:])/a)\n",
    "print('NDCG_at_1_pop: ', np.mean(NDCG_at_1_pop[1:])/a)\n",
    "print('NDCG_at_5_pop: ', np.mean(NDCG_at_5_pop[1:])/a)\n",
    "print('NDCG_at_10_pop: ', np.mean(NDCG_at_10_pop[1:])/a)\n",
    "print('NDCG_at_20_pop: ', np.mean(NDCG_at_20_pop[1:])/a)\n",
    "print('NDCG_at_50_pop: ', np.mean(NDCG_at_50_pop[1:])/a)\n",
    "print('NDCG_at_100_pop: ', np.mean(NDCG_at_100_pop[1:])/a)\n",
    "\n",
    "print('POS_at_1_lime: ', np.mean(POS_at_1_lime[1:])/a)\n",
    "print('pos_at_5_lime: ', np.mean(pos_at_5_lime[1:])/a)\n",
    "print('POS_at_10_lime: ', np.mean(POS_at_10_lime[1:])/a)\n",
    "print('POS_at_20_lime: ', np.mean(POS_at_20_lime[1:])/a)\n",
    "print('POS_at_50_lime: ', np.mean(POS_at_50_lime[1:])/a)\n",
    "print('POS_at_100_lime: ', np.mean(POS_at_100_lime[1:])/a)\n",
    "print('NEG_at_1_lime: ', np.mean(NEG_at_1_lime[1:])/a)\n",
    "print('NEG_at_5_lime: ', np.mean(NEG_at_5_lime[1:])/a)\n",
    "print('NEG_at_10_lime: ', np.mean(NEG_at_10_lime[1:])/a)\n",
    "print('NEG_at_20_lime: ', np.mean(NEG_at_20_lime[1:])/a)\n",
    "print('NEG_at_50_lime: ', np.mean(NEG_at_50_lime[1:])/a)\n",
    "print('NEG_at_100_lime: ', np.mean(NEG_at_100_lime[1:])/a)\n",
    "print('users_DEL_lime: ', np.mean(users_DEL_lime[1:])/a)\n",
    "print('users_INS_lime: ', np.mean(users_INS_lime[1:])/a)\n",
    "print('rank_at_1_lime: ', np.mean(rank_at_1_lime[1:])/a)\n",
    "print('rank_at_5_lime: ', np.mean(rank_at_5_lime[1:])/a)\n",
    "print('rank_at_10_A_lime: ', np.mean(rank_at_10_A_lime[1:])/a)\n",
    "print('rank_at_20_A_lime: ', np.mean(rank_at_20_A_lime[1:])/a)\n",
    "print('rank_at_50_A_lime: ', np.mean(rank_at_50_A_lime[1:])/a)\n",
    "print('rank_at_100_A_lime: ', np.mean(rank_at_100_A_lime[1:])/a)\n",
    "print('rank_at_k_B_lime: ', np.mean(rank_at_k_B_lime[1:])/a)\n",
    "print('NDCG_at_1_lime: ', np.mean(NDCG_at_1_lime[1:])/a)\n",
    "print('NDCG_at_5_lime: ', np.mean(NDCG_at_5_lime[1:])/a)\n",
    "print('NDCG_at_10_lime: ', np.mean(NDCG_at_10_lime[1:])/a)\n",
    "print('NDCG_at_20_lime: ', np.mean(NDCG_at_20_lime[1:])/a)\n",
    "print('NDCG_at_50_lime: ', np.mean(NDCG_at_50_lime[1:])/a)\n",
    "print('NDCG_at_100_lime: ', np.mean(NDCG_at_100_lime[1:])/a)\n",
    "\n",
    "print('POS_at_1_tf_idf: ', np.mean(POS_at_1_tf_idf[1:])/a)\n",
    "print('pos_at_5_tf_idf: ', np.mean(pos_at_5_tf_idf[1:])/a)\n",
    "print('POS_at_10_tf_idf: ', np.mean(POS_at_10_tf_idf[1:])/a)\n",
    "print('POS_at_20_tf_idf: ', np.mean(POS_at_20_tf_idf[1:])/a)\n",
    "print('POS_at_50_tf_idf: ', np.mean(POS_at_50_tf_idf[1:])/a)\n",
    "print('POS_at_100_tf_idf: ', np.mean(POS_at_100_tf_idf[1:])/a)\n",
    "print('NEG_at_1_tf_idf: ', np.mean(NEG_at_1_tf_idf[1:])/a)\n",
    "print('NEG_at_5_tf_idf: ', np.mean(NEG_at_5_tf_idf[1:])/a)\n",
    "print('NEG_at_10_tf_idf: ', np.mean(NEG_at_10_tf_idf[1:])/a)\n",
    "print('NEG_at_20_tf_idf: ', np.mean(NEG_at_20_tf_idf[1:])/a)\n",
    "print('NEG_at_50_tf_idf: ', np.mean(NEG_at_50_tf_idf[1:])/a)\n",
    "print('NEG_at_100_tf_idf: ', np.mean(NEG_at_100_tf_idf[1:])/a)\n",
    "print('users_DEL_tf_idf: ', np.mean(users_DEL_tf_idf[1:])/a)\n",
    "print('users_INS_tf_idf: ', np.mean(users_INS_tf_idf[1:])/a)\n",
    "print('rank_at_1_tf_idf: ', np.mean(rank_at_1_tf_idf[1:])/a)\n",
    "print('rank_at_5_tf_idf: ', np.mean(rank_at_5_tf_idf[1:])/a)\n",
    "print('rank_at_10_A_tf_idf: ', np.mean(rank_at_10_A_tf_idf[1:])/a)\n",
    "print('rank_at_20_A_tf_idf: ', np.mean(rank_at_20_A_tf_idf[1:])/a)\n",
    "print('rank_at_50_A_tf_idf: ', np.mean(rank_at_50_A_tf_idf[1:])/a)\n",
    "print('rank_at_100_A_tf_idf: ', np.mean(rank_at_100_A_tf_idf[1:])/a)\n",
    "print('rank_at_k_B_tf_idf: ', np.mean(rank_at_k_B_tf_idf[1:])/a)\n",
    "print('NDCG_at_1_tf_idf: ', np.mean(NDCG_at_1_tf_idf[1:])/a)\n",
    "print('NDCG_at_5_tf_idf: ', np.mean(NDCG_at_5_tf_idf[1:])/a)\n",
    "print('NDCG_at_10_tf_idf: ', np.mean(NDCG_at_10_tf_idf[1:])/a)\n",
    "print('NDCG_at_20_tf_idf: ', np.mean(NDCG_at_20_tf_idf[1:])/a)\n",
    "print('NDCG_at_50_tf_idf: ', np.mean(NDCG_at_50_tf_idf[1:])/a)\n",
    "print('NDCG_at_100_tf_idf: ', np.mean(NDCG_at_100_tf_idf[1:])/a)\n",
    "\n",
    "print('POS_at_1_lxr: ', np.mean(POS_at_1_lxr[1:])/a)\n",
    "print('pos_at_5_lxr: ', np.mean(pos_at_5_lxr[1:])/a)\n",
    "print('POS_at_10_lxr: ', np.mean(POS_at_10_lxr[1:])/a)\n",
    "print('POS_at_20_lxr: ', np.mean(POS_at_20_lxr[1:])/a)\n",
    "print('POS_at_50_lxr: ', np.mean(POS_at_50_lxr[1:])/a)\n",
    "print('POS_at_100_lxr: ', np.mean(POS_at_100_lxr[1:])/a)\n",
    "print('NEG_at_1_lxr: ', np.mean(NEG_at_1_lxr[1:])/a)\n",
    "print('NEG_at_5_lxr: ', np.mean(NEG_at_5_lxr[1:])/a)\n",
    "print('NEG_at_10_lxr: ', np.mean(NEG_at_10_lxr[1:])/a)\n",
    "print('NEG_at_20_lxr: ', np.mean(NEG_at_20_lxr[1:])/a)\n",
    "print('NEG_at_50_lxr: ', np.mean(NEG_at_50_lxr[1:])/a)\n",
    "print('NEG_at_100_lxr: ', np.mean(NEG_at_100_lxr[1:])/a)\n",
    "print('users_DEL_lxr: ', np.mean(users_DEL_lxr[1:])/a)\n",
    "print('users_INS_lxr: ', np.mean(users_INS_lxr[1:])/a)\n",
    "print('rank_at_1_lxr: ', np.mean(rank_at_1_lxr[1:])/a)\n",
    "print('rank_at_5_lxr: ', np.mean(rank_at_5_lxr[1:])/a)\n",
    "print('rank_at_10_A_lxr: ', np.mean(rank_at_10_A_lxr[1:])/a)\n",
    "print('rank_at_20_A_lxr: ', np.mean(rank_at_20_A_lxr[1:])/a)\n",
    "print('rank_at_50_A_lxr: ', np.mean(rank_at_50_A_lxr[1:])/a)\n",
    "print('rank_at_100_A_lxr: ', np.mean(rank_at_100_A_lxr[1:])/a)\n",
    "print('rank_at_k_B_lxr: ', np.mean(rank_at_k_B_lxr[1:])/a)\n",
    "print('NDCG_at_1_lxr: ', np.mean(NDCG_at_1_lxr[1:])/a)\n",
    "print('NDCG_at_5_lxr: ', np.mean(NDCG_at_5_lxr[1:])/a)\n",
    "print('NDCG_at_10_lxr: ', np.mean(NDCG_at_10_lxr[1:])/a)\n",
    "print('NDCG_at_20_lxr: ', np.mean(NDCG_at_20_lxr[1:])/a)\n",
    "print('NDCG_at_50_lxr: ', np.mean(NDCG_at_50_lxr[1:])/a)\n",
    "print('NDCG_at_100_lxr: ', np.mean(NDCG_at_100_lxr[1:])/a)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
