{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afc5481",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Softmax\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cb33cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from scipy import sparse\n",
    "import os\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import operator\n",
    "\n",
    "SEED = 3\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c66373-6942-47fe-9d73-2cd11efdc9ba",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893637cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_DIR = \"Data\"\n",
    "export_dir = Path(os.getcwd())\n",
    "files_path = Path(export_dir.parent, ML_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece46b76-8c96-4abf-b29e-20203e20e867",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(Path(files_path, \"yahoo_music_subset.csv\"), sep=\",\", engine=\"python\",\n",
    "                   names=[\"user_id_original\",\"item_id_original\", \"rating\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ced37c7-ea5e-47e5-857e-98950f383bd5",
   "metadata": {},
   "source": [
    "## Data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9a34ac-3e79-4a5b-8f47-14210405b8ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e896802-7bd8-4bc1-a741-068bd6333f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2600bc7a-08c9-442b-a15c-61e7875564c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Users with one or two items in the interaction\n",
    "user_ids = list(data.groupby('user_id_original')['item_id_original'].count()[lambda x: x < 3].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a601fb8a-47b0-4a1b-b964-58838e63efbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove such users from the data set\n",
    "filtered_data = data[~data['user_id_original'].isin(user_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4492044c-0942-4b3e-b22c-d3f764644bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification \n",
    "print(filtered_data.user_id_original.nunique()+len(user_ids) == data.user_id_original.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c13a8f7-ceaa-439f-a708-bc0f5e484f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5db2279-890e-41fa-9452-0fc3a6608d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(data.groupby('user_id_original')['item_id_original'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768c512c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the ratings to binary values (1 if rating exists, 0 otherwise)\n",
    "data[\"rating\"] = data[\"rating\"].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "\n",
    "# Encode target values\n",
    "data[\"user_id\"] = LabelEncoder().fit_transform(data.user_id_original)\n",
    "data[\"item_id\"] = LabelEncoder().fit_transform(data.item_id_original)\n",
    "\n",
    "# Get the number of users and items in the dataset\n",
    "num_users = data.user_id.unique().shape[0]\n",
    "num_items = data.item_id.unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e501ca8e-197c-448f-a546-8db0e90a7bf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_users = data.user_id.unique().shape[0]\n",
    "num_items = data.item_id.unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88046fa-ffdd-43c5-9e24-99d9fef5aaf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a2c4eb-91a5-4852-b8d4-a8e23311e151",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51181b12-b769-481f-853e-4f2975b7885a",
   "metadata": {},
   "source": [
    "## Data processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6424daca-b928-4f82-8cd5-933913413b02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_group = data[[\"user_id\",\"item_id\"]].groupby(data.user_id)\n",
    "\n",
    "users_data = pd.DataFrame(\n",
    "    data={\n",
    "        \"user_id\": list(user_group.groups.keys()),\n",
    "        \"item_ids\": list(user_group.item_id.apply(list)),\n",
    "    }    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9b495d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "user_one_hot = pd.DataFrame(mlb.fit_transform(users_data[\"item_ids\"]),columns=mlb.classes_, index=users_data[\"item_ids\"].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a824ee70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_one_hot[\"user_id\"] = users_data[\"user_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b7ba1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11485910-e042-4506-9601-40f66fe108bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_one_hot.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03117a7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_y_values(numpy_arr):\n",
    "    \"\"\"\n",
    "    Sample items from consumed items to predict (positive examples)\n",
    "    \"\"\"\n",
    "    y_values = []\n",
    "    users_arr = np.split(numpy_arr[:, 1], np.unique(numpy_arr[:, 0], return_index=True)[1][1:])\n",
    "    for u in users_arr:\n",
    "          y_values.append(int(np.random.choice(u[:-1],1,replace=False)))\n",
    "    return y_values    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afd2a35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_indices = get_y_values(np.argwhere(user_one_hot.to_numpy()>0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f785b727",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(y_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33d8a39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_one_hot[\"y_positive\"] = y_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf1a461",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create representation of each item as a binary vector\n",
    "items_values = pd.DataFrame(np.eye(num_items, dtype=int), columns=np.arange(num_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aa2809",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "items_values.to_csv(Path(export_dir, \"items_values_Yahoo.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8271bfb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "items_values.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a36b3a-9dd7-438a-b04d-8414a8e0b03e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "items_values_dict = {}\n",
    "for i in range(items_values.shape[0]):\n",
    "    items_values_dict[i] = items_values.iloc[i,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7222810-6617-42cb-9794-b3b194fd981e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = 'items_values_dict_Yahoo.pkl'\n",
    "\n",
    "# Open the file in write binary mode and use pickle.dump to save the dictionary\n",
    "with open(Path(export_dir,file_path), 'wb') as f:\n",
    "    pickle.dump(items_values_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa39022e-829a-4a80-801a-525a73dd495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.user_id.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f574642-365c-430e-ae04-2cc6931f8773",
   "metadata": {},
   "source": [
    "## Split to training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbeb6ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_state = 12\n",
    "# The list of users IDs\n",
    "users_indices = data.user_id.unique()\n",
    "\n",
    "# Set the split ratio (80% for training, 20% for testing)\n",
    "split_ratio = 0.8\n",
    "\n",
    "# Calculate the split index\n",
    "shuffled_users_ids = np.random.permutation(users_indices)\n",
    "split_index = int(len(shuffled_users_ids) * split_ratio)\n",
    "\n",
    "# Split the list of user IDs into training and testing sets\n",
    "train_user_ids = shuffled_users_ids[:split_index]\n",
    "test_user_ids = shuffled_users_ids[split_index:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865a6301",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_user_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26201527",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the user hot encoding matrix into training and testing sets based on the selected user IDs\n",
    "train_data = user_one_hot.loc[train_user_ids]\n",
    "test_data = user_one_hot.loc[test_user_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c9f3cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a6a6db-32ac-4c33-afc9-cb04a272a1e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_data = data[data.user_id.isin(train_user_ids)]\n",
    "\n",
    "# Group by item_id and aggregate user_id values into a list\n",
    "item_group = training_data.groupby(\"item_id\")\n",
    "\n",
    "items_data = pd.DataFrame(\n",
    "    data={\n",
    "        \"item_id\": list(item_group.groups.keys()),\n",
    "        \"users_ids\": list(item_group.user_id.apply(list)),\n",
    "    }    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0ca0e0-6a50-449d-a69c-7d017a9f9f57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "items_data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881f6323-0a43-4e83-ba03-d82601441a1b",
   "metadata": {},
   "source": [
    "## Creating files for baseline calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1729be7d-6537-4ff9-8df3-cf9aaeb0ffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(set1, set2):\n",
    "    \"\"\"\n",
    "    Calculate the Jaccard similarity between two sets of genres\n",
    "    \"\"\"\n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc3338e-158f-4e7c-88ed-f871df216828",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_user_based_Jaccard_sim():\n",
    "    \"\"\"\n",
    "    Calculate similarity between items based on users that consumed these items (Jaccard similarity)\n",
    "    \"\"\"\n",
    "    item_similarities = {}\n",
    "    for index1, row1 in items_data.iterrows():\n",
    "        for index2, row2 in items_data.iterrows():\n",
    "            if row1[\"item_id\"]!= row2[\"item_id\"]: #and item2>item1:\n",
    "                similarity = jaccard_similarity(set(row1[\"users_ids\"]), set(row2[\"users_ids\"]))\n",
    "            else:\n",
    "                similarity = 1\n",
    "            item_similarities[(row1[\"item_id\"], row2[\"item_id\"])] = similarity\n",
    "\n",
    "    return item_similarities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e709fc-2d77-4718-b531-86ec0f025a3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_similarities_Jaccard = create_user_based_Jaccard_sim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d69ab8-fcc7-4985-9ffc-c4edc9404fbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = 'user_similarities_Jaccard_Yahoo.pkl'\n",
    "\n",
    "# Open the file in write binary mode and use pickle.dump to save the dictionary\n",
    "with open(Path(export_dir,file_path), 'wb') as f:\n",
    "    pickle.dump(user_similarities_Jaccard, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273c792c-4221-4c88-96d1-b132487040ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use MultiLabelBinarizer to encode the user IDs for each item into a one-hot matrix\n",
    "mlb = MultiLabelBinarizer(classes=train_user_ids)  # Only include train_user_ids\n",
    "item_one_hot = pd.DataFrame(\n",
    "    mlb.fit_transform(items_data[\"users_ids\"]),\n",
    "    columns=mlb.classes_,\n",
    "    index=items_data[\"item_id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305d87b5-8a9d-4776-bdf7-0e5688f3e341",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_data.item_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfc00bc-db63-4d59-bccc-f5fb7960e82e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "item_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346f2535",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def item_user_based_cos_sim():\n",
    "    \"\"\"\n",
    "    Item similarity based on cosine between user and item\n",
    "    \"\"\"\n",
    "    return cosine_similarity(item_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da400bb-6018-4f0e-b670-adb99d4cfe55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cosine_items = item_user_based_cos_sim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b773e6-29fd-45d2-bae8-fe06659c1b7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cosine_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bd7139-77d0-41aa-9ddd-0e8b8ab166e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cosine_items_dict = {}\n",
    "\n",
    "# Loop through the rows and columns of the ndarray and add each element to the dictionary\n",
    "for i in range(cosine_items.shape[0]):\n",
    "    for j in range(cosine_items.shape[1]):\n",
    "        cosine_items_dict[(i, j)] = cosine_items[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a80d28-5640-4d9c-8152-e6872ade755c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cosine_items_dict[(0,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ada5339-cb46-407d-851b-f208738eb520",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = 'cosine_items_Yahoo.pkl'\n",
    "\n",
    "# Open the file in write binary mode and use pickle.dump to save the dictionary\n",
    "with open(Path(export_dir, file_path), 'wb') as f:\n",
    "    pickle.dump(cosine_items_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3e4efa-1c12-4045-8eb4-43fb9047dc7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cosine_items = cosine_items_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72a3fb9",
   "metadata": {},
   "source": [
    "### Add negative examples to training data & Calculate the popularity of each item in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943d3530",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "popularity_dict = train_data.iloc[:,:-2].sum(axis=0).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fe268a-997e-4faf-80c4-cb21426f446e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = 'pop_dict_Yahoo.pkl'\n",
    "\n",
    "with open(Path(export_dir, file_path), 'wb') as f:\n",
    "    pickle.dump(popularity_dict,f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45e19fa-13c2-422e-8cb2-6f11acd12c53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prob_dict = {}\n",
    "for k, v in popularity_dict.items():\n",
    "    prob_dict[k] = v / sum(popularity_dict.values())\n",
    "print(len(prob_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450a0368-41f8-4501-9e32-059966c3efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'prob_dict_Yahoo.pkl'\n",
    "\n",
    "with open(Path(export_dir, file_path), 'wb') as f:\n",
    "    pickle.dump(prob_dict,f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29de5b25-1cdd-498f-9f32-ac9602cbf7e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorted(prob_dict.items(), key=operator.itemgetter(1),reverse=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845995c6-8e9d-48d8-bb9f-ae2a5e20112e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_negative_samples(numpy_arr,num):\n",
    "    \"\"\"\n",
    "    Sample negative points \n",
    "    \"\"\"\n",
    "    negative_values = []\n",
    "    users_arr = np.split(numpy_arr[:, 1], np.unique(numpy_arr[:, 0], return_index=True)[1][1:])\n",
    "    for u in users_arr:\n",
    "        items_from_dict_keys = [d for d in popularity_dict.keys() if d in u]\n",
    "        sum_popularity= 0\n",
    "        for it in items_from_dict_keys:\n",
    "            sum_popularity += popularity_dict[it] \n",
    "        items_probs = [popularity_dict[d]/sum_popularity for d in items_from_dict_keys]\n",
    "        negative_samples = np.random.choice(items_from_dict_keys, size=num, replace=False, p=items_probs)\n",
    "        if(num == 1):\n",
    "            negative_values.append(int(negative_samples))\n",
    "        else:\n",
    "            negative_values.append(negative_samples) \n",
    "                               \n",
    "    return negative_values   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aeb35d-95be-47e5-848f-ebc347f4dd0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_negative = get_negative_samples(np.argwhere(train_data.iloc[:,:-2].to_numpy()==0), num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a1001d-ebba-44c7-9d07-64f3f721d322",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data[\"y_negative\"] = y_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ac8e59-e093-4c99-ba34-88f39b8bbab8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0036609-1467-4d59-8ea3-6280874c5acc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12cb90e",
   "metadata": {},
   "source": [
    "### Positive and Negative examples merged together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381ff222-8524-416f-83ff-52a59e5d632a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_mixed = train_data.merge(train_data.loc[:,['user_id','y_positive','y_negative']].melt('user_id', value_name='y_values').replace({'y_positive': 1, 'y_negative': 0}), on=\"user_id\").rename(columns={'variable': 'interaction'}).drop(['y_positive', 'y_negative'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42d2a3c-12ae-4652-abf5-5604509f88fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_mixed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479c204f-a9cb-4d97-bccd-a536e4192411",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_mixed.to_csv(Path(export_dir,'train_data_mixed_Yahoo.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5e7530-31df-4896-a82f-ca5cbf513e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv(Path(export_dir,\"test_data_Yahoo.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d67040-4a13-4cfc-bdb2-b45bc96d60fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array = train_data_mixed.to_numpy()\n",
    "test_array = test_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7847bfb0-e146-445c-8e19-1688102c661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_data =(train_array[train_array[:,-2]==0][:,:-3]).T\n",
    "test_data = (test_array[:,:-2]).T\n",
    "\n",
    "# Compute the IDF scores for each item in the train data\n",
    "num_docs = train_data.shape[0]\n",
    "idf = np.log(num_docs / (np.sum(train_data, axis=0) + 1))\n",
    "\n",
    "# Compute the TF-IDF scores for each item in the train data\n",
    "tfidf_matrix = np.zeros(train_data.shape)\n",
    "for i in range(train_data.shape[1]):\n",
    "    tf = train_data[:, i] / np.sum(train_data[:, i])\n",
    "    tfidf_matrix[:, i] = tf * idf[i]  \n",
    "\n",
    "tf_idf_items_dict = {(i, j): tfidf_matrix[i, j] for i in range(tfidf_matrix.shape[0]) for j in range(tfidf_matrix.shape[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a583109b-e349-4c39-8a23-51a5c8447999",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'tf_idf_items_Yahoo.pkl'\n",
    "\n",
    "# Open the file in write binary mode and use pickle.dump to save the dictionary\n",
    "with open(Path(export_dir, file_path), 'wb') as f:\n",
    "    pickle.dump(tf_idf_items_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd86059f-2b2e-4820-8e67-bc43bb2f5cc7",
   "metadata": {},
   "source": [
    "## Prepare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffa3fc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP_G(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(MLP_G, self).__init__()\n",
    "        self.linear_x = nn.Linear(input_size, hidden_size, bias = False)\n",
    "        self.linear_y = nn.Linear(input_size, hidden_size, bias = False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, user, item):\n",
    "        user_representation = self.linear_x(user.float())\n",
    "        item_representation = self.linear_y(item.float())\n",
    "        dot_prod = torch.matmul(user_representation, item_representation.T)\n",
    "        dot_sigmoid = self.sigmoid(dot_prod)\n",
    "        \n",
    "        return dot_sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5debee37-78bb-408a-8134-f9ba779e7ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recommender_G(nn.Module):\n",
    "    def __init__(self, num_items, hidden_size):\n",
    "        super(Recommender_G, self).__init__()\n",
    "        self.mlp = MLP_G(num_items, hidden_size).to(device)\n",
    "\n",
    "    def forward(self, user_vector, item_vector):\n",
    "        user_vector = user_vector.to(device)\n",
    "        item_vector = item_vector.to(device)\n",
    "        output = self.mlp(user_vector, item_vector)\n",
    "        return output.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689b97a8-fc7f-4c28-9548-5764e07c0f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k(user_vector, original_user_vector, num_items, model, top_k):\n",
    "    item_prob_dict = {}\n",
    "    user_tensor = torch.Tensor(user_vector).to(device)\n",
    "    item_tensor = torch.FloatTensor(items_array).to(device)\n",
    "    output_model = [float(i) for i in model(user_tensor, item_tensor).cpu().detach().numpy()]\n",
    "    \n",
    "    original_user_vector = np.array(original_user_vector.cpu())\n",
    "    neg = np.ones_like(original_user_vector)- original_user_vector\n",
    "    output = neg*output_model\n",
    "    for i in range(len(output)):\n",
    "        item_prob_dict[i]=output[i]\n",
    "\n",
    "    sorted_items_by_prob  = sorted(item_prob_dict.items(), key=lambda item: item[1],reverse=True)\n",
    "\n",
    "    return dict(sorted_items_by_prob[0:top_k])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed25629-6a08-4a5c-b08f-c4486cf6eaa9",
   "metadata": {},
   "source": [
    "## Train data on these models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96e4c1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the model on GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "hidden_dim = 20\n",
    "print(\"num_users is \", num_users)\n",
    "print(\"num_items is \", num_items)\n",
    "recommender_model_g = Recommender_G(num_items, hidden_dim)\n",
    "recommender_loss_g = nn.BCELoss()\n",
    "recommender_optimizer_g = torch.optim.Adam(recommender_model_g.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1fd3b70a-bb23-4a27-a5a6-6f45e604e850",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [79], line 28\u001b[0m\n\u001b[1;32m     24\u001b[0m     rec_loss \u001b[38;5;241m=\u001b[39m recommender_loss_g(recommender_output, interact_tensor)\n\u001b[1;32m     26\u001b[0m     train_loss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mrec_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 28\u001b[0m     \u001b[43mrec_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     recommender_optimizer_g\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     30\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss\u001b[38;5;241m/\u001b[39mtrain_array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    if epoch == 20:\n",
    "        recommender_optimizer.lr = 0.0001\n",
    "    train_loss = 0\n",
    "    for i in range(train_array.shape[0]):\n",
    "        item_id = train_array[i][-1]\n",
    "        user_id = train_array[i][-3]\n",
    "        item_vector = items_values_dict[item_id]\n",
    "        user_vector = train_array[i][:-3]\n",
    "        user_vector[item_id] = 0 \n",
    "        interact = train_array[i][-2]\n",
    "        \n",
    "        user_tensor = torch.FloatTensor(user_vector).to(device)\n",
    "        item_tensor = torch.FloatTensor(item_vector).to(device)\n",
    "      \n",
    "        interact_tensor = torch.FloatTensor([interact]).to(device)\n",
    "        interact_tensor = torch.tensor(interact_tensor.item()).to(device)\n",
    "        \n",
    "        recommender_optimizer_g.zero_grad()\n",
    "        recommender_output = (recommender_model_g(user_tensor, item_tensor)).to(device)\n",
    "        rec_loss = recommender_loss_g(recommender_output, interact_tensor)\n",
    "        \n",
    "        train_loss+=rec_loss.item()\n",
    "            \n",
    "        rec_loss.backward()\n",
    "        recommender_optimizer_g.step()\n",
    "    train_losses.append(train_loss/train_array.shape[0])\n",
    "    print(f\"Epoch {epoch}, Train Loss {train_loss/train_array.shape[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d53ffa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(recommender_model_g.state_dict(), Path(export_dir,'recommender_model_yahoo.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfd47fe",
   "metadata": {},
   "source": [
    "### Recommender Freezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524e4244-be51-407b-8526-31e761b4b5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in recommender_model_g.parameters():\n",
    "    param.requires_grad= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9440fce-fdbc-44e7-8e8e-2d65bb712d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_model = recommender_model_g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3229c3f-a706-4d1e-975c-60fb36fb481d",
   "metadata": {},
   "source": [
    "## Save top k items in training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1014b79-b4a6-4628-bb6a-ef307954aac8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Get users vectors to create topk\n",
    "unique_indices = np.unique(train_array[:,-3], return_index=True, axis=0)[1]\n",
    "\n",
    "# create a new array with only the unique users\n",
    "train_unique_arr = train_array[unique_indices, :]\n",
    "items_array = items_values.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf27d5da-3fc8-43a9-98b6-ebcdccf0d859",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create top_k dictionary that contains user, item, score for the rec_model\n",
    "#and is stored as (user_id, item_id, score) values\n",
    "def create_topk_data(data_, rec_model, data_type = \"train\"):\n",
    "\n",
    "    user_item_matrix = np.zeros((num_users, num_items))\n",
    "    \n",
    "    for i in range(data_.shape[0]):\n",
    "\n",
    "        if(data_type == \"train\"):\n",
    "            user_id = data_[i][-3]\n",
    "            user_vector = data_[i][:-3]\n",
    "        elif(data_type == \"test\"):\n",
    "            user_id = data_[i][-2]\n",
    "            user_vector = data_[i][:-2]\n",
    "            \n",
    "        user_tensor = torch.FloatTensor(user_vector).to(device)\n",
    "            \n",
    "        top_items = get_top_k(user_tensor, user_tensor, num_items, rec_model, num_items)\n",
    "      \n",
    "        for item_id in top_items.keys(): \n",
    "            user_item_matrix[user_id, item_id] = top_items[item_id]\n",
    "        \n",
    "        if(i % 100 == 0):\n",
    "            print(i)\n",
    "            \n",
    "    return user_item_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1b2bed-e049-43af-972f-26c13e700e0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topk_train = create_topk_data(train_unique_arr, rec_model, data_type=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf100df-077d-4140-bc76-695ea4ce8fd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topk_test = create_topk_data(test_array, rec_model, data_type=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6b4816-1b4c-431c-8407-68dcc5821a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'topk_train_Yahoo.pkl'\n",
    "\n",
    "# open the file in write-binary mode and save the array\n",
    "with open(Path(export_dir, file_path), 'wb') as f:\n",
    "    pickle.dump(topk_train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d5cdcd-5192-4fdf-829f-7d1cdcfb3352",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = 'topk_test_Yahoo.pkl'\n",
    "\n",
    "# open the file in write-binary mode and save the array\n",
    "with open(Path(export_dir, file_path), 'wb') as f:\n",
    "    pickle.dump(topk_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d53a2b1-64d4-479c-9c97-c135fd41d5a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d151d01-aaeb-4938-a623-eac9c52196b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
