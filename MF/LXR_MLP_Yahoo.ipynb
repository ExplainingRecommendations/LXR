{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2afc5481",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Softmax\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3cb33cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "    \n",
    "from scipy import sparse\n",
    "from os import path\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012d0825-dfd5-492c-8aa9-c80e0809f316",
   "metadata": {},
   "source": [
    "## Load baselines data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47a2c4eb-91a5-4852-b8d4-a8e23311e151",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "softmax = nn.Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b7222810-6617-42cb-9794-b3b194fd981e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = 'items_values_dict_Yahoo.pkl'\n",
    "\n",
    "with open(file_path, 'rb') as f:\n",
    "    items_values_dict = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "04d69ab8-fcc7-4985-9ffc-c4edc9404fbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = 'user_similarities_Jaccard_Yahoo.pkl'\n",
    "\n",
    "with open(file_path, 'rb') as f:\n",
    "    user_similarities_Jaccard = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2ada5339-cb46-407d-851b-f208738eb520",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = 'cosine_items_Yahoo.pkl'\n",
    "\n",
    "with open(file_path, 'rb') as f:\n",
    "    cosine_items_dict = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3a3e4efa-1c12-4045-8eb4-43fb9047dc7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cosine_items = cosine_items_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2e027c20-6f3d-4bfb-96ea-64ded575f683",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'tf_idf_items_Yahoo.pkl'\n",
    "\n",
    "with open(file_path, 'rb') as f:\n",
    "    tf_idf_items = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "450a0368-41f8-4501-9e32-059966c3efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'pop_dict_Yahoo.pkl'\n",
    "\n",
    "with open(file_path, 'rb') as f:\n",
    "    popularity_dict = pickle.load(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "29de5b25-1cdd-498f-9f32-ac9602cbf7e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5e5ac75b-5892-4227-9a30-7048e3c43b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import ipynb\n",
    "from ipynb.fs.defs.lime import distance_to_proximity, LimeBase, get_lime_args, gaussian_kernel, mlp_get_lime_args\n",
    "importlib.reload(ipynb.fs.defs.lime)\n",
    "from ipynb.fs.defs.lime import distance_to_proximity, LimeBase, get_lime_args, gaussian_kernel, mlp_get_lime_args\n",
    "'''\n",
    "distance_to_proximity(distances_list) - takes distances from origin user and returns proximity\n",
    "LimeBase() - class that gets kernel function\n",
    "get_lime_args(user_vetor, item_id, model, items_array, min_pert = 10, max_pert = 20, num_of_perturbations = 5, seed = 0) - \n",
    "    returns neighborhood_data, neighborhood_labels, distances, item_id \n",
    "'''\n",
    "\n",
    "lime = LimeBase(distance_to_proximity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d55b036-92a0-4257-9ec6-08620c27534c",
   "metadata": {},
   "source": [
    "## Models (Recommender and Explainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a5e7530-31df-4896-a82f-ca5cbf513e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_G(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(MLP_G, self).__init__()\n",
    "        self.linear_x = nn.Linear(input_size, hidden_size, bias = False)\n",
    "        self.linear_y = nn.Linear(input_size, hidden_size, bias = False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, user, item):\n",
    "        user_representation = self.linear_x(user.float())\n",
    "        item_representation = self.linear_y(item.float())\n",
    "        dot_prod = torch.matmul(user_representation, item_representation.T)\n",
    "        dot_sigmoid = self.sigmoid(dot_prod)\n",
    "        \n",
    "        return dot_sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "041f1d25-56b4-413f-9676-f249d709d549",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recommender_G(nn.Module):\n",
    "    def __init__(self, num_items, hidden_size):\n",
    "        super(Recommender_G, self).__init__()\n",
    "        self.mlp = MLP_G(num_items, hidden_size).to(device)\n",
    "\n",
    "    def forward(self, user_vector, item_vector):\n",
    "        user_vector = user_vector.to(device)\n",
    "        item_vector = item_vector.to(device)\n",
    "        output = self.mlp(user_vector, item_vector)\n",
    "        return output.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ffa3fc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Explainer_G(nn.Module):\n",
    "    def __init__(self, recommender_model_g, input_size, hidden_size):\n",
    "        super(Explainer_G, self).__init__()\n",
    "        \n",
    "        backbone_children = list(recommender_model_g.children())[0]\n",
    "\n",
    "        self.slice1 = nn.Sequential(*list(backbone_children.children())[:1])\n",
    "        self.slice2 = nn.Sequential(*list(backbone_children.children())[1:2])\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features = hidden_size*2, out_features=hidden_size*4),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features = hidden_size*4, out_features=hidden_size*4),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features = hidden_size*4, out_features=input_size),\n",
    "            nn.Sigmoid()\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        slice1_output = self.slice1(user.float())\n",
    "        slice2_output = self.slice2(item.float())\n",
    "        combined_output = torch.cat((slice1_output, slice2_output), dim=-1)\n",
    "        mask = self.bottleneck(combined_output).to(device)\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb926432-c8ef-4b77-8666-e914ba3ad651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class LossModelCombined(torch.nn.Module):\n",
    "    def __init__(self,alpha_parameter, recommender_model_g, explainer_model_g, hidden_size):\n",
    "        \n",
    "        super().__init__()\n",
    "            \n",
    "        self.recommender_model_g =  recommender_model_g\n",
    "        self.explainer_model_g= explainer_model_g\n",
    "        self.hidden_size = hidden_size\n",
    "        self.alpha_parameter = alpha_parameter\n",
    "\n",
    "        \n",
    "    def forward(self,user_hist, y_true):\n",
    "        \n",
    "        user_hist = user_hist.to(device)\n",
    "        y_true = torch.tensor(y_true).float().to(device)\n",
    "        \n",
    "        mask = self.explainer_model_g(user_hist,y_true).to(device)\n",
    "         # \"weakened\" history \n",
    "        x_masked = user_hist * mask\n",
    "        y_item_id = np.argmax(y_true.cpu().detach().numpy())\n",
    "        \n",
    "        y_masked = self.recommender_model_g(x_masked,y_true).unsqueeze(0).to(device)      \n",
    "        #cross entropy between the new \"masked\" prediction and the original one\n",
    "        cross_entropy_loss = -torch.log(y_masked).to(device)\n",
    "        #l1 loss on mask\n",
    "        l1_loss = torch.mean(torch.abs(mask)).to(device)\n",
    "        #combined loss\n",
    "        comb_loss = cross_entropy_loss + self.alpha_parameter * l1_loss\n",
    "        \n",
    "        \n",
    "        #print(\"Cross-entropy loss: {:.4f}, L1 loss: {:.4f}\".format(cross_entropy_loss.item(), bce_loss.item()))\n",
    "        \n",
    "        return x_masked, comb_loss  \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a96e4c1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_users is  13725\n",
      "num_items is  10265\n"
     ]
    }
   ],
   "source": [
    "# Train the model on GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cpu\"\n",
    "hidden_dim = 20\n",
    "num_users = 13725\n",
    "num_items = 10265\n",
    "print(\"num_users is \", num_users)\n",
    "print(\"num_items is \", num_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37cab73e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_model = Recommender_G(num_items, hidden_dim)\n",
    "rec_model.load_state_dict(torch.load('recommender_model_yahoo.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fd3b70a-bb23-4a27-a5a6-6f45e604e850",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in rec_model.parameters():\n",
    "    param.requires_grad= False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda70495-861d-4abe-9885-a907675729ce",
   "metadata": {},
   "source": [
    "## Read data from the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59a95439-e2de-4bfb-aec4-46d7274b7793",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_mixed = pd.read_csv('train_data_mixed_Yahoo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f451f90-03f1-4207-ab58-ff1ee7ebd9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test_data_Yahoo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb80d99e-0525-4a09-b4c7-6be306a86bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>10258</th>\n",
       "      <th>10259</th>\n",
       "      <th>10260</th>\n",
       "      <th>10261</th>\n",
       "      <th>10262</th>\n",
       "      <th>10263</th>\n",
       "      <th>10264</th>\n",
       "      <th>user_id</th>\n",
       "      <th>interaction</th>\n",
       "      <th>y_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7987</td>\n",
       "      <td>1</td>\n",
       "      <td>5303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7987</td>\n",
       "      <td>0</td>\n",
       "      <td>3309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10191</td>\n",
       "      <td>1</td>\n",
       "      <td>7053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10191</td>\n",
       "      <td>0</td>\n",
       "      <td>2273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3867</td>\n",
       "      <td>1</td>\n",
       "      <td>7389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 10268 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6  7  8  9  ...  10258  10259  10260  10261  10262  \\\n",
       "0  0  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
       "1  0  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
       "2  0  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
       "3  0  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
       "4  0  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
       "\n",
       "   10263  10264  user_id  interaction  y_values  \n",
       "0      0      0     7987            1      5303  \n",
       "1      0      0     7987            0      3309  \n",
       "2      0      0    10191            1      7053  \n",
       "3      0      0    10191            0      2273  \n",
       "4      0      0     3867            1      7389  \n",
       "\n",
       "[5 rows x 10268 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_mixed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d198ccb-e89f-4350-b9e9-4f786148bd77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>10257</th>\n",
       "      <th>10258</th>\n",
       "      <th>10259</th>\n",
       "      <th>10260</th>\n",
       "      <th>10261</th>\n",
       "      <th>10262</th>\n",
       "      <th>10263</th>\n",
       "      <th>10264</th>\n",
       "      <th>user_id</th>\n",
       "      <th>y_positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4891</td>\n",
       "      <td>3025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12745</td>\n",
       "      <td>7426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9355</td>\n",
       "      <td>5693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6963</td>\n",
       "      <td>6269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10601</td>\n",
       "      <td>6542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 10267 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6  7  8  9  ...  10257  10258  10259  10260  10261  \\\n",
       "0  0  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
       "1  0  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
       "2  0  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
       "3  0  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
       "4  0  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
       "\n",
       "   10262  10263  10264  user_id  y_positive  \n",
       "0      0      0      0     4891        3025  \n",
       "1      0      0      0    12745        7426  \n",
       "2      0      0      0     9355        5693  \n",
       "3      0      0      0     6963        6269  \n",
       "4      0      0      0    10601        6542  \n",
       "\n",
       "[5 rows x 10267 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dad84345-6533-4680-9dd8-1b14137fd15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('items_values_dict_Yahoo.pkl', 'rb') as f:\n",
    "    items_values_dict = pickle.load(f) # deserialize using load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5492be4d-77a2-4027-bf9c-02a59b4e0b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_values= pd.read_csv('items_values_Yahoo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5bfc5b9b-dd0b-4eca-9e86-3ad934735248",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_array = items_values.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6a10384-2767-45f2-9f38-149755c9b20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array = train_data_mixed.to_numpy()\n",
    "test_array = test_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "253a1f53-6f41-4fea-a2a2-ffb619d1d8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top k dictionaries for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a154033f-5fed-41a0-88f7-26de7d2e664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'topk_train_Yahoo.pkl'\n",
    "\n",
    "with open(filename, 'rb') as f:\n",
    "    topk_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c47f1600-5173-425b-b909-5ce178903677",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'topk_test_Yahoo.pkl'\n",
    "\n",
    "with open(filename, 'rb') as f:\n",
    "    topk_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eaaf4db5-ec68-4e47-9df6-432dcd806334",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'shap_values_MF_Yahoo.pkl'\n",
    "with open(file_path, 'rb') as f:\n",
    "    shap_values = pickle.load(f)\n",
    "\n",
    "file_path = 'item_to_cluster_Yahoo.pkl'\n",
    "with open(file_path, 'rb') as f:\n",
    "    item_to_cluster = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f896b18-7bdf-4035-a928-6d82a8ca811b",
   "metadata": {},
   "source": [
    "## Metrics calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e1b2bed-e049-43af-972f-26c13e700e0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_top_k(user_vector, original_user_vector, num_items, model, top_k):\n",
    "    item_prob_dict = {}\n",
    "    user_tensor = torch.Tensor(user_vector).to(device)\n",
    "    item_tensor = torch.FloatTensor(items_array).to(device)\n",
    "    output_model = [float(i) for i in model(user_tensor, item_tensor).cpu().detach().numpy()]\n",
    "    \n",
    "    original_user_vector = np.array(original_user_vector.cpu())\n",
    "    neg = np.ones_like(original_user_vector)- original_user_vector\n",
    "    output = neg*output_model\n",
    "    for i in range(len(output)):\n",
    "        item_prob_dict[i]=output[i]\n",
    "\n",
    "    sorted_items_by_prob  = sorted(item_prob_dict.items(), key=lambda item: item[1],reverse=True)\n",
    "\n",
    "    return dict(sorted_items_by_prob[0:top_k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ecf100df-077d-4140-bc76-695ea4ce8fd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_index_in_the_list(user_vector, original_user_vector, item_id, num_items, model):\n",
    "    top_k_list = list(get_top_k(user_vector, original_user_vector, num_items, model, num_items).keys())\n",
    "    return top_k_list.index(item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "195a9425-d6fa-4b50-948c-6bb7ff2e96ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5addf221-c6e9-49d1-9c2f-c0776c2aff0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_ndcg(ranked_list, target_item):\n",
    "    if target_item not in ranked_list:\n",
    "        return 0.0\n",
    "\n",
    "    target_idx = torch.tensor(ranked_list.index(target_item), device=device)\n",
    "    ndcg = torch.reciprocal(torch.log2(target_idx + 2))\n",
    "\n",
    "    return ndcg.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "19286437-e93f-4a10-8201-dbb16e5363d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd101ed-17d8-4654-b41f-d146f018fb03",
   "metadata": {},
   "source": [
    "## Mask calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8d42338-5589-4575-9d2e-9f19d91b004f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_LIME_mask(x, item_id, items_array, min_pert, max_pert, num_of_perturbations, kernel_func, feature_selection, model, num_samples=10, method = 'POS'):\n",
    "    \n",
    "    user_hist = x \n",
    "    # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    lime.kernel_fn = kernel_func\n",
    "    neighborhood_data, neighborhood_labels, distances, item_id = mlp_get_lime_args(user_hist, item_id, model, items_array, min_pert = min_pert, max_pert = max_pert, num_of_perturbations = num_of_perturbations, seed = item_id)\n",
    "                                                                                  \n",
    "    most_pop_items  = lime.explain_instance_with_data(neighborhood_data, neighborhood_labels, distances, item_id, num_samples, feature_selection, pos_neg='POS')\n",
    "    most_pop_items_dict =  {key: value for key, value in most_pop_items}\n",
    "    \n",
    "    return most_pop_items_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "067890dc-7787-4640-9b37-5e4bac4a52e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_shapley_mask(user_vector, user_id, model, shap_values, item_to_cluster):\n",
    "       \n",
    "        item_shap = {}\n",
    "        shapley_values = shap_values[shap_values[:, 0].astype(int) == user_id][:,1:].flatten()\n",
    "        user_vector = user_vector.cpu().detach().numpy().astype(int)\n",
    "       \n",
    "        for i in np.where(user_vector.astype(int) == 1)[0]:\n",
    "            items_cluster = item_to_cluster[i]\n",
    "            item_shap[i] = shapley_values[int(items_cluster)]\n",
    " \n",
    "        return item_shap  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f5f8b5d6-617e-4ff1-bdac-7ad826053bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LXR based similarity\n",
    "def find_LXR_mask(x, y_true, item_id, model_combined):\n",
    "    \n",
    "    user_hist = torch.tensor(x) \n",
    "    user_hist[item_id] = 0 \n",
    "    \n",
    "    x_masked_g, loss_comb_g = model_combined(user_hist, y_true)\n",
    "    \n",
    "    x_masked_g = x_masked_g.to(device)\n",
    "    #item_sim_dict = {i: v for i, v in enumerate(x_masked_l)}\n",
    "    item_sim_dict = {i: x_masked_g[i].item() for i in range(x_masked_g.numel())}\n",
    "    \n",
    "    return item_sim_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "617c4c4a-de4c-42e5-8572-3acf8ad93355",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Genre based similarities using Jaccard\n",
    "def find_jaccard_g_mask(x, item_id, num_items,jaccard_based_sim):\n",
    "    user_hist = torch.tensor(x) # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    item_jaccard_dict = {}\n",
    "    for i in range(num_items): \n",
    "        if(user_hist[i] == 1): # iterate all positive items of the user\n",
    "            if((i,item_id) in jaccard_based_sim.keys()):\n",
    "                item_jaccard_dict[i]=jaccard_based_sim[(i,item_id)] # add Jaccard similarity between items\n",
    "            else:\n",
    "                item_jaccard_dict[i] = 0\n",
    "    \n",
    "    return item_jaccard_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "01a902eb-5acc-40b9-a170-c7025aee3b7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#User based similarities using Jaccard\n",
    "def find_jaccard_u_mask(x, item_id, num_items, user_based_Jaccard_sim):\n",
    "    user_hist = torch.tensor(x) # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    item_jaccard_dict = {}\n",
    "    for i in range(num_items): \n",
    "        if(user_hist[i] == 1): # iterate all positive items of the user\n",
    "            if((i,item_id) in user_based_Jaccard_sim.keys()):\n",
    "                item_jaccard_dict[i]=user_based_Jaccard_sim[(i,item_id)] # add Jaccard similarity between items\n",
    "            else:\n",
    "                item_jaccard_dict[i] = 0\n",
    "            \n",
    "    return item_jaccard_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ad6767b-0c7b-49de-8a9e-625ddb771891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Cosine based similarities between users and items\n",
    "def find_cosine_mask(x, item_id, num_items, item_cosine):\n",
    "\n",
    "    user_hist = torch.tensor(x) # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    item_cosine_dict = {}\n",
    "        \n",
    "    for i in range(num_items): \n",
    "        if(user_hist[i] == 1): # iterate all positive items of the user\n",
    "            if((i,item_id) in item_cosine.keys()):\n",
    "                item_cosine_dict[i]=item_cosine[(i,item_id)] # add cosine similarity between items\n",
    "            else:\n",
    "                item_cosine_dict[i] = 0\n",
    "    \n",
    "    return item_cosine_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5cc8fa68-dc09-4707-88a3-07eda457c43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf_idf mask\n",
    "def find_tf_idf_mask(x, item_id, num_items, tf_idf_sim):\n",
    "    \n",
    "    x = x.cpu().detach().numpy()\n",
    "    x[item_id] = 0\n",
    "    \n",
    "    positive_items = np.where(x == 1)[0]\n",
    "    tf_idf_dict = {i: tf_idf_sim.get((i, item_id), 0) for i in positive_items}\n",
    "    \n",
    "    return tf_idf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "960f4701-a38b-485d-bc34-942d4fe5e6f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#popularity mask\n",
    "def find_pop_mask(x, item_id, num_items):\n",
    "    user_hist = torch.tensor(x) # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    item_pop_dict = {}\n",
    "    for i in range(num_items): \n",
    "        if(user_hist[i] == 1): # iterate over all positive items of the user\n",
    "            item_pop_dict[i]=popularity_dict[i] # add the pop of the item to the dictionary\n",
    "            \n",
    "            \n",
    "    return item_pop_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "754aae1c-8ceb-4a99-bdb9-113458285286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_user_metrics(user_vector, item_id, k, num_of_bins, num_items, recommender_model, model_combined, y_pred = None, mask_type = None, user_id = None):\n",
    "\n",
    "    \n",
    "    user_tensor = torch.FloatTensor(user_vector).to(device)\n",
    "    \n",
    "    item_vector = items_values_dict[item_id]\n",
    "    item_tensor = torch.FloatTensor(item_vector).to(device)\n",
    "    \n",
    "    POS_masked = user_tensor\n",
    "    NEG_masked = user_tensor\n",
    "    POS_masked[item_id]=0\n",
    "    NEG_masked[item_id]=0\n",
    "    \n",
    "    user_hist_size = np.sum(user_vector)\n",
    "\n",
    "    \n",
    "    bins=[0]+[len(x) for x in np.array_split(np.arange(np.sum(user_vector)), num_of_bins, axis=0)]\n",
    "    \n",
    "    POS_at_1 = [0]*(len(bins))\n",
    "    POS_at_5 = [0]*(len(bins))\n",
    "    POS_at_10=[0]*(len(bins))\n",
    "    POS_at_20=[0]*(len(bins))\n",
    "    POS_at_50=[0]*(len(bins))\n",
    "    POS_at_100=[0]*(len(bins))\n",
    "    \n",
    "    NEG_at_1 = [0]*(len(bins))\n",
    "    NEG_at_5 = [0]*(len(bins))\n",
    "    NEG_at_10 = [0]*(len(bins))\n",
    "    NEG_at_20 = [0]*(len(bins))\n",
    "    NEG_at_50 = [0]*(len(bins))\n",
    "    NEG_at_100 = [0]*(len(bins))\n",
    "    \n",
    "    DEL = [0]*(len(bins))\n",
    "    INS = [0]*(len(bins))\n",
    "    \n",
    "    rankA_at_1 = [0]*(len(bins))\n",
    "    rankA_at_5 = [0]*(len(bins))\n",
    "    rankA_at_10 = [0]*(len(bins))\n",
    "    rankA_at_20 = [0]*(len(bins))\n",
    "    rankA_at_50 = [0]*(len(bins))\n",
    "    rankA_at_100 = [0]*(len(bins))\n",
    "    \n",
    "    rankB = [0]*(len(bins))\n",
    "    \n",
    "    NDCG_at_1 = [0]*(len(bins))\n",
    "    NDCG_at_5 = [0]*(len(bins))\n",
    "    NDCG_at_10 = [0]*(len(bins))\n",
    "    NDCG_at_20 = [0]*(len(bins))\n",
    "    NDCG_at_50 = [0]*(len(bins))\n",
    "    NDCG_at_100 = [0]*(len(bins))\n",
    "    \n",
    "    total_items = 0\n",
    "    \n",
    "    for i in range(len(bins)):\n",
    "        total_items += bins[i]\n",
    "        \n",
    "        if i == 0:\n",
    "            if mask_type == 'jaccard_u':\n",
    "                sim_items = find_jaccard_u_mask(POS_masked, item_id, num_items, user_similarities_Jaccard)\n",
    "            elif mask_type == 'cosine':\n",
    "                sim_items = find_cosine_mask(POS_masked, item_id, num_items, cosine_items)\n",
    "            elif mask_type == 'tf_idf':\n",
    "                sim_items = find_tf_idf_mask(POS_masked, item_id, num_items, tf_idf_items)\n",
    "            elif mask_type == 'pop':\n",
    "                sim_items = find_pop_mask(POS_masked, item_id, num_items)\n",
    "            elif mask_type == 'shap': \n",
    "                sim_items = find_shapley_mask(POS_masked, user_id, recommender_model, shap_values, item_to_cluster)\n",
    "            elif mask_type == 'lime':\n",
    "                sim_items = find_LIME_mask(user_vector, item_id, items_array, 50, 100, 150, distance_to_proximity,'highest_weights', model = recommender_model, num_samples=user_hist_size)\n",
    "            elif mask_type == 'lxr':\n",
    "                sim_items = find_LXR_mask(POS_masked, item_vector, item_id, model_combined)\n",
    "            else:\n",
    "                raise Exception(\"Wrong mask type\")\n",
    "        \n",
    "        POS_sim_items  = list(sorted(sim_items.items(), key=lambda item: item[1],reverse=True))[0:user_hist_size]\n",
    "        NEG_sim_items  = list(sorted(dict(POS_sim_items).items(), key=lambda item: item[1],reverse=False))\n",
    "        \n",
    "        POS_masked = torch.zeros_like(user_tensor, dtype=torch.float32, device=device)\n",
    "        for j in POS_sim_items[:total_items]:\n",
    "            POS_masked[j[0]] = 1\n",
    "        POS_masked = user_tensor - POS_masked\n",
    "        \n",
    "        NEG_masked = torch.zeros_like(user_tensor, dtype=torch.float32, device=device)\n",
    "        for j in NEG_sim_items[:total_items]:\n",
    "            NEG_masked[j[0]] = 1\n",
    "        NEG_masked = user_tensor - NEG_masked # remove the masked items from the user history \n",
    "\n",
    "        POS_ranked_list = get_top_k(POS_masked, user_tensor,num_items, recommender_model,num_items)        \n",
    "        POS_index = list(POS_ranked_list.keys()).index(item_id)+1\n",
    "        NEG_index = get_index_in_the_list(NEG_masked,user_tensor, item_id, num_items, recommender_model)+1\n",
    "   \n",
    "        # for pos:\n",
    "        POS_at_1[i] = 1 if POS_index <=1 else 0\n",
    "        POS_at_5[i] = 1 if POS_index <=5 else 0\n",
    "        POS_at_10[i] = 1 if POS_index <=10 else 0\n",
    "        POS_at_20[i] = 1 if POS_index <=20 else 0\n",
    "        POS_at_50[i] = 1 if POS_index <=50 else 0\n",
    "        POS_at_100[i] = 1 if POS_index <=100 else 0\n",
    "\n",
    "        # for neg:\n",
    "        NEG_at_1[i] = 1 if NEG_index <=1 else 0\n",
    "        NEG_at_5[i] = 1 if NEG_index <=5 else 0\n",
    "        NEG_at_10[i] = 1 if NEG_index <=10 else 0\n",
    "        NEG_at_20[i] = 1 if NEG_index <=20 else 0\n",
    "        NEG_at_50[i] = 1 if NEG_index <=50 else 0\n",
    "        NEG_at_100[i] = 1 if NEG_index <=100 else 0\n",
    "        # for del:\n",
    "        DEL[i] = float(recommender_model(POS_masked, item_tensor).detach().cpu().numpy())\n",
    "        \n",
    "        # for ins:\n",
    "        INS[i] = float(recommender_model(user_tensor-POS_masked, item_tensor).detach().cpu().numpy())\n",
    "        \n",
    "       # for rankA:\n",
    "        rankA_at_1[i] = max(0, (1+1-POS_index)/10)\n",
    "        rankA_at_5[i] = max(0, (5+1-POS_index)/20)\n",
    "        rankA_at_10[i] = max(0, (10+1-POS_index)/10)\n",
    "        rankA_at_20[i] = max(0, (20+1-POS_index)/20)\n",
    "        rankA_at_50[i] = max(0, (50+1-POS_index)/50)\n",
    "        rankA_at_100[i] = max(0, (100+1-POS_index)/100)\n",
    "\n",
    "        # for rankB:\n",
    "        rankB[i] = 1/POS_index\n",
    "\n",
    "        #for NDCG:\n",
    "        NDCG_at_1[i]= get_ndcg(list(POS_ranked_list.keys())[:1],item_id)\n",
    "        NDCG_at_5[i]= get_ndcg(list(POS_ranked_list.keys())[:5],item_id)\n",
    "        NDCG_at_10[i]= get_ndcg(list(POS_ranked_list.keys())[:10],item_id)\n",
    "        NDCG_at_20[i]= get_ndcg(list(POS_ranked_list.keys())[:20],item_id)\n",
    "        NDCG_at_50[i]= get_ndcg(list(POS_ranked_list.keys())[:50],item_id)\n",
    "        NDCG_at_100[i]= get_ndcg(list(POS_ranked_list.keys())[:100],item_id)\n",
    "\n",
    "    res = [POS_at_1, POS_at_5, POS_at_10, POS_at_20, POS_at_50, POS_at_100, NEG_at_1, NEG_at_5, NEG_at_10, NEG_at_20, NEG_at_50, NEG_at_100, DEL, INS, rankA_at_1, rankA_at_5, rankA_at_10, rankA_at_20, rankA_at_50, rankA_at_100, rankB, NDCG_at_1, NDCG_at_5, NDCG_at_10, NDCG_at_20, NDCG_at_50, NDCG_at_100]\n",
    "    for i in range(len(res)):\n",
    "        res[i] = np.array(res[i])\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3a55d8-d1be-4cd6-9b54-790602518867",
   "metadata": {},
   "source": [
    "## Train LXR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "075a4a8b-04b6-4391-a70a-bce22d900344",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get users vectors to create topk\n",
    "unique_indices = np.unique(train_array[:,-3], return_index=True, axis=0)[1]\n",
    "\n",
    "# create a new array with only the unique users\n",
    "train_unique_arr = train_array[unique_indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fe054c3b-9c29-4f77-82f5-06ffba6dc2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_model_g = Explainer_G(rec_model, num_items, hidden_dim).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eb999b3f-c39c-4cb1-b2e5-32387d242246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss 0.0886\n",
      "Epoch 1, Train Loss 0.0478\n",
      "Epoch 2, Train Loss 0.0320\n",
      "Epoch 3, Train Loss 0.0278\n",
      "Epoch 4, Train Loss 0.0244\n",
      "Epoch 5, Train Loss 0.0225\n",
      "Epoch 6, Train Loss 0.0201\n",
      "Epoch 7, Train Loss 0.0183\n",
      "Epoch 8, Train Loss 0.0180\n",
      "Epoch 9, Train Loss 0.0178\n"
     ]
    }
   ],
   "source": [
    "#LXR training\n",
    "\n",
    "#torch.manual_seed(42)\n",
    "import time\n",
    "import random\n",
    "train_losses = []\n",
    "epochs = 10\n",
    "hidden_dim = 20\n",
    "num_of_bins = 10\n",
    "alpha_parameter = 0.5\n",
    "\n",
    "model_combined = LossModelCombined(alpha_parameter,rec_model, explainer_model_g, hidden_dim).to(device)\n",
    "optimizer_comb = torch.optim.Adam(model_combined.parameters(), lr=0.01)\n",
    "    \n",
    "#Train LXR\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    \n",
    "    for i in range(train_unique_arr.shape[0]):\n",
    "        \n",
    "        #user data\n",
    "        user_id = train_unique_arr[i][-3]\n",
    "        user_vector = train_unique_arr[i][:-3]\n",
    "        #get top1 of this user for LXR training\n",
    "        top1_item = np.argmax(topk_train[user_id])\n",
    "\n",
    "        positive_sample = torch.FloatTensor(items_values_dict[top1_item]).to(device)\n",
    "        \n",
    "        user_vector[top1_item] = 0 \n",
    "        user_tensor = torch.FloatTensor(user_vector).to(device)\n",
    "\n",
    "        #train model        \n",
    "        optimizer_comb.zero_grad()\n",
    "        x_masked_g, loss_comb_g = model_combined(user_tensor, positive_sample)\n",
    "        \n",
    "        train_loss+=loss_comb_g.item()\n",
    "            \n",
    "        loss_comb_g.backward()\n",
    "        optimizer_comb.step()\n",
    "        \n",
    "  \n",
    "    train_losses.append(train_loss/train_unique_arr.shape[0])\n",
    "\n",
    "    print(f\"Epoch {epoch}, Train Loss {train_loss/train_unique_arr.shape[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e6490426-daf5-401f-8bde-f767660afb95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model_combined.state_dict(), 'model_combined_yahoo_10_05_tanh.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "19ab9629-7c3a-4f52-9779-4940bd3d7dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(explainer_model_g.state_dict(), 'explainer_model_yahoo_10_05_tanh.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07b3563-4d6e-43f6-9e4e-f643d545e93f",
   "metadata": {},
   "source": [
    "## LXR on testing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d38f437f-e6da-410c-b07a-93a0eaa542e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "POS_at_1_j_u:  0.05\n",
      "pos_at_5_j_u:  0.1\n",
      "POS_at_10_j_u:  0.15\n",
      "POS_at_20_j_u:  0.15\n",
      "POS_at_50_j_u:  0.15\n",
      "POS_at_100_j_u:  0.25\n",
      "NEG_at_1_j_u:  0.2\n",
      "NEG_at_5_j_u:  0.25\n",
      "NEG_at_10_j_u:  0.3\n",
      "NEG_at_20_j_u:  0.3\n",
      "NEG_at_50_j_u:  0.45\n",
      "NEG_at_100_j_u:  0.5\n",
      "users_DEL_j_u:  0.4350324422121048\n",
      "users_INS_j_u:  0.4999946057796478\n",
      "rank_at_1_j_u:  0.005\n",
      "rank_at_5_j_u:  0.0225\n",
      "rank_at_10_A_j_u:  0.10999999999999999\n",
      "rank_at_20_A_j_u:  0.13\n",
      "rank_at_50_A_j_u:  0.142\n",
      "rank_at_100_A_j_u:  0.1785\n",
      "rank_at_k_B_j_u:  0.08391528830041572\n",
      "NDCG_at_1_j_u:  0.05\n",
      "NDCG_at_5_j_u:  0.08154648840427399\n",
      "NDCG_at_10_j_u:  0.09731973260641098\n",
      "NDCG_at_20_j_u:  0.09731973260641098\n",
      "NDCG_at_50_j_u:  0.09731973260641098\n",
      "NDCG_at_100_j_u:  0.11366375163197517\n",
      "POS_at_1_c_s:  0.15\n",
      "pos_at_5_c_s:  0.15\n",
      "POS_at_10_c_s:  0.15\n",
      "POS_at_20_c_s:  0.15\n",
      "POS_at_50_c_s:  0.2\n",
      "POS_at_100_c_s:  0.3\n",
      "NEG_at_1_c_s:  0.15\n",
      "NEG_at_5_c_s:  0.15\n",
      "NEG_at_10_c_s:  0.15\n",
      "NEG_at_20_c_s:  0.15\n",
      "NEG_at_50_c_s:  0.2\n",
      "NEG_at_100_c_s:  0.3\n",
      "users_DEL_c_s:  0.37521351013583626\n",
      "users_INS_c_s:  0.449311552150175\n",
      "rank_at_1_c_s:  0.015000000000000003\n",
      "rank_at_5_c_s:  0.0375\n",
      "rank_at_10_A_c_s:  0.15\n",
      "rank_at_20_A_c_s:  0.15\n",
      "rank_at_50_A_c_s:  0.152\n",
      "rank_at_100_A_c_s:  0.21000000000000002\n",
      "rank_at_k_B_c_s:  0.1529772665552318\n",
      "NDCG_at_1_c_s:  0.15\n",
      "NDCG_at_5_c_s:  0.15\n",
      "NDCG_at_10_c_s:  0.15\n",
      "NDCG_at_20_c_s:  0.15\n",
      "NDCG_at_50_c_s:  0.15885919108986854\n",
      "NDCG_at_100_c_s:  0.17529198974370958\n",
      "POS_at_1_pop:  0.15\n",
      "pos_at_5_pop:  0.15\n",
      "POS_at_10_pop:  0.2\n",
      "POS_at_20_pop:  0.25\n",
      "POS_at_50_pop:  0.25\n",
      "POS_at_100_pop:  0.4\n",
      "NEG_at_1_pop:  0.1\n",
      "NEG_at_5_pop:  0.1\n",
      "NEG_at_10_pop:  0.1\n",
      "NEG_at_20_pop:  0.1\n",
      "NEG_at_50_pop:  0.4\n",
      "NEG_at_100_pop:  0.45\n",
      "users_DEL_pop:  0.4746876239776611\n",
      "users_INS_pop:  0.49363228380680085\n",
      "rank_at_1_pop:  0.015000000000000003\n",
      "rank_at_5_pop:  0.0375\n",
      "rank_at_10_A_pop:  0.16499999999999998\n",
      "rank_at_20_A_pop:  0.205\n",
      "rank_at_50_A_pop:  0.23199999999999998\n",
      "rank_at_100_A_pop:  0.26649999999999996\n",
      "rank_at_k_B_pop:  0.1628017813839048\n",
      "NDCG_at_1_pop:  0.15\n",
      "NDCG_at_5_pop:  0.15\n",
      "NDCG_at_10_pop:  0.165773244202137\n",
      "NDCG_at_20_pop:  0.1792851522564888\n",
      "NDCG_at_50_pop:  0.1792851522564888\n",
      "NDCG_at_100_pop:  0.2027492456138134\n",
      "POS_at_1_lime:  0.0\n",
      "pos_at_5_lime:  0.0\n",
      "POS_at_10_lime:  0.0\n",
      "POS_at_20_lime:  0.0\n",
      "POS_at_50_lime:  0.0\n",
      "POS_at_100_lime:  0.1\n",
      "NEG_at_1_lime:  0.35\n",
      "NEG_at_5_lime:  0.4\n",
      "NEG_at_10_lime:  0.4\n",
      "NEG_at_20_lime:  0.45\n",
      "NEG_at_50_lime:  0.45\n",
      "NEG_at_100_lime:  0.5\n",
      "users_DEL_lime:  0.08016855405810416\n",
      "users_INS_lime:  0.4999999046325684\n",
      "rank_at_1_lime:  0.0\n",
      "rank_at_5_lime:  0.0\n",
      "rank_at_10_A_lime:  0.0\n",
      "rank_at_20_A_lime:  0.0\n",
      "rank_at_50_A_lime:  0.0\n",
      "rank_at_100_A_lime:  0.029500000000000005\n",
      "rank_at_k_B_lime:  0.001439599214633416\n",
      "NDCG_at_1_lime:  0.0\n",
      "NDCG_at_5_lime:  0.0\n",
      "NDCG_at_10_lime:  0.0\n",
      "NDCG_at_20_lime:  0.0\n",
      "NDCG_at_50_lime:  0.0\n",
      "NDCG_at_100_lime:  0.016182654350996018\n",
      "POS_at_1_tf_idf:  0.15\n",
      "pos_at_5_tf_idf:  0.15\n",
      "POS_at_10_tf_idf:  0.15\n",
      "POS_at_20_tf_idf:  0.2\n",
      "POS_at_50_tf_idf:  0.3\n",
      "POS_at_100_tf_idf:  0.35\n",
      "NEG_at_1_tf_idf:  0.15\n",
      "NEG_at_5_tf_idf:  0.15\n",
      "NEG_at_10_tf_idf:  0.15\n",
      "NEG_at_20_tf_idf:  0.15\n",
      "NEG_at_50_tf_idf:  0.15\n",
      "NEG_at_100_tf_idf:  0.25\n",
      "users_DEL_tf_idf:  0.38623910905243974\n",
      "users_INS_tf_idf:  0.39988045003265144\n",
      "rank_at_1_tf_idf:  0.015000000000000003\n",
      "rank_at_5_tf_idf:  0.0375\n",
      "rank_at_10_A_tf_idf:  0.15\n",
      "rank_at_20_A_tf_idf:  0.16\n",
      "rank_at_50_A_tf_idf:  0.21600000000000003\n",
      "rank_at_100_A_tf_idf:  0.27349999999999997\n",
      "rank_at_k_B_tf_idf:  0.15682948510416586\n",
      "NDCG_at_1_tf_idf:  0.15\n",
      "NDCG_at_5_tf_idf:  0.15\n",
      "NDCG_at_10_tf_idf:  0.15\n",
      "NDCG_at_20_tf_idf:  0.16199062243103982\n",
      "NDCG_at_50_tf_idf:  0.18149740472435952\n",
      "NDCG_at_100_tf_idf:  0.18962782099843026\n",
      "POS_at_1_lxr:  0.0\n",
      "pos_at_5_lxr:  0.0\n",
      "POS_at_10_lxr:  0.0\n",
      "POS_at_20_lxr:  0.0\n",
      "POS_at_50_lxr:  0.0\n",
      "POS_at_100_lxr:  0.05\n",
      "NEG_at_1_lxr:  0.3\n",
      "NEG_at_5_lxr:  0.45\n",
      "NEG_at_10_lxr:  0.45\n",
      "NEG_at_20_lxr:  0.45\n",
      "NEG_at_50_lxr:  0.45\n",
      "NEG_at_100_lxr:  0.5\n",
      "users_DEL_lxr:  0.10700900637976\n",
      "users_INS_lxr:  0.5\n",
      "rank_at_1_lxr:  0.0\n",
      "rank_at_5_lxr:  0.0\n",
      "rank_at_10_A_lxr:  0.0\n",
      "rank_at_20_A_lxr:  0.0\n",
      "rank_at_50_A_lxr:  0.0\n",
      "rank_at_100_A_lxr:  0.0155\n",
      "rank_at_k_B_lxr:  0.0008999378603922657\n",
      "NDCG_at_1_lxr:  0.0\n",
      "NDCG_at_5_lxr:  0.0\n",
      "NDCG_at_10_lxr:  0.0\n",
      "NDCG_at_20_lxr:  0.0\n",
      "NDCG_at_50_lxr:  0.0\n",
      "NDCG_at_100_lxr:  0.00813041627407074\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Evaluate the model on the test set\n",
    "num_of_bins = 11\n",
    "k = 100\n",
    "\n",
    "POS_at_1_shap = np.zeros(num_of_bins)\n",
    "pos_at_5_shap = np.zeros(num_of_bins)\n",
    "POS_at_10_shap = np.zeros(num_of_bins)\n",
    "POS_at_20_shap = np.zeros(num_of_bins)\n",
    "POS_at_50_shap = np.zeros(num_of_bins)\n",
    "POS_at_100_shap = np.zeros(num_of_bins)\n",
    "NEG_at_1_shap = np.zeros(num_of_bins)\n",
    "NEG_at_5_shap = np.zeros(num_of_bins)\n",
    "NEG_at_10_shap = np.zeros(num_of_bins)\n",
    "NEG_at_20_shap = np.zeros(num_of_bins)\n",
    "NEG_at_50_shap = np.zeros(num_of_bins)\n",
    "NEG_at_100_shap = np.zeros(num_of_bins)\n",
    "users_DEL_shap = np.zeros(num_of_bins)\n",
    "users_INS_shap = np.zeros(num_of_bins)\n",
    "rank_at_1_shap = np.zeros(num_of_bins)\n",
    "rank_at_5_shap = np.zeros(num_of_bins)\n",
    "rank_at_10_A_shap = np.zeros(num_of_bins)\n",
    "rank_at_20_A_shap = np.zeros(num_of_bins)\n",
    "rank_at_50_A_shap = np.zeros(num_of_bins)\n",
    "rank_at_100_A_shap = np.zeros(num_of_bins)\n",
    "rank_at_k_B_shap = np.zeros(num_of_bins)\n",
    "NDCG_at_1_shap = np.zeros(num_of_bins)\n",
    "NDCG_at_5_shap = np.zeros(num_of_bins)\n",
    "NDCG_at_10_shap = np.zeros(num_of_bins)\n",
    "NDCG_at_20_shap = np.zeros(num_of_bins)\n",
    "NDCG_at_50_shap = np.zeros(num_of_bins)\n",
    "NDCG_at_100_shap = np.zeros(num_of_bins)\n",
    "\n",
    "POS_at_1_j_g = np.zeros(num_of_bins)\n",
    "pos_at_5_j_g = np.zeros(num_of_bins)\n",
    "POS_at_10_j_g = np.zeros(num_of_bins)\n",
    "POS_at_20_j_g = np.zeros(num_of_bins)\n",
    "POS_at_50_j_g = np.zeros(num_of_bins)\n",
    "POS_at_100_j_g = np.zeros(num_of_bins)\n",
    "NEG_at_1_j_g = np.zeros(num_of_bins)\n",
    "NEG_at_5_j_g = np.zeros(num_of_bins)\n",
    "NEG_at_10_j_g = np.zeros(num_of_bins)\n",
    "NEG_at_20_j_g = np.zeros(num_of_bins)\n",
    "NEG_at_50_j_g = np.zeros(num_of_bins)\n",
    "NEG_at_100_j_g = np.zeros(num_of_bins)\n",
    "users_DEL_j_g = np.zeros(num_of_bins)\n",
    "users_INS_j_g = np.zeros(num_of_bins)\n",
    "rank_at_1_j_g = np.zeros(num_of_bins)\n",
    "rank_at_5_j_g = np.zeros(num_of_bins)\n",
    "rank_at_10_A_j_g = np.zeros(num_of_bins)\n",
    "rank_at_20_A_j_g = np.zeros(num_of_bins)\n",
    "rank_at_50_A_j_g = np.zeros(num_of_bins)\n",
    "rank_at_100_A_j_g = np.zeros(num_of_bins)\n",
    "rank_at_k_B_j_g = np.zeros(num_of_bins)\n",
    "NDCG_at_1_j_g = np.zeros(num_of_bins)\n",
    "NDCG_at_5_j_g = np.zeros(num_of_bins)\n",
    "NDCG_at_10_j_g = np.zeros(num_of_bins)\n",
    "NDCG_at_20_j_g = np.zeros(num_of_bins)\n",
    "NDCG_at_50_j_g = np.zeros(num_of_bins)\n",
    "NDCG_at_100_j_g = np.zeros(num_of_bins)\n",
    "\n",
    "\n",
    "POS_at_1_j_u = np.zeros(num_of_bins)\n",
    "pos_at_5_j_u = np.zeros(num_of_bins)\n",
    "POS_at_10_j_u = np.zeros(num_of_bins)\n",
    "POS_at_20_j_u = np.zeros(num_of_bins)\n",
    "POS_at_50_j_u = np.zeros(num_of_bins)\n",
    "POS_at_100_j_u = np.zeros(num_of_bins)\n",
    "NEG_at_1_j_u = np.zeros(num_of_bins)\n",
    "NEG_at_5_j_u = np.zeros(num_of_bins)\n",
    "NEG_at_10_j_u = np.zeros(num_of_bins)\n",
    "NEG_at_20_j_u = np.zeros(num_of_bins)\n",
    "NEG_at_50_j_u = np.zeros(num_of_bins)\n",
    "NEG_at_100_j_u = np.zeros(num_of_bins)\n",
    "users_DEL_j_u = np.zeros(num_of_bins)\n",
    "users_INS_j_u = np.zeros(num_of_bins)\n",
    "rank_at_1_j_u = np.zeros(num_of_bins)\n",
    "rank_at_5_j_u = np.zeros(num_of_bins)\n",
    "rank_at_10_A_j_u = np.zeros(num_of_bins)\n",
    "rank_at_20_A_j_u = np.zeros(num_of_bins)\n",
    "rank_at_50_A_j_u = np.zeros(num_of_bins)\n",
    "rank_at_100_A_j_u = np.zeros(num_of_bins)\n",
    "rank_at_k_B_j_u = np.zeros(num_of_bins)\n",
    "NDCG_at_1_j_u = np.zeros(num_of_bins)\n",
    "NDCG_at_5_j_u = np.zeros(num_of_bins)\n",
    "NDCG_at_10_j_u = np.zeros(num_of_bins)\n",
    "NDCG_at_20_j_u = np.zeros(num_of_bins)\n",
    "NDCG_at_50_j_u = np.zeros(num_of_bins)\n",
    "NDCG_at_100_j_u = np.zeros(num_of_bins)\n",
    "\n",
    "\n",
    "POS_at_1_c_s = np.zeros(num_of_bins)\n",
    "pos_at_5_c_s = np.zeros(num_of_bins)\n",
    "POS_at_10_c_s = np.zeros(num_of_bins)\n",
    "POS_at_20_c_s = np.zeros(num_of_bins)\n",
    "POS_at_50_c_s = np.zeros(num_of_bins)\n",
    "POS_at_100_c_s = np.zeros(num_of_bins)\n",
    "NEG_at_1_c_s = np.zeros(num_of_bins)\n",
    "NEG_at_5_c_s = np.zeros(num_of_bins)\n",
    "NEG_at_10_c_s = np.zeros(num_of_bins)\n",
    "NEG_at_20_c_s= np.zeros(num_of_bins)\n",
    "NEG_at_50_c_s = np.zeros(num_of_bins)\n",
    "NEG_at_100_c_s = np.zeros(num_of_bins)\n",
    "users_DEL_c_s = np.zeros(num_of_bins)\n",
    "users_INS_c_s = np.zeros(num_of_bins)\n",
    "rank_at_1_c_s = np.zeros(num_of_bins)\n",
    "rank_at_5_c_s = np.zeros(num_of_bins)\n",
    "rank_at_10_A_c_s = np.zeros(num_of_bins)\n",
    "rank_at_20_A_c_s = np.zeros(num_of_bins)\n",
    "rank_at_50_A_c_s = np.zeros(num_of_bins)\n",
    "rank_at_100_A_c_s = np.zeros(num_of_bins)\n",
    "rank_at_k_B_c_s = np.zeros(num_of_bins)\n",
    "NDCG_at_1_c_s = np.zeros(num_of_bins)\n",
    "NDCG_at_5_c_s = np.zeros(num_of_bins)\n",
    "NDCG_at_10_c_s = np.zeros(num_of_bins)\n",
    "NDCG_at_20_c_s = np.zeros(num_of_bins)\n",
    "NDCG_at_50_c_s = np.zeros(num_of_bins)\n",
    "NDCG_at_100_c_s = np.zeros(num_of_bins)\n",
    "\n",
    "\n",
    "POS_at_1_pop = np.zeros(num_of_bins)\n",
    "pos_at_5_pop = np.zeros(num_of_bins)\n",
    "POS_at_10_pop = np.zeros(num_of_bins)\n",
    "POS_at_20_pop = np.zeros(num_of_bins)\n",
    "POS_at_50_pop = np.zeros(num_of_bins)\n",
    "POS_at_100_pop = np.zeros(num_of_bins)\n",
    "NEG_at_1_pop = np.zeros(num_of_bins)\n",
    "NEG_at_5_pop = np.zeros(num_of_bins)\n",
    "NEG_at_10_pop = np.zeros(num_of_bins)\n",
    "NEG_at_20_pop = np.zeros(num_of_bins)\n",
    "NEG_at_50_pop = np.zeros(num_of_bins)\n",
    "NEG_at_100_pop = np.zeros(num_of_bins)\n",
    "users_DEL_pop = np.zeros(num_of_bins)\n",
    "users_INS_pop = np.zeros(num_of_bins)\n",
    "rank_at_1_pop = np.zeros(num_of_bins)\n",
    "rank_at_5_pop = np.zeros(num_of_bins)\n",
    "rank_at_10_A_pop = np.zeros(num_of_bins)\n",
    "rank_at_20_A_pop = np.zeros(num_of_bins)\n",
    "rank_at_50_A_pop = np.zeros(num_of_bins)\n",
    "rank_at_100_A_pop = np.zeros(num_of_bins)\n",
    "rank_at_k_B_pop = np.zeros(num_of_bins)\n",
    "NDCG_at_1_pop = np.zeros(num_of_bins)\n",
    "NDCG_at_5_pop = np.zeros(num_of_bins)\n",
    "NDCG_at_10_pop = np.zeros(num_of_bins)\n",
    "NDCG_at_20_pop = np.zeros(num_of_bins)\n",
    "NDCG_at_50_pop = np.zeros(num_of_bins)\n",
    "NDCG_at_100_pop = np.zeros(num_of_bins)\n",
    "\n",
    "POS_at_1_lime = np.zeros(num_of_bins)\n",
    "pos_at_5_lime = np.zeros(num_of_bins)\n",
    "POS_at_10_lime = np.zeros(num_of_bins)\n",
    "POS_at_20_lime = np.zeros(num_of_bins)\n",
    "POS_at_50_lime = np.zeros(num_of_bins)\n",
    "POS_at_100_lime = np.zeros(num_of_bins)\n",
    "NEG_at_1_lime = np.zeros(num_of_bins)\n",
    "NEG_at_5_lime = np.zeros(num_of_bins)\n",
    "NEG_at_10_lime = np.zeros(num_of_bins)\n",
    "NEG_at_20_lime = np.zeros(num_of_bins)\n",
    "NEG_at_50_lime = np.zeros(num_of_bins)\n",
    "NEG_at_100_lime = np.zeros(num_of_bins)\n",
    "users_DEL_lime = np.zeros(num_of_bins)\n",
    "users_INS_lime = np.zeros(num_of_bins)\n",
    "rank_at_1_lime = np.zeros(num_of_bins)\n",
    "rank_at_5_lime = np.zeros(num_of_bins)\n",
    "rank_at_10_A_lime = np.zeros(num_of_bins)\n",
    "rank_at_20_A_lime = np.zeros(num_of_bins)\n",
    "rank_at_50_A_lime = np.zeros(num_of_bins)\n",
    "rank_at_100_A_lime = np.zeros(num_of_bins)\n",
    "rank_at_k_B_lime = np.zeros(num_of_bins)\n",
    "NDCG_at_1_lime = np.zeros(num_of_bins)\n",
    "NDCG_at_5_lime = np.zeros(num_of_bins)\n",
    "NDCG_at_10_lime = np.zeros(num_of_bins)\n",
    "NDCG_at_20_lime = np.zeros(num_of_bins)\n",
    "NDCG_at_50_lime = np.zeros(num_of_bins)\n",
    "NDCG_at_100_lime = np.zeros(num_of_bins)\n",
    "\n",
    "POS_at_1_tf_idf = np.zeros(num_of_bins)\n",
    "pos_at_5_tf_idf = np.zeros(num_of_bins)\n",
    "POS_at_10_tf_idf = np.zeros(num_of_bins)\n",
    "POS_at_20_tf_idf = np.zeros(num_of_bins)\n",
    "POS_at_50_tf_idf = np.zeros(num_of_bins)\n",
    "POS_at_100_tf_idf = np.zeros(num_of_bins)\n",
    "NEG_at_1_tf_idf = np.zeros(num_of_bins)\n",
    "NEG_at_5_tf_idf = np.zeros(num_of_bins)\n",
    "NEG_at_10_tf_idf = np.zeros(num_of_bins)\n",
    "NEG_at_20_tf_idf = np.zeros(num_of_bins)\n",
    "NEG_at_50_tf_idf = np.zeros(num_of_bins)\n",
    "NEG_at_100_tf_idf = np.zeros(num_of_bins)\n",
    "users_DEL_tf_idf = np.zeros(num_of_bins)\n",
    "users_INS_tf_idf = np.zeros(num_of_bins)\n",
    "rank_at_1_tf_idf = np.zeros(num_of_bins)\n",
    "rank_at_5_tf_idf = np.zeros(num_of_bins)\n",
    "rank_at_10_A_tf_idf = np.zeros(num_of_bins)\n",
    "rank_at_20_A_tf_idf = np.zeros(num_of_bins)\n",
    "rank_at_50_A_tf_idf = np.zeros(num_of_bins)\n",
    "rank_at_100_A_tf_idf = np.zeros(num_of_bins)\n",
    "rank_at_k_B_tf_idf = np.zeros(num_of_bins)\n",
    "NDCG_at_1_tf_idf = np.zeros(num_of_bins)\n",
    "NDCG_at_5_tf_idf = np.zeros(num_of_bins)\n",
    "NDCG_at_10_tf_idf = np.zeros(num_of_bins)\n",
    "NDCG_at_20_tf_idf = np.zeros(num_of_bins)\n",
    "NDCG_at_50_tf_idf = np.zeros(num_of_bins)\n",
    "NDCG_at_100_tf_idf = np.zeros(num_of_bins)\n",
    "\n",
    "POS_at_1_lxr = np.zeros(num_of_bins)\n",
    "pos_at_5_lxr = np.zeros(num_of_bins)\n",
    "POS_at_10_lxr = np.zeros(num_of_bins)\n",
    "POS_at_20_lxr = np.zeros(num_of_bins)\n",
    "POS_at_50_lxr = np.zeros(num_of_bins)\n",
    "POS_at_100_lxr = np.zeros(num_of_bins)\n",
    "NEG_at_1_lxr = np.zeros(num_of_bins)\n",
    "NEG_at_5_lxr = np.zeros(num_of_bins)\n",
    "NEG_at_10_lxr = np.zeros(num_of_bins)\n",
    "NEG_at_20_lxr = np.zeros(num_of_bins)\n",
    "NEG_at_50_lxr = np.zeros(num_of_bins)\n",
    "NEG_at_100_lxr = np.zeros(num_of_bins)\n",
    "users_DEL_lxr = np.zeros(num_of_bins)\n",
    "users_INS_lxr = np.zeros(num_of_bins)\n",
    "rank_at_1_lxr = np.zeros(num_of_bins)\n",
    "rank_at_5_lxr = np.zeros(num_of_bins)\n",
    "rank_at_10_A_lxr = np.zeros(num_of_bins)\n",
    "rank_at_20_A_lxr = np.zeros(num_of_bins)\n",
    "rank_at_50_A_lxr = np.zeros(num_of_bins)\n",
    "rank_at_100_A_lxr = np.zeros(num_of_bins)\n",
    "rank_at_k_B_lxr = np.zeros(num_of_bins)\n",
    "NDCG_at_1_lxr = np.zeros(num_of_bins)\n",
    "NDCG_at_5_lxr = np.zeros(num_of_bins)\n",
    "NDCG_at_10_lxr = np.zeros(num_of_bins)\n",
    "NDCG_at_20_lxr = np.zeros(num_of_bins)\n",
    "NDCG_at_50_lxr = np.zeros(num_of_bins)\n",
    "NDCG_at_100_lxr = np.zeros(num_of_bins)\n",
    "\n",
    "num_of_bins = 10\n",
    "\n",
    "explainer_model_g.eval()\n",
    "model_combined.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(test_array.shape[0]):\n",
    "\n",
    "        user_id = test_array[i][-2]\n",
    "        user_vector = test_array[i][:-2]\n",
    "\n",
    "        y_pred = topk_test[user_id]\n",
    "        item_id = np.argmax(y_pred)\n",
    "\n",
    "        item_vector = items_values_dict[item_id]\n",
    "        item_tensor = torch.FloatTensor(item_vector).to(device)\n",
    "\n",
    "        user_vector[item_id] = 0\n",
    "        user_tensor = torch.FloatTensor(user_vector).to(device)\n",
    "\n",
    "        ### Jaccard user:\n",
    "        res = single_user_metrics(user_vector, item_id, k, num_of_bins, num_items, rec_model, model_combined, mask_type= 'jaccard_u')\n",
    "        POS_at_1_j_u += res[0]\n",
    "        pos_at_5_j_u += res[1]\n",
    "        POS_at_10_j_u += res[2]\n",
    "        POS_at_20_j_u += res[3]\n",
    "        POS_at_50_j_u += res[4]\n",
    "        POS_at_100_j_u += res[5]\n",
    "        NEG_at_1_j_u += res[6]\n",
    "        NEG_at_5_j_u += res[7]\n",
    "        NEG_at_10_j_u += res[8]\n",
    "        NEG_at_20_j_u += res[9]\n",
    "        NEG_at_50_j_u += res[10]\n",
    "        NEG_at_100_j_u += res[11]\n",
    "        users_DEL_j_u += res[12]\n",
    "        users_INS_j_u += res[13]\n",
    "        rank_at_1_j_u += res[14]\n",
    "        rank_at_5_j_u += res[15]\n",
    "        rank_at_10_A_j_u += res[16]\n",
    "        rank_at_20_A_j_u += res[17]\n",
    "        rank_at_50_A_j_u += res[18]\n",
    "        rank_at_100_A_j_u += res[19]\n",
    "        rank_at_k_B_j_u += res[20]\n",
    "        NDCG_at_1_j_u += res[21]\n",
    "        NDCG_at_5_j_u += res[22]\n",
    "        NDCG_at_10_j_u += res[23]\n",
    "        NDCG_at_20_j_u += res[24]\n",
    "        NDCG_at_50_j_u += res[25]\n",
    "        NDCG_at_100_j_u += res[26]\n",
    "\n",
    "        ### cosine similarity:\n",
    "        res = single_user_metrics(user_vector, item_id, k, num_of_bins, num_items, rec_model, model_combined, mask_type= 'cosine')\n",
    "        POS_at_1_c_s += res[0]\n",
    "        pos_at_5_c_s += res[1]\n",
    "        POS_at_10_c_s += res[2]\n",
    "        POS_at_20_c_s += res[3]\n",
    "        POS_at_50_c_s += res[4]\n",
    "        POS_at_100_c_s += res[5]\n",
    "        NEG_at_1_c_s += res[6]\n",
    "        NEG_at_5_c_s += res[7]\n",
    "        NEG_at_10_c_s += res[8]\n",
    "        NEG_at_20_c_s += res[9]\n",
    "        NEG_at_50_c_s += res[10]\n",
    "        NEG_at_100_c_s += res[11]\n",
    "        users_DEL_c_s += res[12]\n",
    "        users_INS_c_s += res[13]\n",
    "        rank_at_1_c_s += res[14]\n",
    "        rank_at_5_c_s += res[15]\n",
    "        rank_at_10_A_c_s += res[16]\n",
    "        rank_at_20_A_c_s += res[17]\n",
    "        rank_at_50_A_c_s += res[18]\n",
    "        rank_at_100_A_c_s += res[19]\n",
    "        rank_at_k_B_c_s += res[20]\n",
    "        NDCG_at_1_c_s += res[21]\n",
    "        NDCG_at_5_c_s += res[22]\n",
    "        NDCG_at_10_c_s += res[23]\n",
    "        NDCG_at_20_c_s += res[24]\n",
    "        NDCG_at_50_c_s += res[25]\n",
    "        NDCG_at_100_c_s += res[26]\n",
    "        \n",
    "        ## tf-idf similarity:\n",
    "        res = single_user_metrics(user_vector, item_id, k, num_of_bins, num_items, rec_model, model_combined, mask_type= 'tf_idf')\n",
    "        POS_at_1_tf_idf += res[0]\n",
    "        pos_at_5_tf_idf += res[1]\n",
    "        POS_at_10_tf_idf += res[2]\n",
    "        POS_at_20_tf_idf += res[3]\n",
    "        POS_at_50_tf_idf += res[4]\n",
    "        POS_at_100_tf_idf += res[5]\n",
    "        NEG_at_1_tf_idf += res[6]\n",
    "        NEG_at_5_tf_idf += res[7]\n",
    "        NEG_at_10_tf_idf += res[8]\n",
    "        NEG_at_20_tf_idf += res[9]\n",
    "        NEG_at_50_tf_idf += res[10]\n",
    "        NEG_at_100_tf_idf += res[11]\n",
    "        users_DEL_tf_idf += res[12]\n",
    "        users_INS_tf_idf += res[13]\n",
    "        rank_at_1_tf_idf += res[14]\n",
    "        rank_at_5_tf_idf += res[15]\n",
    "        rank_at_10_A_tf_idf += res[16]\n",
    "        rank_at_20_A_tf_idf += res[17]\n",
    "        rank_at_50_A_tf_idf += res[18]\n",
    "        rank_at_100_A_tf_idf += res[19]\n",
    "        rank_at_k_B_tf_idf += res[20]\n",
    "        NDCG_at_1_tf_idf += res[21]\n",
    "        NDCG_at_5_tf_idf += res[22]\n",
    "        NDCG_at_10_tf_idf += res[23]\n",
    "        NDCG_at_20_tf_idf += res[24]\n",
    "        NDCG_at_50_tf_idf += res[25]\n",
    "        NDCG_at_100_tf_idf += res[26]\n",
    "\n",
    "        ### pop:\n",
    "        res = single_user_metrics(user_vector, item_id, k, num_of_bins, num_items, rec_model, model_combined, mask_type= 'pop')    \n",
    "        POS_at_1_pop += res[0]\n",
    "        pos_at_5_pop += res[1]\n",
    "        POS_at_10_pop += res[2]\n",
    "        POS_at_20_pop += res[3]\n",
    "        POS_at_50_pop += res[4]\n",
    "        POS_at_100_pop += res[5]\n",
    "        NEG_at_1_pop += res[6]\n",
    "        NEG_at_5_pop += res[7]\n",
    "        NEG_at_10_pop += res[8]\n",
    "        NEG_at_20_pop += res[9]\n",
    "        NEG_at_50_pop += res[10]\n",
    "        NEG_at_100_pop += res[11]\n",
    "        users_DEL_pop += res[12]\n",
    "        users_INS_pop += res[13]\n",
    "        rank_at_1_pop += res[14]\n",
    "        rank_at_5_pop += res[15]\n",
    "        rank_at_10_A_pop += res[16]\n",
    "        rank_at_20_A_pop += res[17]\n",
    "        rank_at_50_A_pop += res[18]\n",
    "        rank_at_100_A_pop += res[19]\n",
    "        rank_at_k_B_pop += res[20]\n",
    "        NDCG_at_1_pop += res[21]\n",
    "        NDCG_at_5_pop += res[22]\n",
    "        NDCG_at_10_pop += res[23]\n",
    "        NDCG_at_20_pop += res[24]\n",
    "        NDCG_at_50_pop += res[25]\n",
    "        NDCG_at_100_pop += res[26]\n",
    "\n",
    "        ### LXR:\n",
    "        res = single_user_metrics(user_vector, item_id, k, num_of_bins, num_items, rec_model, model_combined, mask_type= 'lxr')    \n",
    "        POS_at_1_lxr += res[0]\n",
    "        pos_at_5_lxr += res[1]\n",
    "        POS_at_10_lxr += res[2]\n",
    "        POS_at_20_lxr += res[3]\n",
    "        POS_at_50_lxr += res[4]\n",
    "        POS_at_100_lxr += res[5]\n",
    "        NEG_at_1_lxr += res[6]\n",
    "        NEG_at_5_lxr += res[7]\n",
    "        NEG_at_10_lxr += res[8]\n",
    "        NEG_at_20_lxr += res[9]\n",
    "        NEG_at_50_lxr += res[10]\n",
    "        NEG_at_100_lxr += res[11]\n",
    "        users_DEL_lxr += res[12]\n",
    "        users_INS_lxr += res[13]\n",
    "        rank_at_1_lxr += res[14]\n",
    "        rank_at_5_lxr += res[15]\n",
    "        rank_at_10_A_lxr += res[16]\n",
    "        rank_at_20_A_lxr += res[17]\n",
    "        rank_at_50_A_lxr += res[18]\n",
    "        rank_at_100_A_lxr += res[19]\n",
    "        rank_at_k_B_lxr += res[20]\n",
    "        NDCG_at_1_lxr += res[21]\n",
    "        NDCG_at_5_lxr += res[22]\n",
    "        NDCG_at_10_lxr += res[23]\n",
    "        NDCG_at_20_lxr += res[24]\n",
    "        NDCG_at_50_lxr += res[25]\n",
    "        NDCG_at_100_lxr += res[26]\n",
    "        \n",
    "        ### Lime:\n",
    "        res = single_user_metrics(user_vector, item_id, k, num_of_bins, num_items, rec_model, model_combined, mask_type= 'lime')    \n",
    "        POS_at_1_lime += res[0]\n",
    "        pos_at_5_lime += res[1]\n",
    "        POS_at_10_lime += res[2]\n",
    "        POS_at_20_lime += res[3]\n",
    "        POS_at_50_lime += res[4]\n",
    "        POS_at_100_lime += res[5]\n",
    "        NEG_at_1_lime += res[6]\n",
    "        NEG_at_5_lime += res[7]\n",
    "        NEG_at_10_lime += res[8]\n",
    "        NEG_at_20_lime += res[9]\n",
    "        NEG_at_50_lime += res[10]\n",
    "        NEG_at_100_lime += res[11]\n",
    "        users_DEL_lime += res[12]\n",
    "        users_INS_lime += res[13]\n",
    "        rank_at_1_lime += res[14]\n",
    "        rank_at_5_lime += res[15]\n",
    "        rank_at_10_A_lime += res[16]\n",
    "        rank_at_20_A_lime += res[17]\n",
    "        rank_at_50_A_lime += res[18]\n",
    "        rank_at_100_A_lime += res[19]\n",
    "        rank_at_k_B_lime += res[20]\n",
    "        NDCG_at_1_lime += res[21]\n",
    "        NDCG_at_5_lime += res[22]\n",
    "        NDCG_at_10_lime += res[23]\n",
    "        NDCG_at_20_lime += res[24]\n",
    "        NDCG_at_50_lime += res[25]\n",
    "        NDCG_at_100_lime += res[26]\n",
    "\n",
    "\n",
    "        if(i%100 == 0):\n",
    "            print(i)\n",
    "           \n",
    "a = i+1\n",
    "\n",
    "print('POS_at_1_j_u: ', np.mean(POS_at_1_j_u[1:])/a)\n",
    "print('pos_at_5_j_u: ', np.mean(pos_at_5_j_u[1:])/a)\n",
    "print('POS_at_10_j_u: ', np.mean(POS_at_10_j_u[1:])/a)\n",
    "print('POS_at_20_j_u: ', np.mean(POS_at_20_j_u[1:])/a)\n",
    "print('POS_at_50_j_u: ', np.mean(POS_at_50_j_u[1:])/a)\n",
    "print('POS_at_100_j_u: ', np.mean(POS_at_100_j_u[1:])/a)\n",
    "print('NEG_at_1_j_u: ', np.mean(NEG_at_1_j_u[1:])/a)\n",
    "print('NEG_at_5_j_u: ', np.mean(NEG_at_5_j_u[1:])/a)\n",
    "print('NEG_at_10_j_u: ', np.mean(NEG_at_10_j_u[1:])/a)\n",
    "print('NEG_at_20_j_u: ', np.mean(NEG_at_20_j_u[1:])/a)\n",
    "print('NEG_at_50_j_u: ', np.mean(NEG_at_50_j_u[1:])/a)\n",
    "print('NEG_at_100_j_u: ', np.mean(NEG_at_100_j_u[1:])/a)\n",
    "print('users_DEL_j_u: ', np.mean(users_DEL_j_u[1:])/a)\n",
    "print('users_INS_j_u: ', np.mean(users_INS_j_u[1:])/a)\n",
    "print('rank_at_1_j_u: ', np.mean(rank_at_1_j_u[1:])/a)\n",
    "print('rank_at_5_j_u: ', np.mean(rank_at_5_j_u[1:])/a)\n",
    "print('rank_at_10_A_j_u: ', np.mean(rank_at_10_A_j_u[1:])/a)\n",
    "print('rank_at_20_A_j_u: ', np.mean(rank_at_20_A_j_u[1:])/a)\n",
    "print('rank_at_50_A_j_u: ', np.mean(rank_at_50_A_j_u[1:])/a)\n",
    "print('rank_at_100_A_j_u: ', np.mean(rank_at_100_A_j_u[1:])/a)\n",
    "print('rank_at_k_B_j_u: ', np.mean(rank_at_k_B_j_u[1:])/a)\n",
    "print('NDCG_at_1_j_u: ', np.mean(NDCG_at_1_j_u[1:])/a)\n",
    "print('NDCG_at_5_j_u: ', np.mean(NDCG_at_5_j_u[1:])/a)\n",
    "print('NDCG_at_10_j_u: ', np.mean(NDCG_at_10_j_u[1:])/a)\n",
    "print('NDCG_at_20_j_u: ', np.mean(NDCG_at_20_j_u[1:])/a)\n",
    "print('NDCG_at_50_j_u: ', np.mean(NDCG_at_50_j_u[1:])/a)\n",
    "print('NDCG_at_100_j_u: ', np.mean(NDCG_at_100_j_u[1:])/a)\n",
    "\n",
    "\n",
    "print('POS_at_1_c_s: ', np.mean(POS_at_1_c_s[1:])/a)\n",
    "print('pos_at_5_c_s: ', np.mean(pos_at_5_c_s[1:])/a)\n",
    "print('POS_at_10_c_s: ', np.mean(POS_at_10_c_s[1:])/a)\n",
    "print('POS_at_20_c_s: ', np.mean(POS_at_20_c_s[1:])/a)\n",
    "print('POS_at_50_c_s: ', np.mean(POS_at_50_c_s[1:])/a)\n",
    "print('POS_at_100_c_s: ', np.mean(POS_at_100_c_s[1:])/a)\n",
    "print('NEG_at_1_c_s: ', np.mean(NEG_at_1_c_s[1:])/a)\n",
    "print('NEG_at_5_c_s: ', np.mean(NEG_at_5_c_s[1:])/a)\n",
    "print('NEG_at_10_c_s: ', np.mean(NEG_at_10_c_s[1:])/a)\n",
    "print('NEG_at_20_c_s: ', np.mean(NEG_at_20_c_s[1:])/a)\n",
    "print('NEG_at_50_c_s: ', np.mean(NEG_at_50_c_s[1:])/a)\n",
    "print('NEG_at_100_c_s: ', np.mean(NEG_at_100_c_s[1:])/a)\n",
    "print('users_DEL_c_s: ', np.mean(users_DEL_c_s[1:])/a)\n",
    "print('users_INS_c_s: ', np.mean(users_INS_c_s[1:])/a)\n",
    "print('rank_at_1_c_s: ', np.mean(rank_at_1_c_s[1:])/a)\n",
    "print('rank_at_5_c_s: ', np.mean(rank_at_5_c_s[1:])/a)\n",
    "print('rank_at_10_A_c_s: ', np.mean(rank_at_10_A_c_s[1:])/a)\n",
    "print('rank_at_20_A_c_s: ', np.mean(rank_at_20_A_c_s[1:])/a)\n",
    "print('rank_at_50_A_c_s: ', np.mean(rank_at_50_A_c_s[1:])/a)\n",
    "print('rank_at_100_A_c_s: ', np.mean(rank_at_100_A_c_s[1:])/a)\n",
    "print('rank_at_k_B_c_s: ', np.mean(rank_at_k_B_c_s[1:])/a)\n",
    "print('NDCG_at_1_c_s: ', np.mean(NDCG_at_1_c_s[1:])/a)\n",
    "print('NDCG_at_5_c_s: ', np.mean(NDCG_at_5_c_s[1:])/a)\n",
    "print('NDCG_at_10_c_s: ', np.mean(NDCG_at_10_c_s[1:])/a)\n",
    "print('NDCG_at_20_c_s: ', np.mean(NDCG_at_20_c_s[1:])/a)\n",
    "print('NDCG_at_50_c_s: ', np.mean(NDCG_at_50_c_s[1:])/a)\n",
    "print('NDCG_at_100_c_s: ', np.mean(NDCG_at_100_c_s[1:])/a)\n",
    "\n",
    "print('POS_at_1_pop: ', np.mean(POS_at_1_pop[1:])/a)\n",
    "print('pos_at_5_pop: ', np.mean(pos_at_5_pop[1:])/a)\n",
    "print('POS_at_10_pop: ', np.mean(POS_at_10_pop[1:])/a)\n",
    "print('POS_at_20_pop: ', np.mean(POS_at_20_pop[1:])/a)\n",
    "print('POS_at_50_pop: ', np.mean(POS_at_50_pop[1:])/a)\n",
    "print('POS_at_100_pop: ', np.mean(POS_at_100_pop[1:])/a)\n",
    "print('NEG_at_1_pop: ', np.mean(NEG_at_1_pop[1:])/a)\n",
    "print('NEG_at_5_pop: ', np.mean(NEG_at_5_pop[1:])/a)\n",
    "print('NEG_at_10_pop: ', np.mean(NEG_at_10_pop[1:])/a)\n",
    "print('NEG_at_20_pop: ', np.mean(NEG_at_20_pop[1:])/a)\n",
    "print('NEG_at_50_pop: ', np.mean(NEG_at_50_pop[1:])/a)\n",
    "print('NEG_at_100_pop: ', np.mean(NEG_at_100_pop[1:])/a)\n",
    "print('users_DEL_pop: ', np.mean(users_DEL_pop[1:])/a)\n",
    "print('users_INS_pop: ', np.mean(users_INS_pop[1:])/a)\n",
    "print('rank_at_1_pop: ', np.mean(rank_at_1_pop[1:])/a)\n",
    "print('rank_at_5_pop: ', np.mean(rank_at_5_pop[1:])/a)\n",
    "print('rank_at_10_A_pop: ', np.mean(rank_at_10_A_pop[1:])/a)\n",
    "print('rank_at_20_A_pop: ', np.mean(rank_at_20_A_pop[1:])/a)\n",
    "print('rank_at_50_A_pop: ', np.mean(rank_at_50_A_pop[1:])/a)\n",
    "print('rank_at_100_A_pop: ', np.mean(rank_at_100_A_pop[1:])/a)\n",
    "print('rank_at_k_B_pop: ', np.mean(rank_at_k_B_pop[1:])/a)\n",
    "print('NDCG_at_1_pop: ', np.mean(NDCG_at_1_pop[1:])/a)\n",
    "print('NDCG_at_5_pop: ', np.mean(NDCG_at_5_pop[1:])/a)\n",
    "print('NDCG_at_10_pop: ', np.mean(NDCG_at_10_pop[1:])/a)\n",
    "print('NDCG_at_20_pop: ', np.mean(NDCG_at_20_pop[1:])/a)\n",
    "print('NDCG_at_50_pop: ', np.mean(NDCG_at_50_pop[1:])/a)\n",
    "print('NDCG_at_100_pop: ', np.mean(NDCG_at_100_pop[1:])/a)\n",
    "\n",
    "print('POS_at_1_lime: ', np.mean(POS_at_1_lime[1:])/a)\n",
    "print('pos_at_5_lime: ', np.mean(pos_at_5_lime[1:])/a)\n",
    "print('POS_at_10_lime: ', np.mean(POS_at_10_lime[1:])/a)\n",
    "print('POS_at_20_lime: ', np.mean(POS_at_20_lime[1:])/a)\n",
    "print('POS_at_50_lime: ', np.mean(POS_at_50_lime[1:])/a)\n",
    "print('POS_at_100_lime: ', np.mean(POS_at_100_lime[1:])/a)\n",
    "print('NEG_at_1_lime: ', np.mean(NEG_at_1_lime[1:])/a)\n",
    "print('NEG_at_5_lime: ', np.mean(NEG_at_5_lime[1:])/a)\n",
    "print('NEG_at_10_lime: ', np.mean(NEG_at_10_lime[1:])/a)\n",
    "print('NEG_at_20_lime: ', np.mean(NEG_at_20_lime[1:])/a)\n",
    "print('NEG_at_50_lime: ', np.mean(NEG_at_50_lime[1:])/a)\n",
    "print('NEG_at_100_lime: ', np.mean(NEG_at_100_lime[1:])/a)\n",
    "print('users_DEL_lime: ', np.mean(users_DEL_lime[1:])/a)\n",
    "print('users_INS_lime: ', np.mean(users_INS_lime[1:])/a)\n",
    "print('rank_at_1_lime: ', np.mean(rank_at_1_lime[1:])/a)\n",
    "print('rank_at_5_lime: ', np.mean(rank_at_5_lime[1:])/a)\n",
    "print('rank_at_10_A_lime: ', np.mean(rank_at_10_A_lime[1:])/a)\n",
    "print('rank_at_20_A_lime: ', np.mean(rank_at_20_A_lime[1:])/a)\n",
    "print('rank_at_50_A_lime: ', np.mean(rank_at_50_A_lime[1:])/a)\n",
    "print('rank_at_100_A_lime: ', np.mean(rank_at_100_A_lime[1:])/a)\n",
    "print('rank_at_k_B_lime: ', np.mean(rank_at_k_B_lime[1:])/a)\n",
    "print('NDCG_at_1_lime: ', np.mean(NDCG_at_1_lime[1:])/a)\n",
    "print('NDCG_at_5_lime: ', np.mean(NDCG_at_5_lime[1:])/a)\n",
    "print('NDCG_at_10_lime: ', np.mean(NDCG_at_10_lime[1:])/a)\n",
    "print('NDCG_at_20_lime: ', np.mean(NDCG_at_20_lime[1:])/a)\n",
    "print('NDCG_at_50_lime: ', np.mean(NDCG_at_50_lime[1:])/a)\n",
    "print('NDCG_at_100_lime: ', np.mean(NDCG_at_100_lime[1:])/a)\n",
    "\n",
    "print('POS_at_1_tf_idf: ', np.mean(POS_at_1_tf_idf[1:])/a)\n",
    "print('pos_at_5_tf_idf: ', np.mean(pos_at_5_tf_idf[1:])/a)\n",
    "print('POS_at_10_tf_idf: ', np.mean(POS_at_10_tf_idf[1:])/a)\n",
    "print('POS_at_20_tf_idf: ', np.mean(POS_at_20_tf_idf[1:])/a)\n",
    "print('POS_at_50_tf_idf: ', np.mean(POS_at_50_tf_idf[1:])/a)\n",
    "print('POS_at_100_tf_idf: ', np.mean(POS_at_100_tf_idf[1:])/a)\n",
    "print('NEG_at_1_tf_idf: ', np.mean(NEG_at_1_tf_idf[1:])/a)\n",
    "print('NEG_at_5_tf_idf: ', np.mean(NEG_at_5_tf_idf[1:])/a)\n",
    "print('NEG_at_10_tf_idf: ', np.mean(NEG_at_10_tf_idf[1:])/a)\n",
    "print('NEG_at_20_tf_idf: ', np.mean(NEG_at_20_tf_idf[1:])/a)\n",
    "print('NEG_at_50_tf_idf: ', np.mean(NEG_at_50_tf_idf[1:])/a)\n",
    "print('NEG_at_100_tf_idf: ', np.mean(NEG_at_100_tf_idf[1:])/a)\n",
    "print('users_DEL_tf_idf: ', np.mean(users_DEL_tf_idf[1:])/a)\n",
    "print('users_INS_tf_idf: ', np.mean(users_INS_tf_idf[1:])/a)\n",
    "print('rank_at_1_tf_idf: ', np.mean(rank_at_1_tf_idf[1:])/a)\n",
    "print('rank_at_5_tf_idf: ', np.mean(rank_at_5_tf_idf[1:])/a)\n",
    "print('rank_at_10_A_tf_idf: ', np.mean(rank_at_10_A_tf_idf[1:])/a)\n",
    "print('rank_at_20_A_tf_idf: ', np.mean(rank_at_20_A_tf_idf[1:])/a)\n",
    "print('rank_at_50_A_tf_idf: ', np.mean(rank_at_50_A_tf_idf[1:])/a)\n",
    "print('rank_at_100_A_tf_idf: ', np.mean(rank_at_100_A_tf_idf[1:])/a)\n",
    "print('rank_at_k_B_tf_idf: ', np.mean(rank_at_k_B_tf_idf[1:])/a)\n",
    "print('NDCG_at_1_tf_idf: ', np.mean(NDCG_at_1_tf_idf[1:])/a)\n",
    "print('NDCG_at_5_tf_idf: ', np.mean(NDCG_at_5_tf_idf[1:])/a)\n",
    "print('NDCG_at_10_tf_idf: ', np.mean(NDCG_at_10_tf_idf[1:])/a)\n",
    "print('NDCG_at_20_tf_idf: ', np.mean(NDCG_at_20_tf_idf[1:])/a)\n",
    "print('NDCG_at_50_tf_idf: ', np.mean(NDCG_at_50_tf_idf[1:])/a)\n",
    "print('NDCG_at_100_tf_idf: ', np.mean(NDCG_at_100_tf_idf[1:])/a)\n",
    "\n",
    "print('POS_at_1_lxr: ', np.mean(POS_at_1_lxr[1:])/a)\n",
    "print('pos_at_5_lxr: ', np.mean(pos_at_5_lxr[1:])/a)\n",
    "print('POS_at_10_lxr: ', np.mean(POS_at_10_lxr[1:])/a)\n",
    "print('POS_at_20_lxr: ', np.mean(POS_at_20_lxr[1:])/a)\n",
    "print('POS_at_50_lxr: ', np.mean(POS_at_50_lxr[1:])/a)\n",
    "print('POS_at_100_lxr: ', np.mean(POS_at_100_lxr[1:])/a)\n",
    "print('NEG_at_1_lxr: ', np.mean(NEG_at_1_lxr[1:])/a)\n",
    "print('NEG_at_5_lxr: ', np.mean(NEG_at_5_lxr[1:])/a)\n",
    "print('NEG_at_10_lxr: ', np.mean(NEG_at_10_lxr[1:])/a)\n",
    "print('NEG_at_20_lxr: ', np.mean(NEG_at_20_lxr[1:])/a)\n",
    "print('NEG_at_50_lxr: ', np.mean(NEG_at_50_lxr[1:])/a)\n",
    "print('NEG_at_100_lxr: ', np.mean(NEG_at_100_lxr[1:])/a)\n",
    "print('users_DEL_lxr: ', np.mean(users_DEL_lxr[1:])/a)\n",
    "print('users_INS_lxr: ', np.mean(users_INS_lxr[1:])/a)\n",
    "print('rank_at_1_lxr: ', np.mean(rank_at_1_lxr[1:])/a)\n",
    "print('rank_at_5_lxr: ', np.mean(rank_at_5_lxr[1:])/a)\n",
    "print('rank_at_10_A_lxr: ', np.mean(rank_at_10_A_lxr[1:])/a)\n",
    "print('rank_at_20_A_lxr: ', np.mean(rank_at_20_A_lxr[1:])/a)\n",
    "print('rank_at_50_A_lxr: ', np.mean(rank_at_50_A_lxr[1:])/a)\n",
    "print('rank_at_100_A_lxr: ', np.mean(rank_at_100_A_lxr[1:])/a)\n",
    "print('rank_at_k_B_lxr: ', np.mean(rank_at_k_B_lxr[1:])/a)\n",
    "print('NDCG_at_1_lxr: ', np.mean(NDCG_at_1_lxr[1:])/a)\n",
    "print('NDCG_at_5_lxr: ', np.mean(NDCG_at_5_lxr[1:])/a)\n",
    "print('NDCG_at_10_lxr: ', np.mean(NDCG_at_10_lxr[1:])/a)\n",
    "print('NDCG_at_20_lxr: ', np.mean(NDCG_at_20_lxr[1:])/a)\n",
    "print('NDCG_at_50_lxr: ', np.mean(NDCG_at_50_lxr[1:])/a)\n",
    "print('NDCG_at_100_lxr: ', np.mean(NDCG_at_100_lxr[1:])/a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3923739-78f7-417c-a0d6-03497845de41",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze -> MLP_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e975531-a20e-4d05-b133-a7a6a1edbac3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
